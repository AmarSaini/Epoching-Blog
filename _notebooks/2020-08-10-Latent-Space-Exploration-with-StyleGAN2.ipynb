{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Latent Space Exploration with StyleGAN2\n",
    "> A conceptual tutorial of what the latent space is!\n",
    "\n",
    "- toc: true \n",
    "- badges: true\n",
    "- comments: true\n",
    "- categories: [jupyter]\n",
    "- image: images/stylegan2-jeremy.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Introduction & Disclaimers\n",
    "\n",
    "Welcome!\n",
    "\n",
    "This notebook is an introduction to the concept of latent space, using a recent (and amazing) generative network: [StyleGAN2](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "Here are some great blog posts I found useful when learning about the latent space + StyleGAN2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Latent Space Understanding: [Ekin Tiu - Understanding Latent Space in Machine Learning](https://towardsdatascience.com/understanding-latent-space-in-machine-learning-de5a7c687d8d)\n",
    "\n",
    "\n",
    "- A technical overview of the stylegan2 architecture: [Connor Shorten - StyleGAN2](https://towardsdatascience.com/stylegan2-ace6d3da405d)\n",
    "\n",
    "- Overview of GANs + what's changed up to StyleGAN2: [akira - From GAN basic to StyleGAN2](https://medium.com/analytics-vidhya/from-gan-basic-to-stylegan2-680add7abe82)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: This stand-alone Jupyter notebook is all you need to get everything up and running! It'll pull a (small) repo with everything that's needed :) The Repo's README.md contains original sources for the content! [Link to Repo](https://github.com/AmarSaini/Epoching_StyleGan2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: This notebook was successfully ran on Gradient Paperspace's TensorFlow 1.14 Container. If you're interested in running this notebook yourself, I highly recommend running it on gradient paperspace :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Warning: This tutorial is not too code-heavy in the deep learning/model aspects. Primarily because this tutorial uses the [Official StyleGan2 Repo](https://github.com/NVlabs/stylegan2), which uses a depreciated version of Tensorflow (1.14). This blog post abstracts away from the depreciated TensorFlow code, and focuses more on the concepts of latent space traversals :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Experiment Layout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In this notebook, we will be experimenting with the following:\n",
    "\n",
    "1. A Quick Review of GANs.\n",
    "  - A quick refresh of: `z` (latent vectors), `generators`, and `discriminators`.\n",
    "  - GANs vs VAEs\n",
    "\n",
    "2. Generate Images of People who don't Exist\n",
    "  - Example: Use the official StyleGAN2 repo to create `generator` outputs.\n",
    "  - Example: View the latent codes of these generated outputs.\n",
    "  \n",
    "3. Interpolation of Latent Codes.\n",
    "  - How?\n",
    "  - Example: Use the previous `generator` outputs' *latent codes* to \"morph\" images of people together.\n",
    "  \n",
    "4. Facial Image Alignment using Landmark Detection.\n",
    "  - Why?\n",
    "  - Example: Aligning (normalizing) our own input images for projection.\n",
    "  \n",
    "5. Projecting our own Input Images into the Latent Space.\n",
    "  - Why, and how?\n",
    "  - Example: Learning the latent codes of our new aligned input images.\n",
    "  - Example: Interpolation of projected latent codes. (Similar to Section 2, but with our images!)\n",
    "  \n",
    "6. Latent Directions/Controls to modify our projected images.\n",
    "  - What, and How?\n",
    "  - Example: Using pre-computed latent directions to alter facial features of our own images.\n",
    "  \n",
    "7. Bonus: Interactive Widget-App!\n",
    "  - Play with latent controls yourself using this little jupyter app I built using ipywidgets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 1. A Quick Review of GANs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "I'm going to try to keep this section short, and just go over the needed information to understand the rest of this post:\n",
    "\n",
    "GANs (Generative Adversarial Networks) consist of two models:\n",
    "  - The `Generator`: A model that converts a *latent code* into some kind of output (an image of a person, in our case).\n",
    "  - The `Discriminator`: A model that determines whether some input (an image of a person), is real or fake.\n",
    "    - Real: An image from the original dataset.\n",
    "    - Fake: An image from the `Generator`.\n",
    "    \n",
    "The input to a `Generator` is a *latent code*, a vector of numbers if you will. (Such as a vector of 512 numbers).\n",
    "  - During training, the *latent code* is randomly sampled (i.e. a random vector of 512 numbers).\n",
    "  - When this *latent code* is randomly sampled, we can call it a *latent random variable*, as shown in the figure below.\n",
    "  - This magical *latent code* holds information that will allow the `Generator` to create a specific output.\n",
    "  - If you can find a *latent code* for a particular input, you can represent it with smaller amounts of data! (Such as representing a picture of someone with only a *latent vector* of 512 numbers, as opposed to the original image size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Important: Don't confuse GANs with VAEs (Variational Auto-Encoders)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: GANs learn to generate outputs **from random latent vectors** that micmic the appearance of your input data. VAEs learn to **encode your input into a latent vector**, and then also learn to decode latent vectors back to it's (mostly) original form."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: The main difference to takeaway from GANs vs VAEs, is that our `Generator` actually never sees the input images, hence we don't have a way to *automatically* convert images into it's corresponding latent code! **Teaser:** That's what projection is for, section 4 :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/GYlwzLw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Source: https://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-generative-models-and-adversarial-training-upc-2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 2. Generate Images of People who don't Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import sys\n",
    "sys.path.append('stylegan2/')\n",
    "\n",
    "from stylegan2 import pretrained_networks\n",
    "from stylegan2 import dnnlib\n",
    "from stylegan2.dnnlib import tflib\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
    "fps = 20\n",
    "results_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "\n",
    "# Code to load the StyleGAN2 Model\n",
    "def load_model():\n",
    "    _G, _D, Gs = pretrained_networks.load_networks(model_path)\n",
    "    \n",
    "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
    "    \n",
    "    Gs_kwargs = dnnlib.EasyDict()\n",
    "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    Gs_kwargs.randomize_noise = False\n",
    "    \n",
    "    return Gs, noise_vars, Gs_kwargs\n",
    "\n",
    "# Generate images given a random seed (Integer)\n",
    "def generate_image_random(rand_seed):\n",
    "    rnd = np.random.RandomState(rand_seed)\n",
    "    z = rnd.randn(1, *Gs.input_shape[1:])\n",
    "    tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars})\n",
    "    images = Gs.run(z, None, **Gs_kwargs)\n",
    "    return images, z\n",
    "\n",
    "# Generate images given a latent code ( vector of size [1, 512] )\n",
    "def generate_image_from_z(z):\n",
    "    images = Gs.run(z, None, **Gs_kwargs)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lets go ahead and start generating some outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Loading the StyleGAN2 Model!\n",
    "Gs, noise_vars, Gs_kwargs = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Ask the generator to make an output, given a random seed number: 42\n",
    "images, latent_code1 = generate_image_random(42)\n",
    "image1 = Image.fromarray(images[0]).resize((results_size, results_size))\n",
    "latent_code1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: As shown in the previous cell's output, we can see that the latent_code for this output is of size `(1, 512)`. This means that the numbers inside `latent_code1` can be used to create the image below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's make another image!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Ask the generator to make an output, given a random seed number: 1234\n",
    "images, latent_code2 = generate_image_random(1234)\n",
    "image2 = Image.fromarray(images[0]).resize((results_size, results_size))\n",
    "latent_code2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_code1[0][:5], latent_code2[0][:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: We can see the size of `latent_code2` is also `(1, 512)`. However, the two codes are not the same! This is seen in the first five values in the previous cell :). Below is the corresponding image for generating output with `latent_code2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "image2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 3. Interpolation of Latent Codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "So what's the big deal? We have two codes to make two people that don't even exist right? Well, the cool thing about the latent space is that you can \"traverse\" through it!\n",
    "\n",
    "Since the latent space is a compressed representation of some data, things are are similar in appearance should be \"close\" to each other in the latent space.\n",
    "\n",
    "If the latent space is well developed, we can actually transition between points in this space and create intermediate outputs of the endpoints!\n",
    "\n",
    "In other words... we can morph two people together! See gif below for an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/6MgNn3l.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Source: https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "def linear_interpolate(code1, code2, alpha):\n",
    "    return code1 * alpha + code2 * (1 - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's do this on our examples we just generated! :D.\n",
    "\n",
    "Let's interpolate halfway between latent_code1, latent_code2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "interpolated_latent_code = linear_interpolate(latent_code1, latent_code2, 0.5)\n",
    "interpolated_latent_code.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: The latent_code size is still a vector of 512 numbers; We just took 50% of `latent_code1` and 50% of `latent_code2`, (alpha=0.5), and summed them together! Below is the resulting image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "images = generate_image_from_z(interpolated_latent_code)\n",
    "Image.fromarray(images[0]).resize((results_size, results_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's also make a cool interpolation animation; It'll help with visualizing the effect of interpolating from `alpha=0` to `alpha=1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "output_gifs_path = Path('output_gifs')\n",
    "# Make Output Gifs folder if it doesn't exist.\n",
    "if not output_gifs_path.exists():\n",
    "    output_gifs_path.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "def get_concat_h(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst\n",
    "\n",
    "def make_latent_interp_animation(code1, code2, img1, img2, num_interps):\n",
    "    \n",
    "    step_size = 1.0/num_interps\n",
    "    \n",
    "    all_imgs = []\n",
    "    \n",
    "    amounts = np.arange(0, 1, step_size)\n",
    "    \n",
    "    for alpha in tqdm(amounts):\n",
    "        interpolated_latent_code = linear_interpolate(code1, code2, alpha)\n",
    "        images = generate_image_from_z(interpolated_latent_code)\n",
    "        interp_latent_image = Image.fromarray(images[0]).resize((400, 400))\n",
    "        frame = get_concat_h(img1, interp_latent_image)\n",
    "        frame = get_concat_h(frame, img2)\n",
    "        all_imgs.append(frame)\n",
    "\n",
    "    save_name = output_gifs_path/'latent_space_traversal.gif'\n",
    "    all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_latent_interp_animation(latent_code1, latent_code2, image1, image2, num_interps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: If you're running this notebook yourself, the interpolation gif will be saved in `output_gifs/latent_space_traversal.gif` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/todsmOo.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: This gif represents going from `latent_code1` to `latent_code2` by slowly changing alpha from 0 to 1. (increasing alpha by 1/200 per iteration, until it reaches 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 4. Facial Image Alignment using Landmark Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ok so this is all fun and stuff right? How could we play around with our own images, instead of random people that don't exist?\n",
    "\n",
    "Well, we first have to project our own images into this latent space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Important: The first step of projecting our own images is to make sure that they are representative of the training data. StyleGAN2 was trained on the [FFHQ Dataset](https://github.com/NVlabs/ffhq-dataset). More specifically, the images used during training were actually *aligned* first, before giving it to the discriminator in StyleGAN2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To align (normalize) our images for StyleGAN2, we need to use a landmark detection model. This will automatically find the facial keypoints of interest, and crop/rotate accordingly.\n",
    "\n",
    "Below is an example!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: At this point, if you want to run this with your own images, all you need to do is go to the `imgs/` folder, and delete the example images, `Jeremy_Howard.jpg` and `Obama.jpg`. Then upload 2 of your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "# One-Time Download of Facial Landmark Detection Model Weights\n",
    "if Path('shape_predictor_68_face_landmarks.dat') not in list(Path('.').iterdir()):\n",
    "    !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\n",
    "    !bzip2 -dv shape_predictor_68_face_landmarks.dat.bz2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "orig_img_path = Path('imgs')\n",
    "aligned_imgs_path = Path('aligned_imgs')\n",
    "\n",
    "# Make Aligned Images folder if it doesn't exist.\n",
    "if not aligned_imgs_path.exists():\n",
    "    aligned_imgs_path.mkdir()\n",
    "    \n",
    "orig_img_path, aligned_imgs_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "from align_face import align_face\n",
    "\n",
    "# Align all of our images using a landmark detection model!\n",
    "all_imgs = list(orig_img_path.iterdir())\n",
    "for img in all_imgs:\n",
    "    align_face(str(img)).save(aligned_imgs_path/('aligned_'+img.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's load the original + aligned images into Jupyter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "aligned_img_set = list(aligned_imgs_path.iterdir())\n",
    "aligned_img_set.sort()\n",
    "aligned_img_set = [Image.open(x) for x in aligned_img_set]\n",
    "\n",
    "orig_img_set = list(orig_img_path.iterdir())\n",
    "orig_img_set.sort()\n",
    "orig_img_set = [Image.open(x) for x in orig_img_set]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Original image is on the left, Aligned image is on the right. The image size for the original images aren't necessarily rectangular. However, the output size of the aligned images is always 1024x1024, a square! StyleGAN2 uses square shapes :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_concat_h(orig_img_set[0], aligned_img_set[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_concat_h(orig_img_set[1], aligned_img_set[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Important: Just from these visualizations, we can see that the Obama image has much higher resolution! Let's see how this affects things!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 5. Projecting our own Input Images into the Latent Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Warning: At this point, I restarted my notebook kernel to clear my GPU card memory. The previous sections loaded models *inside* our notebook, so I ended up restarting the kernel to free space. This is recommended because the next section will be invoking official stylegan2 python scripts from inside notebook cells, and they load models interally (separately from this notebook). You'll most likely run out of GPU memory if you don't restart! Everything in this notebook will still run nicely from top-to-bottom :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Automatically restart the kernel by running this cell\n",
    "import IPython\n",
    "IPython.Application.instance().kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "import sys\n",
    "sys.path.append('stylegan2/')\n",
    "\n",
    "from stylegan2 import pretrained_networks\n",
    "from stylegan2 import dnnlib\n",
    "from stylegan2.dnnlib import tflib\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_path = 'gdrive:networks/stylegan2-ffhq-config-f.pkl'\n",
    "fps = 20\n",
    "results_size = 400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: First, we'll make an official stylegan2 compatible dataset, to ignore the warnings from deprecated TensorFlow 1.14, we use the `-W ignore` option when executing python on the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "!python -W ignore stylegan2/dataset_tool.py create_from_images datasets/custom_imgs aligned_imgs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Now let's project our previously aligned images into the latent space, and get those latent codes that will hopefully well-represent the images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Projecting an image into the latent space basically means: *Let's figure out what latent code (512 numbers) will cause the generator to make an output that looks like our image*\n",
    "- The question is, how do we figure out the latent code? With VAEs (variational autoencoder), we just throw our image through the encoder and we get our latent code just like that!\n",
    "- With GANs, we don't necessarily have a direct way to extract latent codes from an input image, *but* we can optimize for it.\n",
    "- In a nut shell, here's how we can optimize for a latent code for given input images:\n",
    "\n",
    "For as many iterations as you'd like, do:\n",
    "\n",
    "1. Ask the generator to generate some output from a starting latent vector.\n",
    "2. Take the generator's output image, and take your target image, put them both through a VGG16 model (image feature extractor).\n",
    "3. Take the generator's output image features from the VGG16.\n",
    "4. Take the target image features from the VGG16.\n",
    "5. Compute the loss on the difference of features!\n",
    "6. Backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Important: Note that the loss between the generator's output image, and the target image, is in the *feature space* of a VGG16! Why? If we performed pixel-wise loss on just the raw image pixels, it won't account for any facial features, just color of pixels. Checking the difference of *features* allows us to optimize for a latent code that doesn't just check for pixel values, but also the patterns in the pixels that correspond to high-level features, such as cheekbones, eyebrows, nose size, eye width, smile, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "tot_aligned_imgs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# collapse-hide\n",
    "!python -W ignore stylegan2/epoching_custom_run_projector.py project-real-images --network=$model_path \\\n",
    "  --dataset=custom_imgs --data-dir=datasets --num-images=$tot_aligned_imgs --num-snapshots 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: This cell will take a while to run! 1000 iterations for optimizing latent codes (per image) are occuring here, and we're saving progress-images every 2 steps for a cool gif later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "def get_concat_h(im1, im2):\n",
    "    dst = Image.new('RGB', (im1.width + im2.width, im1.height))\n",
    "    dst.paste(im1, (0, 0))\n",
    "    dst.paste(im2, (im1.width, 0))\n",
    "    return dst\n",
    "\n",
    "def make_project_progress_gifs():\n",
    "    \n",
    "    all_result_folders = list(Path('results/').iterdir())\n",
    "    all_result_folders.sort()\n",
    "\n",
    "    last_result_folder = all_result_folders[-1]\n",
    "    \n",
    "    for img_num in range(tot_aligned_imgs):\n",
    "        all_step_pngs = [x for x in last_result_folder.iterdir() if x.name.endswith('png') and 'image{0:04d}'.format(img_num) in x.name]\n",
    "        all_step_pngs.sort()\n",
    "\n",
    "        target_image = Image.open(all_step_pngs[-1]).resize((results_size, results_size))\n",
    "\n",
    "        all_concat_imgs = []\n",
    "        for step_img_path in all_step_pngs[:-1]:\n",
    "            step_img = Image.open(step_img_path).resize((results_size, results_size))\n",
    "            all_concat_imgs.append(get_concat_h(target_image, step_img))\n",
    "\n",
    "        all_concat_imgs[0].save('output_gifs/image{0:04d}_project_progress.gif'.format(img_num), save_all=True, append_images=all_concat_imgs[1:], duration=1000/fps, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "make_project_progress_gifs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: If you're running this notebook yourself, the project progress gifs will be saved in `output_gifs/image####_project_progress.gif` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/JKkNXIK.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/HwZ6c1U.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's look at the optimized latent codes we have acquired through this projection process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "def get_final_latents():\n",
    "    all_results = list(Path('results/').iterdir())\n",
    "    all_results.sort()\n",
    "    \n",
    "    last_result = all_results[-1]\n",
    "\n",
    "    latent_files = [x for x in last_result.iterdir() if 'final_latent_code' in x.name]\n",
    "    latent_files.sort()\n",
    "    \n",
    "    all_final_latents = []\n",
    "    \n",
    "    for file in latent_files:\n",
    "        with open(file, mode='rb') as latent_pickle:\n",
    "            all_final_latents.append(pickle.load(latent_pickle))\n",
    "    \n",
    "    return all_final_latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "latent_codes = get_final_latents()\n",
    "len(latent_codes), latent_codes[0].shape, latent_codes[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: We now have two latent codes for our Jeremy Howard and Obama images! Notice how these codes are of the shape `(1, 18, 512)`, instead of the `(1, 512)` shape we saw earlier on the generated (fake) examples. This is due to one of the architecture designs of StyleGAN2, it actually re-iterates the base latent vector at different levels in the generator to allow for small deviations in the latent code to support variance in style. During training, just one static latent vector of the shape `(1, 512)` is used. For a more detailed explanation, check out the recommended technical StyleGAN2 overview blog posts mentioned in the introduction! :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "def load_model():\n",
    "    _G, _D, Gs = pretrained_networks.load_networks(model_path)\n",
    "    \n",
    "    noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith('noise')]\n",
    "    \n",
    "    Gs_kwargs = dnnlib.EasyDict()\n",
    "    Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True)\n",
    "    Gs_kwargs.randomize_noise = False\n",
    "    \n",
    "    return Gs, noise_vars, Gs_kwargs\n",
    "\n",
    "def generate_image_from_projected_latents(latent_vector):\n",
    "    images = Gs.components.synthesis.run(latent_vector, **Gs_kwargs)\n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Loading the StyleGAN2 Model!\n",
    "Gs, noise_vars, Gs_kwargs = load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's check out what our images look like from our latent codes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "output_gifs_path = Path('output_gifs/')\n",
    "aligned_imgs_path = Path('aligned_imgs/')\n",
    "aligned_img_set = list(aligned_imgs_path.iterdir())\n",
    "aligned_img_set.sort()\n",
    "aligned_img_set = [Image.open(x) for x in aligned_img_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Ask the generator to make an output, given a latent code we found from the projection process.\n",
    "images = generate_image_from_projected_latents(latent_codes[0])\n",
    "recreated_img1 = Image.fromarray(images[0]).resize((results_size, results_size))\n",
    "\n",
    "orig_img1 = aligned_img_set[1].resize((results_size, results_size))\n",
    "get_concat_h(orig_img1, recreated_img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "# Ask the generator to make an output, given a latent code we found from the projection process.\n",
    "images = generate_image_from_projected_latents(latent_codes[1])\n",
    "recreated_img2 = Image.fromarray(images[0]).resize((results_size, results_size))\n",
    "\n",
    "orig_img2 = aligned_img_set[0].resize((results_size, results_size))\n",
    "get_concat_h(orig_img2, recreated_img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Not bad at all!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now let's re-run the interpolation animation we did back in section 2, but this time with our own projected latent codes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "def linear_interpolate(code1, code2, alpha):\n",
    "    return code1 * alpha + code2 * (1 - alpha)\n",
    "\n",
    "def make_latent_interp_animation_real_faces(code1, code2, img1, img2, num_interps):\n",
    "    \n",
    "    step_size = 1.0/num_interps\n",
    "    \n",
    "    all_imgs = []\n",
    "    \n",
    "    amounts = np.arange(0, 1, step_size)\n",
    "    \n",
    "    for alpha in tqdm(amounts):\n",
    "        interpolated_latent_code = linear_interpolate(code1, code2, alpha)\n",
    "        images = generate_image_from_projected_latents(interpolated_latent_code)\n",
    "        interp_latent_image = Image.fromarray(images[0]).resize((400, 400))\n",
    "        frame = get_concat_h(img2, interp_latent_image)\n",
    "        frame = get_concat_h(frame, img1)\n",
    "        all_imgs.append(frame)\n",
    "\n",
    "    save_name = output_gifs_path/'projected_latent_space_traversal.gif'\n",
    "    all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_latent_interp_animation_real_faces(latent_codes[0], latent_codes[1], recreated_img1, recreated_img2, num_interps=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: If you're running this notebook yourself, the interpolation gif will be saved in `output_gifs/projected_latent_space_traversal.gif` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/rtrDqNd.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: This gif represents going from Jeremy's latent code to Obama's latent code by slowly changing alpha from 0 to 1. (increasing alpha by 1/200 per iteration, until it reaches 1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 6. Latent Directions/Controls to modify our projected images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Time to be an astronaut and explore space! Well, the hidden (latent) kind of space. Alright... I admit, that joke was blatant. Sorry for the puns, I'm just trying to re**late** things together. Ok ok ok, you need some *space*, got it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "There are ways to learn latent directions (both supervised, and unsupervised) in the latent space to control features. In other words, people have found, and open-sourced, directional latent vectors for StyleGAN2 that allow us to \"move\" in the latent space and control a particular feature.\n",
    "\n",
    "To move in a latent direction we can do the following operation:\n",
    "`latent_code = latent_code + latent_direction * magnitude`\n",
    "\n",
    "- `latent_code` is our latent code, such as our recently optimized latent code!\n",
    "- `latent_direction` is a learnt directional vector that is of shape `(18, 512)`. This vector tells you *where* to move in the latent space to control a feature, but not *how much* to move.\n",
    "- `magnitude` is the amount to move in the direction of `latent_direction`\n",
    "\n",
    "This means we can create more interpolations in the latent space! Yay, more animations :). Only this time, rather than interpolating between to points, we are slowing moving in a specific latent_direction.\n",
    "\n",
    "- Instead of mixing two latent codes together, we slowly add more magnitude to our base latent code, and observe how it changes with respect to magnitude!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some references for learning these latent directions:\n",
    "\n",
    "- \n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "def get_control_latent_vectors(path):\n",
    "    files = [x for x in Path(path).iterdir() if str(x).endswith('.npy')]\n",
    "    latent_vectors = {f.name[:-4]:np.load(f) for f in files}\n",
    "    return latent_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "latent_controls = get_control_latent_vectors('stylegan2directions/')\n",
    "len(latent_controls), latent_controls.keys(), latent_controls['age'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: We have 16 latent \"controls\" we can play around with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "def make_latent_control_animation(feature, start_amount, end_amount, step_size, person):\n",
    "    \n",
    "    all_imgs = []\n",
    "    \n",
    "    amounts = np.linspace(start_amount, end_amount, abs(end_amount-start_amount)/step_size)\n",
    "    \n",
    "    for amount_to_move in tqdm(amounts):\n",
    "        modified_latent_code = np.array(latent_code_to_use)\n",
    "        modified_latent_code += latent_controls[feature]*amount_to_move\n",
    "        images = generate_image_from_projected_latents(modified_latent_code)\n",
    "        latent_img = Image.fromarray(images[0]).resize((results_size, results_size))\n",
    "        all_imgs.append(get_concat_h(image_to_use, latent_img))\n",
    "\n",
    "    save_name = output_gifs_path/'{0}_{1}.gif'.format(person, feature)\n",
    "    all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "latent_code_to_use = latent_codes[1]\n",
    "image_to_use = recreated_img2\n",
    "make_latent_control_animation(feature='age', start_amount=-10, end_amount=10, step_size=0.1, person='jeremy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#collapse-hide\n",
    "latent_code_to_use = latent_codes[0]\n",
    "image_to_use = recreated_img1\n",
    "make_latent_control_animation(feature='age', start_amount=-5, end_amount=5, step_size=0.1, person='obama')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: If you're running this notebook yourself, the latent control interpolation gifs will be saved in `output_gifs/person_feature.gif` :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Here are 5 traversals in the latent space using latent directions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Aging Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/6yl9Vfi.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Female Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/Mm59Qa8.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Closing Mouth Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/JvynpwL.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Aging Obama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/dRFrwSs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Smiling Obama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/P1V0Ts3.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 7. Bonus: Interactive Widget-App!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#input-hide\n",
    "def apply_latent_controls(self):\n",
    "    \n",
    "    image_outputs = controller.children[0]\n",
    "    feature_sliders = controller.children[1]\n",
    "    \n",
    "    slider_hboxes = feature_sliders.children[:-2]\n",
    "    latent_movements = [(x.children[1].value, x.children[0].value) for x in slider_hboxes]\n",
    "\n",
    "    modified_latent_code = np.array(latent_code_to_use)\n",
    "    for feature, amount_to_move in latent_movements:\n",
    "        modified_latent_code += latent_controls[feature]*amount_to_move\n",
    "\n",
    "    images = generate_image_from_projected_latents(modified_latent_code)\n",
    "    latent_img = Image.fromarray(images[0]).resize((400, 400))\n",
    "    \n",
    "    latent_img_output = image_outputs.children[1]\n",
    "    with latent_img_output:\n",
    "        latent_img_output.clear_output()\n",
    "        display(latent_img)\n",
    "\n",
    "def reset_latent_controls(self):\n",
    "    \n",
    "    image_outputs = controller.children[0]\n",
    "    feature_sliders = controller.children[1]\n",
    "    \n",
    "    slider_hboxes = feature_sliders.children[:-2]\n",
    "    for x in slider_hboxes:\n",
    "        x.children[0].value = 0\n",
    "        \n",
    "    latent_img_output = image_outputs.children[1]\n",
    "    with latent_img_output:\n",
    "        latent_img_output.clear_output()\n",
    "        display(image_to_use)\n",
    "\n",
    "def create_interactive_latent_controller():\n",
    "    orig_img_output = widgets.Output()\n",
    "\n",
    "    with orig_img_output:\n",
    "        orig_img_output.clear_output()\n",
    "        display(image_to_use)\n",
    "\n",
    "    latent_img_output = widgets.Output()\n",
    "\n",
    "    with latent_img_output:\n",
    "        latent_img_output.clear_output()\n",
    "        display(image_to_use)\n",
    "\n",
    "    image_outputs = widgets.VBox([orig_img_output, latent_img_output])\n",
    "\n",
    "    #collapse-hide\n",
    "    generate_button = widgets.Button(description='Generate', layout=widgets.Layout(width='75%', height='10%'))\n",
    "    generate_button.on_click(apply_latent_controls)\n",
    "\n",
    "    reset_button = widgets.Button(description='Reset Latent Controls', layout=widgets.Layout(width='75%', height='10%'))\n",
    "    reset_button.on_click(reset_latent_controls)\n",
    "\n",
    "    feature_sliders = []\n",
    "    for feature in latent_controls:\n",
    "        label = widgets.Label(feature)\n",
    "        slider = widgets.FloatSlider(min=-50, max=50)\n",
    "        feature_sliders.append(widgets.HBox([slider, label]))\n",
    "    feature_sliders.append(generate_button)\n",
    "    feature_sliders.append(reset_button)\n",
    "    feature_sliders = widgets.VBox(feature_sliders)\n",
    "\n",
    "    return widgets.HBox([image_outputs, feature_sliders])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Tip: Try running this widget-app yourself to see how well-fit some of the latent directions/controls are, and how loose others are!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "latent_code_to_use = latent_codes[0]\n",
    "image_to_use = recreated_img1\n",
    "\n",
    "controller = create_interactive_latent_controller()\n",
    "controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#collapse-show\n",
    "latent_code_to_use = latent_codes[1]\n",
    "image_to_use = recreated_img2\n",
    "\n",
    "controller = create_interactive_latent_controller()\n",
    "controller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Old Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/5IWcFcp.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Young Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/OlVnW0J.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Female Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/qRwg3Lm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Close Mouth Jeremy:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/DJX11cU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Old Obama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/i3oqzcF.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Young Obama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/jkxMHxU.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Close Mouth Obama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/G9LYGYf.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "> Note: Smiling Obama:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "![](https://i.imgur.com/ezJwmNa.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  
    
        "post0": {
            "title": "Self-Supervision with FastAI",
            "content": "Intro . This notebook is an introduction to self-supervised learning. In short, self-supervised learning has 2 components: . Pretrain on a pretext task, where the labels can come from the data itself! | Transfer the features, and train on the actual classification labels!&quot;What if we can get labels for free for unlabelled data and train unsupervised dataset in a supervised manner? We can achieve this by framing a supervised learning task in a special form to predict only a subset of information using the rest. In this way, all the information needed, both inputs and labels, has been provided. This is known as self-supervised learning.&quot; - Lilian Weng . | Using FastAI2, we&#39;ll use rotation as a pretext task for learning representations/features of our data. . Here are some great overviews of self-supervised learning that I&#39;ve come across: Lilian Weng - Self-Supervised Representation Learning Jeremy Howard - Self-supervised learning and computer vision . Experiment Layout . In this notebook, we will be using the MNIST dataset. . Also check out ImageWang from FastAI themselves! It&#39;s a dataset designed for self-supervision tasks! . Train a model on a rotation prediction task. . We will use all the training data for rotation prediction. | Input: A rotated image. | Target/Label: Classify the amount of degrees rotated. | Our model should learn useful features that can transfer well for a classification task. | (The model should learn what digits look like in order to be able to successfully predict the amount of rotation). | . | Transfer our rotation pretraining features to solve the classification task with much fewer labels, &lt; 1% of the original data. . Input: A normal image. | Target/Label: The images true categorical label. | Classification accuracy should be decent, even with only using &lt; 1% of the original data. | . | Train a classifier from scratch on the same amount of data used in experiment 2. . Input: A normal image. | Target/Label: The images true categorical label. | Classification accuracy should be low (lack of transfer learning &amp; too few labeled data!) | Model may overfit. | . | FastAI Vision Model Creation Methods . from fastai2.vision.all import * . . Important: Pay attention! It&#8217;s important. We will be using a small ConvNet to test our self-supervised learning method. The architecture is defined below in simple_arch. . Note that simple_arch takes in one argument, pretrained. This is to allow FastAI to pass pretrained=True or pretrained=False when creating the model body! Below are some use cases of when we would want pretrained=True or pretrained=False. . pretrained=False = For training a new model on our rotation task. | pretrained=True = For transferring the learnt features from our rotation task pretraining to solve a classification task. | pretrained=False = For training a new model from scratch on the main classification task (no transfer learning). | #collapse-show def simple_arch(pretrained=False): # Note that FastAI will automatically cut at pooling layer for the body! model = nn.Sequential( nn.Conv2d(1, 4, 3, 1), nn.BatchNorm2d(4), nn.ReLU(), nn.Conv2d(4, 16, 3, 1), nn.BatchNorm2d(16), nn.ReLU(), nn.Conv2d(16, 32, 3, 1), nn.BatchNorm2d(32), nn.AdaptiveAvgPool2d(1), ) if (pretrained): print(&quot;Loading pretrained model...&quot;) pretrained_weights = torch.load(save_path/&#39;rot_pretrained.pt&#39;) print(model.load_state_dict(pretrained_weights)) return model . . The follow below code snippets are examples of how FastAI creates CNNs. Every model will have a body and a head . #collapse-hide body = create_body(arch=simple_arch, pretrained=False) body . . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . #collapse-hide head = create_head(nf=32*2, n_out=8, lin_ftrs=[]) head . . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) . #collapse-hide # Note that FastAI automatically determines nf for the head! model = create_cnn_model(arch=simple_arch, pretrained=False, n_out=8, lin_ftrs=[]) model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) ) . PyTorch Rotation/Classification Self-Supervised Dataset . # Functions to convert between Torch Tensors and PIL Images import torchvision tensorToImage = torchvision.transforms.ToPILImage() imageToTensor = torchvision.transforms.ToTensor() . # Download MNIST dataset from PyTorch if not downloaded already! torchvision.datasets.MNIST(&#39;data/&#39;, download=&#39;True&#39;) . Dataset MNIST Number of datapoints: 60000 Root location: data/ Split: Train . Below we define a dataset, here&#39;s the docstring: . A Dataset for Rotation-based Self-Supervision! Images are rotated clockwise. . file - MNIST processed .pt file. | pct - percent of data to use | classification - False = Use rotation labels. True = Use original classification labels. | . #collapse-hide class Custom_Dataset_MNIST(): &#39;&#39;&#39; A Dataset for Rotation-based Self-Supervision! Images are rotated clockwise. - file - MNIST processed .pt file. - pct - percent of data to use - classification - False = Use rotation labels. True = Use original classification labels. &#39;&#39;&#39; def __init__(self, file, pct, classification): data = torch.load(file) self.imgs = data[0] self.labels = data[1] self.pct = pct self.classification = classification slice_idx = int(len(self.imgs)*self.pct) self.imgs = self.imgs[:slice_idx] def __len__(self): return len(self.imgs) def __getitem__(self, idx): img = self.imgs[idx].unsqueeze(0) img = tensorToImage(img) img = img.resize((32, 32), resample=1) img = imageToTensor(img) if (not self.classification): # 4 classes for rotation degrees = [0, 45, 90, 135, 180, 225, 270, 315] rand_choice = random.randint(0, len(degrees)-1) img = tensorToImage(img) img = img.rotate(degrees[rand_choice]) img = imageToTensor(img) return img, torch.tensor(rand_choice).long() return img, self.labels[idx] def show_batch(self, n=3): fig, axs = plt.subplots(n, n) fig.tight_layout() for i in range(n): for j in range(n): rand_idx = random.randint(0, len(self)-1) img, label = self.__getitem__(rand_idx) axs[i, j].imshow(tensorToImage(img), cmap=&#39;gray&#39;) if self.classification: axs[i, j].set_title(&#39;Label: {0} (Digit #{1})&#39;.format(label.item(), label.item())) else: axs[i, j].set_title(&#39;Label: {0} ({1} Degrees)&#39;.format(label.item(), label.item()*45)) axs[i, j].axis(&#39;off&#39;) . . Pretraining with Rotation Task . . Note: 60k training data and 10k validation data! . # Make rotation datasets train_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/training.pt&#39;, pct=1.0, classification=False) valid_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/test.pt&#39;, pct=1.0, classification=False) print(&#39;{0} Training Samples | {1} Validation Samples&#39;.format(len(train_ds), len(valid_ds))) . 60000 Training Samples | 10000 Validation Samples . . Note: Notice that our labels don&#8217;t correspond to digits! They correspond to the amount of degrees rotated! Specifically from this predefined set: [0, 45, 90, 135, 180, 225, 270, 315] . from fastai2.data.core import DataLoaders dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() # Override the show_batch function of dls to the one used in our dataset! dls.show_batch = train_ds.show_batch # We have 8 classes! [0, 1, 2, 3, 4, 5, 6, 7] that correspond to the [0, 45, 90, 135, 180, 225, 270, 315] degrees of rotation. dls.c = 8 dls.show_batch() . FastAI Vision Learner [Rotation] . # Create a config for our model&#39;s head! head_config = cnn_config(lin_ftrs=[]) head_config . {&#39;lin_ftrs&#39;: []} . . Note: We want to measure top_2_accuracy along with regular (top_1) accuracy, because there are hard-cases where it&#8217;s understandable why our model got it wrong. For example: A &#8217;0&#8217; rotated 90 or 270 degrees, or a &#8217;1&#8217; rotated 0 or 180 degrees? . # Top_2 accuracy is a nice metric for hard-cases: # - A zero rotated 90 or 270 degrees? # - A one rotated 0 or 180 degrees? # etc :P top_2_accuracy = lambda inp, targ: top_k_accuracy(inp, targ, k=2) top_2_accuracy . &lt;function __main__.&lt;lambda&gt;(inp, targ)&gt; . Here, we train a model on the rotation prediction task! . #collapse-show # Note to set a value for lin_ftrs, we use the defined config above. learner = cnn_learner(dls, simple_arch, pretrained=False, loss_func=CrossEntropyLossFlat(), config=head_config, metrics=[accuracy, top_2_accuracy]) learner.model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) ) . #collapse-hide learner.summary() . . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 True ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 True ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 True ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 8 512 True ________________________________________________________________ Total params: 6,016 Total trainable params: 6,016 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . #collapse-show learner.lr_find() . . (0.09120108485221863, 3.6307804407442745e-07) . #collapse-show learner.fit_one_cycle(5, lr_max=3e-2) . . epoch train_loss valid_loss accuracy &lt;/th&gt; time &lt;/tr&gt; &lt;/thead&gt; 0 | 0.931490 | 1.331268 | 0.487900 | 0.822600 | 00:11 | . 1 | 0.824483 | 0.871249 | 0.617000 | 0.901600 | 00:11 | . 2 | 0.757743 | 0.984435 | 0.570000 | 0.868800 | 00:11 | . 3 | 0.622819 | 0.568738 | 0.757100 | 0.962500 | 00:11 | . 4 | 0.578483 | 0.489926 | 0.799200 | 0.972800 | 00:11 | . &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Important: We were able to achieve 79.9% top-1 accuracy, and 97.3% top-2 accuracy after just 5 epochs! Now we want to grab our model from our Learner, and save the body of it! . . Note: Our model has two components, the body and the head. model is a list of size 2, where model[0] is the body, and model[1] is the head! . #collapse-show # Access the body of our model trained_body = learner.model[0] trained_body . . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . . Tip: To save a model in PyTorch, save it&#8217;s state_dict function! You can use model.load_state_dict to re-load the weights. . # Make save directory if it doesn&#39;t exist save_path = Path(&#39;rotation_cps/&#39;) if not save_path.exists(): save_path.mkdir() # Save the rotation-pretraining weights of our model body torch.save(trained_body.state_dict(), save_path/&#39;rot_pretrained.pt&#39;) . Transferring to Classification Data . Now that we have pretrained our model on the rotation prediction task, we want to switch over to the original labeled data for the classification task. . . Note: We&#8217;re only using 180 samples for training! . # Use 0.3% classification labeled data for training! # Use 100% classification labeled data for validation! train_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/training.pt&#39;, pct=0.003, classification=True) valid_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/test.pt&#39;, pct=1.0, classification=True) print(&#39;{0} Training Samples | {1} Validation Samples&#39;.format(len(train_ds), len(valid_ds))) . 180 Training Samples | 10000 Validation Samples . . Note: Notice the labels now correspond to the digit class! . from fastai2.data.core import DataLoaders dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() dls.show_batch = train_ds.show_batch # We have 10 classes! One for each digit label! dls.c = 10 dls.show_batch() . FastAI Vision Learner [Transfer-Classification] . Here we will toggle pretrained=True to transfer our rotation prediction features, and train on the original 180 labeled data. . #collapse-show # pretrained=True will load the saved rotation pretraining weights into our model&#39;s body! # See simple_arch() function definition for more details! learner = cnn_learner(dls, simple_arch, pretrained=True, loss_func=CrossEntropyLossFlat(), config=head_config, metrics=[accuracy]) learner.model . . Loading pretrained model... &lt;All keys matched successfully&gt; . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) ) . . Tip: Freezing a model&#8217;s body after transferring the weights over, allows the new head to get calibrated with the rest of the model! . learner.freeze() . . Note: Looking at the model summary, we can see that the model is frozen up to the new head! Good! . #collapse-show learner.summary() . . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 False ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 False ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 False ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 10 640 True ________________________________________________________________ Total params: 6,144 Total trainable params: 872 Total non-trainable params: 5,272 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . #collapse-show learner.lr_find() . . (0.06309573650360108, 0.0691830962896347) . #collapse-show learner.fit_one_cycle(10, lr_max=3e-2) . . epoch train_loss valid_loss accuracy time . 0 | 3.503277 | 5.978975 | 0.097400 | 00:02 | . 1 | 3.477977 | 5.445812 | 0.097400 | 00:02 | . 2 | 3.231445 | 4.004679 | 0.097400 | 00:02 | . 3 | 2.933150 | 3.224835 | 0.101100 | 00:02 | . 4 | 2.651888 | 2.746891 | 0.164300 | 00:02 | . 5 | 2.403430 | 2.261896 | 0.232900 | 00:01 | . 6 | 2.231838 | 2.060806 | 0.278600 | 00:02 | . 7 | 2.078108 | 1.910456 | 0.323700 | 00:02 | . 8 | 1.946290 | 1.764125 | 0.386200 | 00:02 | . 9 | 1.845896 | 1.637488 | 0.442200 | 00:02 | . . Tip: Unfreeze the model after calibrating the new head with the transferred body, and train a little more! . learner.unfreeze() . #collapse-hide learner.summary() . . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 True ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 True ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 True ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 10 640 True ________________________________________________________________ Total params: 6,144 Total trainable params: 6,144 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . #collapse-show learner.lr_find() . . (0.025118863582611083, 3.6307804407442745e-07) . #collapse-show learner.fine_tune(5, base_lr=3e-2) . . epoch train_loss valid_loss accuracy time . 0 | 0.995149 | 1.549676 | 0.479500 | 00:02 | . epoch train_loss valid_loss accuracy time . 0 | 0.982638 | 1.478698 | 0.512300 | 00:02 | . 1 | 0.994823 | 1.416823 | 0.541200 | 00:02 | . 2 | 1.004630 | 1.354943 | 0.569300 | 00:02 | . 3 | 1.007987 | 1.292374 | 0.597100 | 00:02 | . 4 | 0.976209 | 1.245139 | 0.617300 | 00:01 | . . Important: We were able to get 61.7% accuracy using transfer learning from our pretraining on the rotation prediction task! . FastAI Vision Learner [Fully-Classification] . Here we train a model from scratch on the original 180 labeled data. . #collapse-show # pretrained=False, Create the same model as before, but without using the rotation pretraining weights! learner = cnn_learner(dls, simple_arch, pretrained=False, loss_func=CrossEntropyLossFlat(), config=head_config, metrics=[accuracy]) learner.model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) ) . #collapse-hide learner.summary() . . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 True ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 True ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 True ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 10 640 True ________________________________________________________________ Total params: 6,144 Total trainable params: 6,144 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . #collapse-show learner.lr_find() . . (0.05248074531555176, 0.0010000000474974513) . #collapse-show learner.fit_one_cycle(10, lr_max=3e-2) . . epoch train_loss valid_loss accuracy time . 0 | 3.452045 | 2.400267 | 0.098200 | 00:02 | . 1 | 2.946759 | 2.611470 | 0.098200 | 00:02 | . 2 | 2.596977 | 3.005997 | 0.098200 | 00:01 | . 3 | 2.313555 | 3.860830 | 0.098200 | 00:02 | . 4 | 2.055550 | 4.996419 | 0.098200 | 00:02 | . 5 | 1.897385 | 5.686416 | 0.098200 | 00:02 | . 6 | 1.752764 | 5.904824 | 0.098200 | 00:02 | . 7 | 1.620871 | 6.086377 | 0.098200 | 00:02 | . 8 | 1.537340 | 6.286355 | 0.100200 | 00:02 | . 9 | 1.469650 | 6.423689 | 0.130200 | 00:02 | . . Important: We were able to only get 13% accuracy with training from scratch . Conclusion . Self-supervised learning can help find rich features you could use for other down-stream tasks! . Be sure to try other self-supervised learning methods (or perhaps find your own!) and compete on the ImageWang Leadboard! . &lt;/div&gt; .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html",
            "relUrl": "/jupyter/2020/03/23/Self-Supervision-with-FastAI.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # Title &gt; Awesome summary - toc: true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://amarsaini.github.io/Epoching-Blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This is where you put the contents of your About page. Like all your pages, it’s in Markdown format. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://amarsaini.github.io/Epoching-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}
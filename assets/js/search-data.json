{
  
    
        "post0": {
            "title": "Latent Space Exploration with StyleGAN2",
            "content": "Introduction &amp; Disclaimers . Welcome! . This notebook is an introduction to the concept of latent space, using a recent (and amazing) generative network: StyleGAN2 . Here are some great blog posts I found useful when learning about the latent space + StyleGAN2 . Latent Space Understanding: Ekin Tiu - Understanding Latent Space in Machine Learning | . A technical overview of the stylegan2 architecture: Connor Shorten - StyleGAN2 . | Overview of GANs + what&#39;s changed up to StyleGAN2: akira - From GAN basic to StyleGAN2 . | . . Tip: This stand-alone Jupyter notebook is all you need to get everything up and running! It&#8217;ll pull a (small) repo with everything that&#8217;s needed :D The Repo&#8217;s README.md contains original source links for the content! Link to Repo . . Tip: This notebook was successfully ran on Google Colab &amp; Gradient Paperspace&#8217;s TensorFlow 1.14 Container: every cell, from top-to-bottom! Feel free to run/experiment with this notebook yourselves! . . Warning: This tutorial is not too code-heavy in the deep learning/model aspects. Primarily because this tutorial uses the Official StyleGan2 Repo, which uses a depreciated version of Tensorflow (1.14). This blog post abstracts away from the depreciated TensorFlow code, and focuses more on the concepts of latent space and traversals :) . Experiment Layout . In this notebook, we will be experimenting with the following: . 0. Set-Up (Run only once!) . This section simply pulls the small repo containing necessary files needed to run things! | . 1. A Quick Review of GANs . A quick refresh of: z (latent vectors), Generators, and Discriminators. | GANs vs VAEs | . 2. Generate Images of People who don&#39;t Exist . Use the official StyleGAN2 repo to create Generator outputs. | View the latent codes of these generated outputs. | . 3. Interpolation of Latent Codes . Use the previous Generator outputs&#39; latent codes to morph images of people together. | . 4. Facial Image Alignment using Landmark Detection . Aligning (normalizing) our own input images for latent space projection. | . 5. Projecting our own Input Images into the Latent Space . Learning the latent codes of our new aligned input images. | Interpolation of projected latent codes. (Similar to Section 3, but with our images!) | . 6. Latent Directions/Controls to modify our projected images . Using pre-computed latent directions to alter facial features of our own images. | . 7. Bonus: Interactive Widget-App! . Play with latent controls yourself using this little jupyter app I built using ipywidgets. | . 0. Set-Up (Run only once!) . Clone Repo and extract contents . !git clone https://github.com/AmarSaini/Epoching_StyleGan2_Setup.git . import shutil from pathlib import Path repo_root = Path(&#39;Epoching_StyleGan2_Setup/&#39;) # Pull contents out of the repo, into our current directory. for content in repo_root.iterdir(): shutil.move(str(content), &#39;.&#39;) shutil.rmtree(repo_root) . Pip install needed packages . # These Python packages are the only thing missing from Gradient Paperspace&#39;s TensorFlow 1.14 container! !pip install requests !pip install Pillow !pip install tqdm !pip install dlib . If you&#39;re running this on Google Colab, uncomment and run the following cell: . #!pip install tensorflow==1.14 #!pip install tensorflow-gpu==1.14 . 1. A Quick Review of GANs . I&#39;m going to try to keep this section short, and just go over the needed information to understand the rest of this post: . GANs (Generative Adversarial Networks) consist of two models: . The Generator: A model that converts a latent code into some kind of output (an image of a person, in our case). | The Discriminator: A model that determines whether some input (an image of a person), is real or fake. Real: An image from the original dataset. | Fake: An image from the Generator. | . | . The input to a Generator is a latent code z, a vector of numbers if you will. (Such as: a vector of 512 numbers). . During training, the latent code is randomly sampled (i.e. a random vector of 512 numbers). | When this latent code is randomly sampled, we can call it a latent random variable, as shown in the figure below. | This magical latent code holds information that will allow the Generator to create a specific output. | If you can find a latent code for a particular input, you can represent it with smaller amounts of data! (Such as representing a picture of someone with only a latent vector of 512 numbers, as opposed to the original image size) | . . Important: Don&#8217;t confuse GANs with VAEs (Variational Auto-Encoders)! . . Note: GANs learn to generate outputs from random latent vectors that mimic the appearance of your input data, but not necessarily the exact samples of your input data. VAEs learn to encode your input samples into latent vectors, and then also learn to decode latent vectors back to it&#8217;s (mostly) original form. . . Tip: The main difference to takeaway from GANs vs VAEs, is that our Generator actually never sees the input images, hence we don&#8217;t have a way to automatically convert images into it&#8217;s corresponding latent code! Teaser: That&#8217;s what projection is for, Section 6 :) . . Source: https://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-generative-models-and-adversarial-training-upc-2016 . 2. Generate Images of People who don&#39;t Exist . #collapse-hide import sys sys.path.append(&#39;stylegan2/&#39;) from stylegan2 import pretrained_networks from stylegan2 import dnnlib from stylegan2.dnnlib import tflib from pathlib import Path from PIL import Image import pickle import numpy as np import ipywidgets as widgets from tqdm import tqdm model_path = &#39;gdrive:networks/stylegan2-ffhq-config-f.pkl&#39; fps = 20 results_size = 400 . . #collapse-hide # Code to load the StyleGAN2 Model def load_model(): _G, _D, Gs = pretrained_networks.load_networks(model_path) noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith(&#39;noise&#39;)] Gs_kwargs = dnnlib.EasyDict() Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) Gs_kwargs.randomize_noise = False return Gs, noise_vars, Gs_kwargs # Generate images given a random seed (Integer) def generate_image_random(rand_seed): rnd = np.random.RandomState(rand_seed) z = rnd.randn(1, *Gs.input_shape[1:]) tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) images = Gs.run(z, None, **Gs_kwargs) return images, z # Generate images given a latent code ( vector of size [1, 512] ) def generate_image_from_z(z): images = Gs.run(z, None, **Gs_kwargs) return images . . Lets go ahead and start generating some outputs! . # Loading the StyleGAN2 Model! Gs, noise_vars, Gs_kwargs = load_model() . Downloading http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl ... done Setting up TensorFlow plugin &#34;fused_bias_act.cu&#34;: Preprocessing... Compiling... Loading... Done. Setting up TensorFlow plugin &#34;upfirdn_2d.cu&#34;: Preprocessing... Compiling... Loading... Done. . # Ask the generator to make an output, given a random seed number: 42 images, latent_code1 = generate_image_random(42) image1 = Image.fromarray(images[0]).resize((results_size, results_size)) latent_code1.shape . (1, 512) . . Note: As shown in the previous cell&#8217;s output, we can see that the latent_code for this output is of size (1, 512). This means that the numbers inside latent_code1 can be used to create the image below! . image1 . Let&#39;s make another image! . # Ask the generator to make an output, given a random seed number: 1234 images, latent_code2 = generate_image_random(1234) image2 = Image.fromarray(images[0]).resize((results_size, results_size)) latent_code2.shape . (1, 512) . latent_code1[0][:5], latent_code2[0][:5] . (array([ 0.49671415, -0.1382643 , 0.64768854, 1.52302986, -0.23415337]), array([ 0.47143516, -1.19097569, 1.43270697, -0.3126519 , -0.72058873])) . . Note: We can see the size of latent_code2 is also (1, 512). However, the two codes are not the same! This is seen in the first five values in the previous cell. Below is the corresponding image for generating output with latent_code2 . image2 . 3. Interpolation of Latent Codes . So what&#39;s the big deal? We have two codes to make two people that don&#39;t even exist right? Well, the cool thing about the latent space is that you can &quot;traverse&quot; through it! . Since the latent space is a compressed representation of some data, things that are similar in appearance should be &quot;close&quot; to each other in the latent space. . If the latent space is well developed, we can actually transition/interpolate between points in this space and create intermediate outputs! . In other words... we can morph two people together! See the gif below for a quick example! . . Source: https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df . Now let&#39;s do this on our examples we just generated! :D. . Let&#39;s interpolate halfway between latent_code1, latent_code2 . def linear_interpolate(code1, code2, alpha): return code1 * alpha + code2 * (1 - alpha) . interpolated_latent_code = linear_interpolate(latent_code1, latent_code2, 0.5) interpolated_latent_code.shape . (1, 512) . . Note: The latent_code size is still a vector of 512 numbers; We just took 50% of latent_code1 and 50% of latent_code2, (alpha=0.5), and summed them together! Below is the resulting image. . images = generate_image_from_z(interpolated_latent_code) Image.fromarray(images[0]).resize((results_size, results_size)) . Let&#39;s also make a cool interpolation animation; It&#39;ll help with visualizing the effect of interpolating from alpha=0 to alpha=1 . #collapse-hide output_gifs_path = Path(&#39;output_gifs&#39;) # Make Output Gifs folder if it doesn&#39;t exist. if not output_gifs_path.exists(): output_gifs_path.mkdir() . . #collapse-hide def get_concat_h(im1, im2): dst = Image.new(&#39;RGB&#39;, (im1.width + im2.width, im1.height)) dst.paste(im1, (0, 0)) dst.paste(im2, (im1.width, 0)) return dst def make_latent_interp_animation(code1, code2, img1, img2, num_interps): step_size = 1.0/num_interps all_imgs = [] amounts = np.arange(0, 1, step_size) for alpha in tqdm(amounts): interpolated_latent_code = linear_interpolate(code1, code2, alpha) images = generate_image_from_z(interpolated_latent_code) interp_latent_image = Image.fromarray(images[0]).resize((400, 400)) frame = get_concat_h(img1, interp_latent_image) frame = get_concat_h(frame, img2) all_imgs.append(frame) save_name = output_gifs_path/&#39;latent_space_traversal.gif&#39; all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0) . . make_latent_interp_animation(latent_code1, latent_code2, image1, image2, num_interps=200) . 100%|██████████| 200/200 [00:31&lt;00:00, 6.35it/s] . . Tip: If you&#8217;re running this notebook yourself, the interpolation gif will be saved in the following location: output_gifs/latent_space_traversal.gif :) . . . Note: This gif represents going from latent_code1 to latent_code2 by slowly changing alpha from 0 to 1. (increasing alpha by 1/200 per iteration, until it reaches 1.0) . 4. Facial Image Alignment using Landmark Detection . Ok so this is all fun and stuff right? How could we play around with our own images, instead of random people that don&#39;t exist? . Well, we first have to project our own images into this latent space. . . Important: The first step of projecting our own images is to make sure that they are representative of the training data. StyleGAN2 was trained on the FFHQ Dataset. More specifically, the images used during training were actually aligned first, before giving it to the discriminator in StyleGAN2. . To align (normalize) our images for StyleGAN2, we need to use a landmark detection model. This will automatically find the facial keypoints of interest, and crop/rotate accordingly. . Below is an example! . . Tip: At this point, if you want to run this with your own images, all you need to do is go to the imgs/ folder, and delete the example images, Jeremy_Howard.jpg and Obama.jpg. Then upload 2 of your own! . #collapse-hide orig_img_path = Path(&#39;imgs&#39;) aligned_imgs_path = Path(&#39;aligned_imgs&#39;) # Make Aligned Images folder if it doesn&#39;t exist. if not aligned_imgs_path.exists(): aligned_imgs_path.mkdir() orig_img_path, aligned_imgs_path . . (PosixPath(&#39;imgs&#39;), PosixPath(&#39;aligned_imgs&#39;)) . # One-Time Download of Facial Landmark Detection Model Weights if not Path(&#39;shape_predictor_68_face_landmarks.dat&#39;).exists(): !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 !bzip2 -dv shape_predictor_68_face_landmarks.dat.bz2 . --2020-08-11 18:28:05-- http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 Resolving dlib.net (dlib.net)... 107.180.26.78 Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 64040097 (61M) Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’ shape_predictor_68_ 100%[===================&gt;] 61.07M 952KB/s in 3m 25s 2020-08-11 18:31:30 (306 KB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097] shape_predictor_68_face_landmarks.dat.bz2: done . from align_face import align_face # Align all of our images using a landmark detection model! all_imgs = list(orig_img_path.iterdir()) for img in all_imgs: align_face(str(img)).save(aligned_imgs_path/(&#39;aligned_&#39;+img.name)) . Number of faces detected: 1 Detection 0: Left: 375 Top: 333 Right: 760 Bottom: 718 Part 0: (377, 507), Part 1: (391, 550) ... Number of faces detected: 1 Detection 0: Left: 1224 Top: 514 Right: 2022 Bottom: 1313 Part 0: (1333, 660), Part 1: (1323, 758) ... . Let&#39;s load the original + aligned images into Jupyter! . #collapse-hide aligned_img_set = list(aligned_imgs_path.iterdir()) aligned_img_set.sort() aligned_img_set = [Image.open(x) for x in aligned_img_set] orig_img_set = list(orig_img_path.iterdir()) orig_img_set.sort() orig_img_set = [Image.open(x) for x in orig_img_set] . . . Note: Original image is on the left, Aligned image is on the right. The image size for the original images aren&#8217;t necessarily square. However, the output size of the aligned images is always 1024x1024, a square! StyleGAN2 uses square shapes :) . get_concat_h(orig_img_set[0], aligned_img_set[0]) . get_concat_h(orig_img_set[1], aligned_img_set[1]) . . Important: From these visualizations, we can see that this alignment process gives us a more focused view of the face, and orients the face upward/straight (removing any tilt of the person in the image) . 5. Projecting our own Input Images into the Latent Space . . Warning: At this point, I restarted my notebook kernel to clear my GPU card memory. The previous sections loaded models inside our notebook, so I ended up restarting the kernel to free space. This is recommended because the next section will be invoking official stylegan2 python scripts from inside notebook cells, and they load models interally (separately from this notebook). You&#8217;ll most likely run out of GPU memory if you don&#8217;t restart! Everything in this notebook will still run nicely from top-to-bottom, don&#8217;t worry! . You can either manually restart your kernel by going to Kernel -&gt; Restart, or run the below cell: . #collapse-show # Automatically restart the kernel by running this cell import IPython IPython.Application.instance().kernel.do_shutdown(True) . . {&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True} . #collapse-hide import sys sys.path.append(&#39;stylegan2/&#39;) from stylegan2 import pretrained_networks from stylegan2 import dnnlib from stylegan2.dnnlib import tflib from pathlib import Path from PIL import Image import pickle import numpy as np import ipywidgets as widgets from tqdm import tqdm model_path = &#39;gdrive:networks/stylegan2-ffhq-config-f.pkl&#39; fps = 20 results_size = 400 . . . Note: First, we&#8217;ll make an official stylegan2 compatible dataset, to ignore the warnings from deprecated TensorFlow 1.14, we use the -W ignore option when executing python on the command line. . !python -W ignore stylegan2/dataset_tool.py create_from_images datasets_stylegan2/custom_imgs aligned_imgs/ . Loading images from &#34;aligned_imgs/&#34; Creating dataset &#34;datasets_stylegan2/custom_imgs&#34; Added 2 images. . . Note: Now let&#8217;s project our previously aligned images into the latent space, and get those latent codes that will hopefully well-represent the images. . Some things to explain here: . Projecting an image into the latent space basically means: Let&#39;s figure out what latent code (512 numbers) will cause the generator to make an output that looks like our image | The question is, how do we figure out the latent code? With VAEs (variational autoencoder), we just throw our image through the encoder and we get our latent code just like that! | With GANs, we don&#39;t necessarily have a direct way to extract latent codes from an input image, but we can optimize for it. | In a nut shell, here&#39;s how we can optimize for a latent code for given input images: | . For as many iterations as you&#39;d like, do: . Ask the generator to generate some output from a starting latent vector. | Take the generator&#39;s output image, and take your target image, put them both through a VGG16 model (image feature extractor). | Take the generator&#39;s output image features from the VGG16. | Take the target image features from the VGG16. | Compute the loss on the difference of features! | Backprop | . Important: Note that the loss between the generator&#8217;s output image, and the target image, is in the feature space of a VGG16! Why? If we performed pixel-wise loss on just the raw image pixels, it won&#8217;t account for any facial features, just color of pixels. Checking the difference of features allows us to optimize for a latent code that doesn&#8217;t just check for pixel values, but also the patterns in the pixels that correspond to high-level features, such as cheekbones, eyebrows, nose size, eye width, smile, etc. . Let&#39;s project our 2 custom aligned images of Jeremy and Obama! . tot_aligned_imgs = 2 . !python -W ignore stylegan2/epoching_custom_run_projector.py project-real-images --network=$model_path --dataset=custom_imgs --data-dir=datasets_stylegan2 --num-images=$tot_aligned_imgs --num-snapshots 500 . Local submit - run_dir: results/00001-project-real-images dnnlib: Running run_projector.project_real_images() on localhost... Loading networks from &#34;gdrive:networks/stylegan2-ffhq-config-f.pkl&#34;... Setting up TensorFlow plugin &#34;fused_bias_act.cu&#34;: Preprocessing... Loading... Done. Setting up TensorFlow plugin &#34;upfirdn_2d.cu&#34;: Preprocessing... Loading... Done. Downloading http://d36zk2xti64re0.cloudfront.net/stylegan1/networks/metrics/vgg16_zhang_perceptual.pkl ... done Loading images from &#34;custom_imgs&#34;... Projecting image 0/2 ... 2020-08-11 18:46:58.427331: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU. To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile. Projecting image 1/2 ... dnnlib: Finished run_projector.project_real_images() in 28m 47s. . . Note: This cell will take a while to run! 1000 iterations for optimizing latent codes (per image) are occuring here, and we&#8217;re saving progress-images every 2 steps for a cool gif later! . #collapse-hide def get_concat_h(im1, im2): dst = Image.new(&#39;RGB&#39;, (im1.width + im2.width, im1.height)) dst.paste(im1, (0, 0)) dst.paste(im2, (im1.width, 0)) return dst def make_project_progress_gifs(): all_result_folders = list(Path(&#39;results/&#39;).iterdir()) all_result_folders.sort() last_result_folder = all_result_folders[-1] for img_num in range(tot_aligned_imgs): all_step_pngs = [x for x in last_result_folder.iterdir() if x.name.endswith(&#39;png&#39;) and &#39;image{0:04d}&#39;.format(img_num) in x.name] all_step_pngs.sort() target_image = Image.open(all_step_pngs[-1]).resize((results_size, results_size)) all_concat_imgs = [] for step_img_path in all_step_pngs[:-1]: step_img = Image.open(step_img_path).resize((results_size, results_size)) all_concat_imgs.append(get_concat_h(target_image, step_img)) all_concat_imgs[0].save(&#39;output_gifs/image{0:04d}_project_progress.gif&#39;.format(img_num), save_all=True, append_images=all_concat_imgs[1:], duration=1000/fps, loop=0) . . make_project_progress_gifs() . . Tip: If you&#8217;re running this notebook yourself, the project progress gifs will be saved in output_gifs/image####_project_progress.gif :) . . . Let&#39;s look at the optimized latent codes we have acquired through this projection process! . #collapse-hide def get_final_latents(): all_results = list(Path(&#39;results/&#39;).iterdir()) all_results.sort() last_result = all_results[-1] latent_files = [x for x in last_result.iterdir() if &#39;final_latent_code&#39; in x.name] latent_files.sort() all_final_latents = [] for file in latent_files: with open(file, mode=&#39;rb&#39;) as latent_pickle: all_final_latents.append(pickle.load(latent_pickle)) return all_final_latents . . latent_codes = get_final_latents() len(latent_codes), latent_codes[0].shape, latent_codes[1].shape . (2, (1, 18, 512), (1, 18, 512)) . . Note: We now have two latent codes for our Jeremy Howard and Obama images! Notice how these codes are of the shape (1, 18, 512), instead of the (1, 512) shape we saw earlier on the generated (fake) examples. This is due to one of the architecture designs of StyleGAN2, it actually re-iterates the base latent vector at different levels in the generator to allow for small deviations in the latent code to support variance in style. During training, just one static latent vector of the shape (1, 512) is used. For a more detailed explanation, check out the recommended technical StyleGAN2 overview blog posts mentioned in the introduction! :) . Let&#39;s check out what our images look like from our latent codes! . #collapse-hide def load_model(): _G, _D, Gs = pretrained_networks.load_networks(model_path) noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith(&#39;noise&#39;)] Gs_kwargs = dnnlib.EasyDict() Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) Gs_kwargs.randomize_noise = False return Gs, noise_vars, Gs_kwargs def generate_image_from_projected_latents(latent_vector): images = Gs.components.synthesis.run(latent_vector, **Gs_kwargs) return images . . # Loading the StyleGAN2 Model! Gs, noise_vars, Gs_kwargs = load_model() . Setting up TensorFlow plugin &#34;fused_bias_act.cu&#34;: Preprocessing... Loading... Done. Setting up TensorFlow plugin &#34;upfirdn_2d.cu&#34;: Preprocessing... Loading... Done. . #collapse-hide # Must re-define these variables because we restarted our kernel! output_gifs_path = Path(&#39;output_gifs/&#39;) aligned_imgs_path = Path(&#39;aligned_imgs/&#39;) aligned_img_set = list(aligned_imgs_path.iterdir()) aligned_img_set.sort() aligned_img_set = [Image.open(x) for x in aligned_img_set] . . # Ask the generator to make an output, given a latent code we found from the projection process. images = generate_image_from_projected_latents(latent_codes[0]) recreated_img1 = Image.fromarray(images[0]).resize((results_size, results_size)) orig_img1 = aligned_img_set[1].resize((results_size, results_size)) get_concat_h(orig_img1, recreated_img1) . # Ask the generator to make an output, given a latent code we found from the projection process. images = generate_image_from_projected_latents(latent_codes[1]) recreated_img2 = Image.fromarray(images[0]).resize((results_size, results_size)) orig_img2 = aligned_img_set[0].resize((results_size, results_size)) get_concat_h(orig_img2, recreated_img2) . . Note: Not bad at all! There are better ways to get more accurate latent codes that have better &quot;visual fidelity&quot;, such as optimizing all (1, 18, 512) latent codes, instead of just the base latent code (1, 512), as mentioned in this blog post by 5agado . Now let&#39;s re-run the interpolation animation we did back in Section 4, but this time with our own projected latent codes! . #collapse-hide def linear_interpolate(code1, code2, alpha): return code1 * alpha + code2 * (1 - alpha) def make_latent_interp_animation_real_faces(code1, code2, img1, img2, num_interps): step_size = 1.0/num_interps all_imgs = [] amounts = np.arange(0, 1, step_size) for alpha in tqdm(amounts): interpolated_latent_code = linear_interpolate(code1, code2, alpha) images = generate_image_from_projected_latents(interpolated_latent_code) interp_latent_image = Image.fromarray(images[0]).resize((400, 400)) frame = get_concat_h(img2, interp_latent_image) frame = get_concat_h(frame, img1) all_imgs.append(frame) save_name = output_gifs_path/&#39;projected_latent_space_traversal.gif&#39; all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0) . . make_latent_interp_animation_real_faces(latent_codes[0], latent_codes[1], recreated_img1, recreated_img2, num_interps=200) . 100%|██████████| 200/200 [00:31&lt;00:00, 6.35it/s] . . Tip: If you&#8217;re running this notebook yourself, the interpolation gif will be saved in output_gifs/projected_latent_space_traversal.gif :) . . . Note: This gif represents going from Jeremy&#8217;s latent code to Obama&#8217;s latent code by slowly changing alpha from 0 to 1. (increasing alpha by 1/200 per iteration, until it reaches 1.0) . 6. Latent Directions/Controls to modify our projected images . Time to be an astronaut and explore space! Well, the hidden (latent) kind of space. Alright... I admit, that joke was blatant. Sorry for the puns, I&#39;m just trying to relate things together. Ok ok ok, you need some space, got it. . There are ways to learn latent directions (both supervised, and unsupervised) in the latent space to control features. People have already open-sourced some directional latent vectors for StyleGAN2 that allow us to &quot;move&quot; in the latent space and control a particular feature. . Supervised Method of learning these latent directions:&quot;We first collect multiple samples (image + latent) from our model and manually classify the images for our target attribute (e.g. smiling VS not smiling), trying to guarantee proper class representation balance. We then train a model to classify or regress on our latents and manual labels. At this point we can use the learned functions of these support models as transition directions&quot; - 5agado&#39;s Blog . | Unsupervised Method: Unsupervised Discovery of Interpretable Directions in the GAN Latent Space | . To move in a latent direction we can do the following operation: latent_code = latent_code + latent_direction * magnitude . latent_code is our latent code, such as our recently optimized latent code! | latent_direction is a learnt directional vector that is of shape (18, 512). This vector tells you where to move in the latent space to control a feature, but not how much to move. | magnitude is the amount to move in the direction of latent_direction | . This means we can create more interpolations in the latent space! Yay, more animations :). Only this time, rather than interpolating between 2 points, we are slowing moving in a specific latent direction. . Instead of mixing two latent codes together, we slowly add more magnitude to our base latent code, and observe how it changes with respect to magnitude! | . . Note: We&#8217;ll be using the learnt latent directions shared by Robert Luxemburg . #collapse-show def get_control_latent_vectors(path): files = [x for x in Path(path).iterdir() if str(x).endswith(&#39;.npy&#39;)] latent_vectors = {f.name[:-4]:np.load(f) for f in files} return latent_vectors . . latent_controls = get_control_latent_vectors(&#39;stylegan2directions/&#39;) len(latent_controls), latent_controls.keys(), latent_controls[&#39;age&#39;].shape . (16, dict_keys([&#39;age&#39;, &#39;eye_distance&#39;, &#39;eye_eyebrow_distance&#39;, &#39;eye_ratio&#39;, &#39;eyes_open&#39;, &#39;gender&#39;, &#39;lip_ratio&#39;, &#39;mouth_open&#39;, &#39;mouth_ratio&#39;, &#39;nose_mouth_distance&#39;, &#39;nose_ratio&#39;, &#39;nose_tip&#39;, &#39;pitch&#39;, &#39;roll&#39;, &#39;smile&#39;, &#39;yaw&#39;]), (18, 512)) . . Note: We have 16 latent &quot;controls&quot; we can play around with! . #collapse-hide def make_latent_control_animation(feature, start_amount, end_amount, step_size, person): all_imgs = [] amounts = np.linspace(start_amount, end_amount, abs(end_amount-start_amount)/step_size) for amount_to_move in tqdm(amounts): modified_latent_code = np.array(latent_code_to_use) modified_latent_code += latent_controls[feature]*amount_to_move images = generate_image_from_projected_latents(modified_latent_code) latent_img = Image.fromarray(images[0]).resize((results_size, results_size)) all_imgs.append(get_concat_h(image_to_use, latent_img)) save_name = output_gifs_path/&#39;{0}_{1}.gif&#39;.format(person, feature) all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0) . . latent_code_to_use = latent_codes[1] image_to_use = recreated_img2 make_latent_control_animation(feature=&#39;age&#39;, start_amount=-10, end_amount=10, step_size=0.1, person=&#39;jeremy&#39;) . 100%|██████████| 200/200 [00:31&lt;00:00, 6.39it/s] . latent_code_to_use = latent_codes[0] image_to_use = recreated_img1 make_latent_control_animation(feature=&#39;age&#39;, start_amount=-5, end_amount=5, step_size=0.1, person=&#39;obama&#39;) . 100%|██████████| 100/100 [00:15&lt;00:00, 6.38it/s] . . Tip: If you&#8217;re running this notebook yourself, the latent control interpolation gifs will be saved in output_gifs/person_feature.gif :) . . Important: Here are 5 traversals in the latent space using latent directions: . . Note: Aging Jeremy: . . . Note: Female Jeremy: . . . Note: Closing Mouth Jeremy: . . . Note: Aging Obama: . . . Note: Smiling Obama: . . 7. Bonus: Interactive Widget-App! . #input-hide def apply_latent_controls(self): image_outputs = controller.children[0] feature_sliders = controller.children[1] slider_hboxes = feature_sliders.children[:-2] latent_movements = [(x.children[1].value, x.children[0].value) for x in slider_hboxes] modified_latent_code = np.array(latent_code_to_use) for feature, amount_to_move in latent_movements: modified_latent_code += latent_controls[feature]*amount_to_move images = generate_image_from_projected_latents(modified_latent_code) latent_img = Image.fromarray(images[0]).resize((400, 400)) latent_img_output = image_outputs.children[1] with latent_img_output: latent_img_output.clear_output() display(latent_img) def reset_latent_controls(self): image_outputs = controller.children[0] feature_sliders = controller.children[1] slider_hboxes = feature_sliders.children[:-2] for x in slider_hboxes: x.children[0].value = 0 latent_img_output = image_outputs.children[1] with latent_img_output: latent_img_output.clear_output() display(image_to_use) def create_interactive_latent_controller(): orig_img_output = widgets.Output() with orig_img_output: orig_img_output.clear_output() display(image_to_use) latent_img_output = widgets.Output() with latent_img_output: latent_img_output.clear_output() display(image_to_use) image_outputs = widgets.VBox([orig_img_output, latent_img_output]) #collapse-hide generate_button = widgets.Button(description=&#39;Generate&#39;, layout=widgets.Layout(width=&#39;75%&#39;, height=&#39;10%&#39;)) generate_button.on_click(apply_latent_controls) reset_button = widgets.Button(description=&#39;Reset Latent Controls&#39;, layout=widgets.Layout(width=&#39;75%&#39;, height=&#39;10%&#39;)) reset_button.on_click(reset_latent_controls) feature_sliders = [] for feature in latent_controls: label = widgets.Label(feature) slider = widgets.FloatSlider(min=-50, max=50) feature_sliders.append(widgets.HBox([slider, label])) feature_sliders.append(generate_button) feature_sliders.append(reset_button) feature_sliders = widgets.VBox(feature_sliders) return widgets.HBox([image_outputs, feature_sliders]) . . . Tip: Try running this widget-app yourself to see how well-fit some of the latent directions/controls are, and how loose others are! . . Important: You can also try plugging-in your own latent directions. (Maybe find another set of latent directions someone open-sourced. Who knows, they might work better than this set of latent directions!) You can even try to learn your own latent directions! . #collapse-hide latent_code_to_use = latent_codes[0] image_to_use = recreated_img1 controller = create_interactive_latent_controller() controller . . #collapse-hide latent_code_to_use = latent_codes[1] image_to_use = recreated_img2 controller = create_interactive_latent_controller() controller . . . Important: Below are some examples of the interactive widget-app :D . . Tip: This is super useful for finding good starting/end points for the feature-changing gifs shown in the previous Section 6. . . Note: Old Jeremy: . . . Note: Young Jeremy: . . . Note: Female Jeremy: . . . Note: Close Mouth Jeremy: . . . Note: Old Obama: . . . Note: Young Obama: . . . Note: Close Mouth Obama: . . . Note: Smiling Obama: . . Conclusion . Thanks for reading, and I hope you had as much fun playing/exploring the latent space as I did! .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/2020/08/10/Latent-Space-Exploration-with-StyleGAN2.html",
            "relUrl": "/jupyter/2020/08/10/Latent-Space-Exploration-with-StyleGAN2.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Self-Supervision with FastAI",
            "content": "Introduction . This notebook is an introduction to self-supervised learning. In short, self-supervised learning has 2 components: . Pretrain on a pretext task, where the labels can come from the data itself! | Transfer the features, and train on the actual classification labels!&quot;What if we can get labels for free for unlabelled data and train unsupervised dataset in a supervised manner? We can achieve this by framing a supervised learning task in a special form to predict only a subset of information using the rest. In this way, all the information needed, both inputs and labels, has been provided. This is known as self-supervised learning.&quot; - Lilian Weng . | Using FastAI2, we&#39;ll use rotation as a pretext task for learning representations/features of our data. . Here are some great overviews of self-supervised learning that I&#39;ve come across: . Lilian Weng - Self-Supervised Representation Learning . | Jeremy Howard - Self-supervised learning and computer vision . | . Experiment Layout . In this notebook, we will be using the MNIST dataset. . Also check out ImageWang from FastAI themselves! It&#39;s a dataset designed for self-supervision tasks! . Train a model on a rotation prediction task. . We will use all the training data for rotation prediction. | Input: A rotated image. | Target/Label: Classify the amount of degrees rotated. | Our model should learn useful features that can transfer well for a classification task. | (The model should learn what digits look like in order to be able to successfully predict the amount of rotation). | . | Transfer our rotation pretraining features to solve the classification task with much fewer labels, &lt; 1% of the original data. . Input: A normal image. | Target/Label: The images&#39; original categorical label. | Classification accuracy should be decent, even with only using &lt; 1% of the original data. | . | Train a classifier from scratch on the same amount of data used in experiment 2. . Input: A normal image. | Target/Label: The images&#39; original categorical label. | Classification accuracy should be low (lack of transfer learning &amp; too few labeled data!) | Model may overfit. | . | FastAI Vision Model Creation Methods . . Warning: This Jupyter notebook runs with fastai2! Make sure you have it installed, use the cell below to install it :) . !pip install fastai2 . . Important: Pay attention! It&#8217;s important. We will be using a small ConvNet to test our self-supervised learning method. The architecture is defined below in simple_arch. Note that simple_arch takes in one argument, pretrained. This is to allow FastAI to pass pretrained=True or pretrained=False when creating the model body! Below are some use cases of when we would want pretrained=True or pretrained=False. . pretrained=False = For training a new model on our rotation prediction task. | pretrained=True = For transferring the learnt features from our rotation task pretraining to solve a classification task. | pretrained=False = For training a new model from scratch on the main classification task (no transfer learning). | from fastai2.vision.all import * . #collapse-show def simple_arch(pretrained=False): # Note that FastAI will automatically cut at pooling layer for the body! model = nn.Sequential( nn.Conv2d(1, 4, 3, 1), nn.BatchNorm2d(4), nn.ReLU(), nn.Conv2d(4, 16, 3, 1), nn.BatchNorm2d(16), nn.ReLU(), nn.Conv2d(16, 32, 3, 1), nn.BatchNorm2d(32), nn.AdaptiveAvgPool2d(1), ) if (pretrained): print(&quot;Loading pretrained model...&quot;) pretrained_weights = torch.load(save_path/&#39;rot_pretrained.pt&#39;) print(model.load_state_dict(pretrained_weights)) return model . . The follow below code snippets are examples of how FastAI creates CNNs. Every model will have a body and a head . #collapse-show body = create_body(arch=simple_arch, pretrained=False) body . . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . #collapse-show head = create_head(nf=32*2, n_out=8, lin_ftrs=[]) head . . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) . #collapse-show # Note that FastAI automatically determines nf for the head! model = create_cnn_model(arch=simple_arch, pretrained=False, n_out=8, lin_ftrs=[]) model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) ) . PyTorch Rotation/Classification Self-Supervised Dataset . # Functions to convert between Torch Tensors and PIL Images import torchvision tensorToImage = torchvision.transforms.ToPILImage() imageToTensor = torchvision.transforms.ToTensor() . # Download MNIST dataset from PyTorch if not downloaded already! torchvision.datasets.MNIST(&#39;data/&#39;, download=&#39;True&#39;) . Dataset MNIST Number of datapoints: 60000 Root location: data/ Split: Train . Below we define a dataset, here&#39;s the docstring: . A Dataset for Rotation-based Self-Supervision! Images are rotated clockwise. . file - MNIST processed .pt file. | pct - percent of data to use | classification - False=Use rotation labels. True=Use original classification labels. | . #collapse-hide class Custom_Dataset_MNIST(): &#39;&#39;&#39; A Dataset for Rotation-based Self-Supervision! Images are rotated clockwise. - file - MNIST processed .pt file. - pct - percent of data to use - classification - False=Use rotation labels. True=Use original classification labels. &#39;&#39;&#39; def __init__(self, file, pct, classification): data = torch.load(file) self.imgs = data[0] self.labels = data[1] self.pct = pct self.classification = classification slice_idx = int(len(self.imgs)*self.pct) self.imgs = self.imgs[:slice_idx] def __len__(self): return len(self.imgs) def __getitem__(self, idx): img = self.imgs[idx].unsqueeze(0) img = tensorToImage(img) img = img.resize((32, 32), resample=1) img = imageToTensor(img) if (not self.classification): # 4 classes for rotation degrees = [0, 45, 90, 135, 180, 225, 270, 315] rand_choice = random.randint(0, len(degrees)-1) img = tensorToImage(img) img = img.rotate(degrees[rand_choice]) img = imageToTensor(img) return img, torch.tensor(rand_choice).long() return img, self.labels[idx] def show_batch(self, n=3): fig, axs = plt.subplots(n, n) fig.tight_layout() for i in range(n): for j in range(n): rand_idx = random.randint(0, len(self)-1) img, label = self.__getitem__(rand_idx) axs[i, j].imshow(tensorToImage(img), cmap=&#39;gray&#39;) if self.classification: axs[i, j].set_title(&#39;Label: {0} (Digit #{1})&#39;.format(label.item(), label.item())) else: axs[i, j].set_title(&#39;Label: {0} ({1} Degrees)&#39;.format(label.item(), label.item()*45)) axs[i, j].axis(&#39;off&#39;) . . Rotation Prediction Data . . Important: 60k training data and 10k validation data! . # Make rotation datasets train_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/training.pt&#39;, pct=1.0, classification=False) valid_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/test.pt&#39;, pct=1.0, classification=False) print(&#39;{0} Training Samples | {1} Validation Samples&#39;.format(len(train_ds), len(valid_ds))) . 60000 Training Samples | 10000 Validation Samples . . Note: Notice that our labels don&#8217;t correspond to digits! They correspond to the amount of degrees rotated! Specifically from this predefined set: [0, 45, 90, 135, 180, 225, 270, 315] . from fastai2.data.core import DataLoaders dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() # Override the show_batch function of dls to the one used in our dataset! dls.show_batch = train_ds.show_batch # We have 8 classes! [0, 1, 2, 3, 4, 5, 6, 7] that correspond to the [0, 45, 90, 135, 180, 225, 270, 315] degrees of rotation. dls.c = 8 dls.show_batch() . FastAI Vision Learner [Rotation] . # Create a config for our model&#39;s head! head_config = cnn_config(lin_ftrs=[]) head_config . {&#39;lin_ftrs&#39;: []} . . Note: We want to measure top_2_accuracy along with regular (top_1) accuracy, because there are hard-cases where it&#8217;s understandable why our model got it wrong. For example: &#8217;0&#8217; rotated 90 or 270 degrees, or &#8217;1&#8217; rotated 0 or 180 degrees. (They can look the same!) . # Top_2 accuracy is a nice metric for hard-cases: # - A zero rotated 90 or 270 degrees? # - A one rotated 0 or 180 degrees? # etc :P top_2_accuracy = lambda inp, targ: top_k_accuracy(inp, targ, k=2) top_2_accuracy . &lt;function __main__.&lt;lambda&gt;(inp, targ)&gt; . Here, we train a model on the rotation prediction task! . #collapse-show # Note to set a value for lin_ftrs, we use the defined config above. learner = cnn_learner(dls, simple_arch, pretrained=False, loss_func=CrossEntropyLossFlat(), config=head_config, metrics=[accuracy, top_2_accuracy]) learner.model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) ) . learner.summary() . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 True ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 True ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 True ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 8 512 True ________________________________________________________________ Total params: 6,016 Total trainable params: 6,016 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . (0.09120108485221863, 3.6307804407442745e-07) . learner.fit_one_cycle(5, lr_max=3e-2) . epoch train_loss valid_loss accuracy top-2 accuracy time . 0 | 0.931490 | 1.331268 | 0.487900 | 0.822600 | 00:11 | . 1 | 0.824483 | 0.871249 | 0.617000 | 0.901600 | 00:11 | . 2 | 0.757743 | 0.984435 | 0.570000 | 0.868800 | 00:11 | . 3 | 0.622819 | 0.568738 | 0.757100 | 0.962500 | 00:11 | . 4 | 0.578483 | 0.489926 | 0.799200 | 0.972800 | 00:11 | . . Important: We were able to achieve 79.9% top-1 accuracy, and 97.3% top-2 accuracy after just 5 epochs! Now we want to grab our model from our Learner, and save the body of it! . . Note: Our model has two components, the body and the head. model is a list of size 2, where model[0] is the body, and model[1] is the head! . # Access the body of our model trained_body = learner.model[0] trained_body . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . . Tip: To save a model in PyTorch, save it&#8217;s state_dict function! You can use model.load_state_dict to re-load the weights. . # Make save directory if it doesn&#39;t exist save_path = Path(&#39;rotation_cps/&#39;) if not save_path.exists(): save_path.mkdir() # Save the rotation-pretraining weights of our model body torch.save(trained_body.state_dict(), save_path/&#39;rot_pretrained.pt&#39;) . Original Classification Data . Now that we have pretrained our model on the rotation prediction task, we want to switch over to the original labeled data for the classification task. . . Important: We&#8217;re only using 180 samples for training! . # Use 0.3% classification labeled data for training! # Use 100% classification labeled data for validation! train_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/training.pt&#39;, pct=0.003, classification=True) valid_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/test.pt&#39;, pct=1.0, classification=True) print(&#39;{0} Training Samples | {1} Validation Samples&#39;.format(len(train_ds), len(valid_ds))) . 180 Training Samples | 10000 Validation Samples . . Note: Notice the labels now correspond to the digit class! . from fastai2.data.core import DataLoaders dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() dls.show_batch = train_ds.show_batch # We have 10 classes! One for each digit label! dls.c = 10 dls.show_batch() . FastAI Vision Learner [Transfer-Classification] . Here we will toggle pretrained=True to transfer our rotation prediction features, and train on the original 180 labeled data. . #collapse-show # pretrained=True will load the saved rotation pretraining weights into our model&#39;s body! # See simple_arch() function definition for more details! learner = cnn_learner(dls, simple_arch, pretrained=True, loss_func=CrossEntropyLossFlat(), config=head_config, metrics=[accuracy]) learner.model . . Loading pretrained model... &lt;All keys matched successfully&gt; . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) ) . . Tip: Freezing a model&#8217;s body after transferring the weights over, allows the new head to get calibrated with the rest of the model! . learner.freeze() . . Note: Looking at the model summary, we can see that the model is frozen up to the new head! Good! . learner.summary() . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 False ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 False ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 False ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 10 640 True ________________________________________________________________ Total params: 6,144 Total trainable params: 872 Total non-trainable params: 5,272 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group number 1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . (0.06309573650360108, 0.0691830962896347) . learner.fit_one_cycle(10, lr_max=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.503277 | 5.978975 | 0.097400 | 00:02 | . 1 | 3.477977 | 5.445812 | 0.097400 | 00:02 | . 2 | 3.231445 | 4.004679 | 0.097400 | 00:02 | . 3 | 2.933150 | 3.224835 | 0.101100 | 00:02 | . 4 | 2.651888 | 2.746891 | 0.164300 | 00:02 | . 5 | 2.403430 | 2.261896 | 0.232900 | 00:01 | . 6 | 2.231838 | 2.060806 | 0.278600 | 00:02 | . 7 | 2.078108 | 1.910456 | 0.323700 | 00:02 | . 8 | 1.946290 | 1.764125 | 0.386200 | 00:02 | . 9 | 1.845896 | 1.637488 | 0.442200 | 00:02 | . . Tip: Unfreeze the model after calibrating the new head with the transferred body, and train a little more! . learner.unfreeze() . learner.summary() . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 True ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 True ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 True ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 10 640 True ________________________________________________________________ Total params: 6,144 Total trainable params: 6,144 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . (0.025118863582611083, 3.6307804407442745e-07) . learner.fine_tune(5, base_lr=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.995149 | 1.549676 | 0.479500 | 00:02 | . epoch train_loss valid_loss accuracy time . 0 | 0.982638 | 1.478698 | 0.512300 | 00:02 | . 1 | 0.994823 | 1.416823 | 0.541200 | 00:02 | . 2 | 1.004630 | 1.354943 | 0.569300 | 00:02 | . 3 | 1.007987 | 1.292374 | 0.597100 | 00:02 | . 4 | 0.976209 | 1.245139 | 0.617300 | 00:01 | . . Important: We were able to get 61.7% accuracy using transfer learning from our pretraining on the rotation prediction task! . FastAI Vision Learner [From Sratch-Classification] . Here we train a model from scratch on the original 180 labeled data. . #collapse-show # pretrained=False, Create the same model as before, but without using the rotation pretraining weights! learner = cnn_learner(dls, simple_arch, pretrained=False, loss_func=CrossEntropyLossFlat(), config=head_config, metrics=[accuracy]) learner.model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) ) . learner.summary() . Sequential (Input shape: [&#39;64 x 1 x 32 x 32&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 4 x 30 x 30 40 True ________________________________________________________________ BatchNorm2d 64 x 4 x 30 x 30 8 True ________________________________________________________________ ReLU 64 x 4 x 30 x 30 0 False ________________________________________________________________ Conv2d 64 x 16 x 28 x 28 592 True ________________________________________________________________ BatchNorm2d 64 x 16 x 28 x 28 32 True ________________________________________________________________ ReLU 64 x 16 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 32 x 26 x 26 4,640 True ________________________________________________________________ BatchNorm2d 64 x 32 x 26 x 26 64 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 32 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 64 0 False ________________________________________________________________ BatchNorm1d 64 x 64 128 True ________________________________________________________________ Dropout 64 x 64 0 False ________________________________________________________________ Linear 64 x 10 640 True ________________________________________________________________ Total params: 6,144 Total trainable params: 6,144 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7f26afadfa70&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . (0.05248074531555176, 0.0010000000474974513) . learner.fit_one_cycle(10, lr_max=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.452045 | 2.400267 | 0.098200 | 00:02 | . 1 | 2.946759 | 2.611470 | 0.098200 | 00:02 | . 2 | 2.596977 | 3.005997 | 0.098200 | 00:01 | . 3 | 2.313555 | 3.860830 | 0.098200 | 00:02 | . 4 | 2.055550 | 4.996419 | 0.098200 | 00:02 | . 5 | 1.897385 | 5.686416 | 0.098200 | 00:02 | . 6 | 1.752764 | 5.904824 | 0.098200 | 00:02 | . 7 | 1.620871 | 6.086377 | 0.098200 | 00:02 | . 8 | 1.537340 | 6.286355 | 0.100200 | 00:02 | . 9 | 1.469650 | 6.423689 | 0.130200 | 00:02 | . . Important: We were able to only get 13% accuracy with training from scratch . Conclusion . . Important: Using self-supervision can help learn features that can transfer to a down-stream task, such as classification! In this example, we used rotation predication as our pretext task for feature representation learning. Pretraining our model on rotation prediction prior to training for classification allowed us to achieve 61.7% accuracy, on just 0.3% of the labeled data (180 samples)! Training from scratch with the same amount of data yields an accuracy of 13%. The motivation for using self-supervised learning is the ability to train models with decent accuracy without the need of much labeled data! . . Note: Be sure to try other self-supervised learning methods (or perhaps find your own!) and compete on the ImageWang Leadboard! How will model size, data difficultly, and dataset size (number of samples) affect self-supervised learning? .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html",
            "relUrl": "/jupyter/2020/03/23/Self-Supervision-with-FastAI.html",
            "date": " • Mar 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Just another person trying to Save the World, One Epoch at a Time… . Socials . Github | Twitter | FastAI Profile | PyTorch Profile | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://amarsaini.github.io/Epoching-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}
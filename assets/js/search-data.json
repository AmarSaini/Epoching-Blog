{
  
    
        "post0": {
            "title": "NLP from Scratch with PyTorch, fastai, and HuggingFace",
            "content": "0. Introduction . Welcome! In this blog post/notebook, we&#39;ll be looking at NLP with 3 different methods: . From Scratch/Ground-Up, with PyTorch | FastAI Language Model (AWD-LSTM) | HuggingFace Transformers (DistilBERT) | . All 3 methods will utilize fastai to assist with keeping things organized and help with training the models, given the libary&#39;s ease of use through it&#39;s lovely Layered-API! . 1. Looking at the Data [Pandas] . For this notebook, we&#39;ll be looking at the Amazon Reviews Polarity dataset! The task is to predict whether a review is of positive or negative sentiment. The original Amazon Reviews dataset contains review scores ranging from 1-5. This polarity dataset combines review scores 1-2 into the negative class, 4-5 into the positive class, and ignores/drops review scores of 3! . from fastai.text.all import * import pandas as pd . path = untar_data(URLs.AMAZON_REVIEWS_POLARITY) path . Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv&#39;) . path.ls() . (#3) [Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv/train.csv&#39;),Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv/readme.txt&#39;),Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv/test.csv&#39;)] . Let&#39;s go ahead and take a look at our two df&#39;s: train_df and valid_df . . Note: We&#8217;re going to use 40k instead of 3.6m samples for training, and 2k instead of 400k samples for validation . train_df = pd.read_csv(path/&#39;train.csv&#39;, names=[&#39;label&#39;, &#39;title&#39;, &#39;text&#39;], nrows=40000) valid_df = pd.read_csv(path/&#39;test.csv&#39;, names=[&#39;label&#39;, &#39;title&#39;, &#39;text&#39;], nrows=2000) train_df.head() . label title text . 0 2 | Stuning even for the non-gamer | This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^ | . 1 2 | The best soundtrack ever to anything. | I&#39;m reading a lot of reviews saying that this is the best &#39;game soundtrack&#39; and I figured that I&#39;d write a review to disagree a bit. This in my opinino is Yasunori Mitsuda&#39;s ultimate masterpiece. The music is timeless and I&#39;m been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny. | . 2 2 | Amazing! | This soundtrack is my favorite music of all time, hands down. The intense sadness of &quot;Prisoners of Fate&quot; (which means all the more if you&#39;ve played the game) and the hope in &quot;A Distant Promise&quot; and &quot;Girl who Stole the Star&quot; have been an important inspiration to me personally throughout my teen years. The higher energy tracks like &quot;Chrono Cross ~ Time&#39;s Scar~&quot;, &quot;Time of the Dreamwatch&quot;, and &quot;Chronomantique&quot; (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer&#39;s work (I haven&#39;t heard the Xenogears s... | . 3 2 | Excellent Soundtrack | I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it&#39;s truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon&#39;s Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Uns... | . 4 2 | Remember, Pull Your Jaw Off The Floor After Hearing it | If you&#39;ve played the game, you know how divine the music is! Every single song tells a story of the game, it&#39;s that good! The greatest songs are without a doubt, Chrono Cross: Time&#39;s Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper. | . . Note: label 1 is negative sentiment and label 2 is positive sentiment . len(train_df), len(valid_df) . (40000, 2000) . 2. Tokenization and Numericalization [PyTorch] . We now want to first tokenize our inputs, then numericalize them using a vocab. Quick recap of these terms: . Tokenization = The process of converting an input string into &quot;pieces&quot; These pieces can be whole words, sub words, or even characters | . | Numericalization = The process of converting a token into a numeric representation (e.g. token -&gt; number) | This is done through the use (and creation of) a vocab | . | . There are many fancy tokenizers out there, but since we&#39;re first doing things from scratch we&#39;ll go ahead and use a simple basic_english tokenizer from torchtext and split on spaces . . Tip: More popular tokenizer pipelines can be found in libraries such as SpaCy, HuggingFace, and fastai! . sample_text = train_df[&#39;text&#39;][0] sample_text . &#39;This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^&#39; . import torch import torchtext from torchtext.data import get_tokenizer tokenizer = get_tokenizer(&quot;basic_english&quot;) . . Tip: Reminder that fastai&#8217;s L is basically list from Python, but has some convienent properties such as displaying the number of elements, and additionally doesn&#8217;t spam your screen with output if the list is too long! . tokens = L(tokenizer(sample_text)) tokens . (#81) [&#39;this&#39;,&#39;sound&#39;,&#39;track&#39;,&#39;was&#39;,&#39;beautiful&#39;,&#39;!&#39;,&#39;it&#39;,&#39;paints&#39;,&#39;the&#39;,&#39;senery&#39;...] . . Note: Since we&#8217;re using a full word tokenizer, many of these words will be quite infrequent, such as typos or words with repetiting characters like Dudeeeee. Other tokenizers can use rules to better handle splitting of big words through subword tokenization, and can also handle numbers like prices as well. This would help with optimizing the vocab&#8217;s embedding table as well as reducing the number of &lt;unk&gt; tokens. . Next we&#39;ll need to check how many tokens there are in our dataset, and keep the frequent ones as part of our vocab. . from collections import Counter token_counter = Counter() for sample_text in train_df[&#39;text&#39;]: tokens = tokenizer(sample_text) token_counter.update(tokens) token_counter.most_common(n=25) . [(&#39;.&#39;, 213962), (&#39;the&#39;, 158787), (&#39;,&#39;, 116525), (&#39;i&#39;, 91270), (&#39;and&#39;, 86059), (&#39;a&#39;, 77977), (&#39;to&#39;, 74984), (&#39;it&#39;, 69999), (&#39;of&#39;, 65144), (&#34;&#39;&#34;, 60523), (&#39;this&#39;, 59382), (&#39;is&#39;, 56445), (&#39;in&#39;, 37890), (&#39;that&#39;, 33891), (&#39;for&#39;, 30532), (&#39;was&#39;, 29163), (&#39;you&#39;, 26740), (&#39;!&#39;, 25238), (&#39;book&#39;, 24698), (&#39;s&#39;, 23897), (&#39;but&#39;, 22602), (&#39;with&#39;, 21998), (&#39;not&#39;, 21988), (&#39;on&#39;, 20759), (&#39;t&#39;, 20097)] . . Note: The top 25 most common tokens are listed above! Let&#8217;s see what the least frequent tokens look like: . token_counter.most_common()[-25:] . [(&#39;knorflefob&#39;, 1), (&#39;&lt;&lt;dan&#39;, 1), (&#39;bg&#39;, 1), (&#39;apquire$&#39;, 1), (&#39;gtube&#39;, 1), (&#39;chafe&#39;, 1), (&#39;lubricates&#39;, 1), (&#39;lubricate&#39;, 1), (&#39;flights/vacation&#39;, 1), (&#39;lambdin&#39;, 1), (&#39;trafalgar&#39;, 1), (&#39;bloodedly&#39;, 1), (&#39;undifferentiated&#39;, 1), (&#39;code--no&#39;, 1), (&#39;exciting--because&#39;, 1), (&#39;nicaea&#39;, 1), (&#39;full-grown&#39;, 1), (&#39;yon&#39;, 1), (&#39;medium-to-large&#39;, 1), (&#39;ms-supplied&#39;, 1), (&#39;well@@&#39;, 1), (&#39;in-touch&#39;, 1), (&#39;*ms&#39;, 1), (&#39;specialist*&#39;, 1), (&#39;schapiro&#39;, 1)] . len(token_counter) . 75889 . token_counter[&#39;well@@&#39;], token_counter[&#39;well&#39;] . (1, 5418) . . Important: There were 78,157 unique tokens! It&#8217;s interesting seeing that some of them are typos, or just have additional characters attached to the word, such as well@@ instead of well . Now that we have our token frequency counter, we can go ahead and make our vocab! . sorted_counter = dict(token_counter.most_common()) # Create vocab containing tokens with a minimum frequency of 20 my_vocab = torchtext.vocab.vocab(sorted_counter, min_freq=20) # Add the unknown token, and use this by default for unknown words unk_token = &#39;&lt;unk&gt;&#39; my_vocab.insert_token(unk_token, 0) my_vocab.set_default_index(0) # Add the pad token pad_token = &#39;&lt;pad&gt;&#39; my_vocab.insert_token(pad_token, 1) # Show vocab size, and examples of tokens len(my_vocab.get_itos()), my_vocab.get_itos()[:25] . (7591, [&#39;&lt;unk&gt;&#39;, &#39;&lt;pad&gt;&#39;, &#39;.&#39;, &#39;the&#39;, &#39;,&#39;, &#39;i&#39;, &#39;and&#39;, &#39;a&#39;, &#39;to&#39;, &#39;it&#39;, &#39;of&#39;, &#34;&#39;&#34;, &#39;this&#39;, &#39;is&#39;, &#39;in&#39;, &#39;that&#39;, &#39;for&#39;, &#39;was&#39;, &#39;you&#39;, &#39;!&#39;, &#39;book&#39;, &#39;s&#39;, &#39;but&#39;, &#39;with&#39;, &#39;not&#39;]) . . Important: We&#8217;ll be using &lt;unk&gt; as our default token for tokens that are out of our vocab! . . Important: Notice how we passed in a min_freq argument. This ensures that the vocab only includes high frequency tokens. We wouldn&#8217;t want to include tokens that only occur once/rarely. This brought our vocab count down from 75,889 to 7,591! A ~90% reduction! . Rather than starting from scratch, we can preload GloVe embeddings into our vocabulary! . glove = torchtext.vocab.GloVe(name = &#39;6B&#39;, dim = 100) glove.vectors.shape . torch.Size([400000, 100]) . Since we&#39;re using GloVe vectors for transfer learning (by preloading our embedding), let&#39;s take a look at how many tokens can be successfully transferred from GloVe into our own vocab. Each token will have an embedding (vector) of size 100. This results in an embedding of size 7591x100 . my_vocab.vectors = glove.get_vecs_by_tokens(my_vocab.get_itos()) my_vocab.vectors.shape . torch.Size([7591, 100]) . By default, tokens that aren&#39;t able to transfer from GloVe into our own dataset get initialized with a vector of 0&#39;s. We can use this to count how many tokens were successfully preloaded! . tot_transferred = 0 for v in my_vocab.vectors: if not v.equal(torch.zeros(100)): tot_transferred += 1 tot_transferred, len(my_vocab) . (7517, 7591) . . Important: Not bad, 7517 out of our 7591 tokens were successfully transferred! . my_vocab.get_itos()[3], my_vocab.vectors[3] . (&#39;the&#39;, tensor([-0.0382, -0.2449, 0.7281, -0.3996, 0.0832, 0.0440, -0.3914, 0.3344, -0.5755, 0.0875, 0.2879, -0.0673, 0.3091, -0.2638, -0.1323, -0.2076, 0.3340, -0.3385, -0.3174, -0.4834, 0.1464, -0.3730, 0.3458, 0.0520, 0.4495, -0.4697, 0.0263, -0.5415, -0.1552, -0.1411, -0.0397, 0.2828, 0.1439, 0.2346, -0.3102, 0.0862, 0.2040, 0.5262, 0.1716, -0.0824, -0.7179, -0.4153, 0.2033, -0.1276, 0.4137, 0.5519, 0.5791, -0.3348, -0.3656, -0.5486, -0.0629, 0.2658, 0.3020, 0.9977, -0.8048, -3.0243, 0.0125, -0.3694, 2.2167, 0.7220, -0.2498, 0.9214, 0.0345, 0.4674, 1.1079, -0.1936, -0.0746, 0.2335, -0.0521, -0.2204, 0.0572, -0.1581, -0.3080, -0.4162, 0.3797, 0.1501, -0.5321, -0.2055, -1.2526, 0.0716, 0.7056, 0.4974, -0.4206, 0.2615, -1.5380, -0.3022, -0.0734, -0.2831, 0.3710, -0.2522, 0.0162, -0.0171, -0.3898, 0.8742, -0.7257, -0.5106, -0.5203, -0.1459, 0.8278, 0.2706])) . my_vocab.get_itos()[6555], my_vocab.vectors[6555] . (&#39;eargels&#39;, tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])) . . Note: Note that the vector for index 3 corresponds to the word/token the and it has non-zero values in the vector since we were able to preload it from GloVe! Index 6559 corresponds to the word eargels and it&#8217;s all zeros, which means that it wasn&#8217;t part of Glove&#8217;s vocab and therefore couldn&#8217;t transfer it&#8217;s embedding to our vocab! . . Note: There were 400k tokens/words in GloVe, and stardust wasn&#8217;t one of them :O . . Warning: Initializing the embeddings to 0 isn&#8217;t the best idea. If we use these embeddings for transfer learning to our embedding layer, then all new tokens not found in GloVe will have an indentical vector representation of all 0s. I found the model to be harder to train because of this, and I concluded that it&#8217;s due to these 0s being multiplied in our model&#8217;s forward pass which creates more 0s, and possibly cause some gradients to be 0, making it much more difficult to learn these new embeddings. . . Tip: Rather than initializing new token embeddings to 0&#8217;s, let&#8217;s use torch.randn to create some diversity between the different token embeddings that weren&#8217;t preloaded with GloVe! . for i in range(my_vocab.vectors.shape[0]): if my_vocab.vectors[i].equal(torch.zeros(100)): my_vocab.vectors[i] = torch.randn(100) . Now let&#39;s use our vocab to numericalize our tokens! . sample_text = train_df[&#39;text&#39;][0] sample_text . &#39;This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^&#39; . tokens = L(tokenizer(sample_text)) tokens . (#81) [&#39;this&#39;,&#39;sound&#39;,&#39;track&#39;,&#39;was&#39;,&#39;beautiful&#39;,&#39;!&#39;,&#39;it&#39;,&#39;paints&#39;,&#39;the&#39;,&#39;senery&#39;...] . We can use our vocab to convert each token to it&#39;s numeric representation one-by-one using a list comprehension! . numericalized_tokens = [my_vocab[token] for token in tokens] numericalized_tokens = torch.tensor(numericalized_tokens) numericalized_tokens . tensor([ 12, 222, 505, 17, 405, 19, 9, 5931, 3, 0, 14, 74, 424, 34, 85, 5, 49, 1490, 9, 78, 8, 120, 70, 704, 0, 2, 255, 133, 19, 5, 27, 545, 3, 255, 0, 2380, 22, 55, 10, 33, 10, 3, 801, 5, 27, 136, 545, 9, 53, 3, 107, 133, 19, 9, 0, 274, 45, 3843, 0, 6, 433, 7, 0, 1368, 23, 0, 3306, 6, 3844, 0, 2, 9, 49, 4234, 183, 70, 2260, 8, 344, 19, 0]) . &#39; &#39;.join([my_vocab.get_itos()[num] for num in numericalized_tokens]) . &#39;this sound track was beautiful ! it paints the &lt;unk&gt; in your mind so well i would recomend it even to people who hate &lt;unk&gt; . game music ! i have played the game &lt;unk&gt; cross but out of all of the games i have ever played it has the best music ! it &lt;unk&gt; away from crude &lt;unk&gt; and takes a &lt;unk&gt; step with &lt;unk&gt; guitars and soulful &lt;unk&gt; . it would impress anyone who cares to listen ! &lt;unk&gt;&#39; . numericalized_tokens.shape . torch.Size([81]) . . Note: Notice how uncommon words have value 0 in their numericalized form! This corresponds to the &lt;unk&gt; token. . This example has 81 tokens, but other examples may have more or less. It&#39;ll be a good idea to cap the number of tokens + pad the amount of tokens to a desired number of tokens. This will be needed in order to batch our samples together, as they can&#39;t vary in size! . max_tokens = 128 . numericalized_tokens = [my_vocab[token] for token in tokens] if len(numericalized_tokens) &lt; max_tokens: numericalized_tokens += [1] * (max_tokens-len(numericalized_tokens)) else: numericalized_tokens = numericalized_tokens[:max_tokens] numericalized_tokens = torch.tensor(numericalized_tokens) numericalized_tokens . tensor([ 12, 222, 505, 17, 405, 19, 9, 5931, 3, 0, 14, 74, 424, 34, 85, 5, 49, 1490, 9, 78, 8, 120, 70, 704, 0, 2, 255, 133, 19, 5, 27, 545, 3, 255, 0, 2380, 22, 55, 10, 33, 10, 3, 801, 5, 27, 136, 545, 9, 53, 3, 107, 133, 19, 9, 0, 274, 45, 3843, 0, 6, 433, 7, 0, 1368, 23, 0, 3306, 6, 3844, 0, 2, 9, 49, 4234, 183, 70, 2260, 8, 344, 19, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]) . . Important: Remember that the pad token is represented by a 1 in it&#8217;s numericalized form! . 3. Dataset &amp; DataLoaders [PyTorch &amp; fastai] . Now that we have everything we need to tokenize and numericalize our input, let&#39;s go ahead and make a simple Dataset class . from torch import nn class Simple_Dataset(torch.utils.data.Dataset): def __init__(self, df, vocab, max_tokens): self.df = df self.vocab = vocab self.max_length = max_tokens self.tokenizer = get_tokenizer(&quot;basic_english&quot;) # label 1 is negative sentiment and label 2 is positive sentiment self.label_map = {1:0, 2:1} def __len__(self): return len(self.df) def decode(self, numericalized_tokens): return &#39; &#39;.join([self.vocab.get_itos()[num] for num in numericalized_tokens]) def __getitem__(self, index): label, title, text = self.df.iloc[index] label = self.label_map[label] label = torch.tensor(label) tokens = tokenizer(text) numericalized_tokens = [my_vocab[token] for token in tokens] if len(numericalized_tokens) &lt; max_tokens: numericalized_tokens += [1] * (max_tokens-len(numericalized_tokens)) else: numericalized_tokens = numericalized_tokens[:max_tokens] numericalized_tokens = torch.tensor(numericalized_tokens) return numericalized_tokens, label . . train_dataset = Simple_Dataset(train_df, vocab=my_vocab, max_tokens=128) valid_dataset = Simple_Dataset(valid_df, vocab=my_vocab, max_tokens=128) len(train_dataset), len(valid_dataset) . (40000, 2000) . . Note: 40k training samples and 2k validation samples. Feel free to try this on a larger dataset! . tokens, label = train_dataset[0] tokens, label . (tensor([ 12, 222, 505, 17, 405, 19, 9, 5931, 3, 0, 14, 74, 424, 34, 85, 5, 49, 1490, 9, 78, 8, 120, 70, 704, 0, 2, 255, 133, 19, 5, 27, 545, 3, 255, 0, 2380, 22, 55, 10, 33, 10, 3, 801, 5, 27, 136, 545, 9, 53, 3, 107, 133, 19, 9, 0, 274, 45, 3843, 0, 6, 433, 7, 0, 1368, 23, 0, 3306, 6, 3844, 0, 2, 9, 49, 4234, 183, 70, 2260, 8, 344, 19, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]), tensor(1)) . train_dataset.decode(tokens) . &#39;this sound track was beautiful ! it paints the &lt;unk&gt; in your mind so well i would recomend it even to people who hate &lt;unk&gt; . game music ! i have played the game &lt;unk&gt; cross but out of all of the games i have ever played it has the best music ! it &lt;unk&gt; away from crude &lt;unk&gt; and takes a &lt;unk&gt; step with &lt;unk&gt; guitars and soulful &lt;unk&gt; . it would impress anyone who cares to listen ! &lt;unk&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;&#39; . We can now create our fastai DataLoaders dls to use for later! . train_dl = DataLoader(train_dataset, bs=32, shuffle=True) valid_dl = DataLoader(valid_dataset, bs=32) dls = DataLoaders(train_dl, valid_dl) dls . &lt;fastai.data.core.DataLoaders at 0x7f19b2962130&gt; . 4. Model [PyTorch] . Now to the model creation section! Our PyTorch model will contain the following layers/components: . Embedding Layer: converts numericalized tokens into their embedding representation | LSTM: processes the sequence of embeddings | Head: Takes final feature vector of LSTM for classification prediction | . class Model(nn.Module): def __init__(self, vocab, num_classes): super().__init__() vocab_size, emb_size = vocab.vectors.shape self.emb = nn.Embedding(vocab_size, emb_size, _weight=vocab.vectors) self.lstm = nn.LSTM(input_size = emb_size, hidden_size = 64, batch_first = True, num_layers = 2) self.head = nn.Sequential(nn.Linear(128, 64), nn.ReLU(), nn.Linear(64, num_classes)) def forward(self, batch_data): token_embs = self.emb(batch_data) outputs, (h_n, c_n) = self.lstm(token_embs) # Assuming a batch size of 32, h_n will have a shape of: # shape = 2, 32, 64 last_hidden_state = h_n # shape = 32, 2, 64 last_hidden_state = last_hidden_state.permute(1,0,2) # shape = 32, 128 last_hidden_state = last_hidden_state.flatten(start_dim=1) logits = self.head(last_hidden_state) return logits . . . Warning: If you forget the _weight=vocab.vectors in the PyTorch nn.Embedding() layer creation function, you&#8217;ll simply initialize your embedding with all random numbers &amp; nothing will transfer from GloVe! Feel free to comment out that bit and see how the performance drops due to the lack of transfer learning! . model = Model(my_vocab, num_classes=2) model . Model( (emb): Embedding(7591, 100) (lstm): LSTM(100, 64, num_layers=2, batch_first=True) (head): Sequential( (0): Linear(in_features=128, out_features=64, bias=True) (1): ReLU() (2): Linear(in_features=64, out_features=2, bias=True) ) ) . Let&#39;s double check that some of our embeddings were successfully loaded from the domain-overlapping tokens from GloVe. Below is our preloaded embedding matrix! . embedding_matrix = list(model.emb.parameters())[0] embedding_matrix . Parameter containing: tensor([[-0.4916, 1.7088, 0.7117, ..., 0.1416, 0.9630, 0.4806], [ 0.4609, -0.7693, 0.0846, ..., -0.5245, 0.7817, -0.2093], [-0.3398, 0.2094, 0.4635, ..., -0.2339, 0.4730, -0.0288], ..., [ 0.2659, 0.1006, -0.1915, ..., 0.4461, 0.2491, 0.0310], [ 0.2600, 0.0077, 0.6122, ..., 0.1818, -0.2181, 0.0772], [-0.2763, -0.2575, -0.1396, ..., 0.1692, -0.1993, 0.4247]], requires_grad=True) . Index 3 corresponds to &#39;the&#39;: . my_vocab.vectors[3].equal(embedding_matrix[3]) . True . . Important: Our pre-loaded embedding parameters for index 3 matches up with our vocab vectors, nice! . total_params = 0 for p in model.parameters(): total_params += p.numel() total_params . 843262 . . Important: ~865k parameters! . Now let&#39;s go ahead and make sure we can do a forward pass through our model, our loss function will be CrossEntropyLoss as it&#39;s a classification task. . batched_data, batched_labels = train_dl.one_batch() print(batched_data.shape, batched_labels.shape) . torch.Size([32, 128]) torch.Size([32]) . with torch.no_grad(): logits = model(batched_data) logits.shape . torch.Size([32, 2]) . loss_func = nn.CrossEntropyLoss() loss = loss_func(logits, batched_labels) loss . tensor(0.7026) . Sweet! . 5. Training/Fitting [fastai] . Time to use fastai to contain our dls, model, and metrics &amp; assist with training using certain best practices! . learn = Learner(dls, model, loss_func=nn.CrossEntropyLoss(), metrics=[accuracy]) learn . &lt;fastai.learner.Learner at 0x7f5c4113b3d0&gt; . learn.lr_find() . SuggestedLRs(valley=tensor(0.0052)) . learn.fit_one_cycle(5, lr_max=3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.426767 | 0.401867 | 0.826000 | 01:52 | . 1 | 0.282645 | 0.295986 | 0.874500 | 02:15 | . 2 | 0.226144 | 0.283338 | 0.878000 | 02:46 | . 3 | 0.145605 | 0.325866 | 0.875500 | 02:43 | . 4 | 0.125661 | 0.348530 | 0.875500 | 02:38 | . Nice, but this model may have started overfitting near the end of training as it doesn&#39;t have any dropout, weight-decay, or other forms of regularization! . 6. Using a Language Model via AWD-LSTM [fastai] . Using a pretrained language model for downstream tasks is a popular and efficient technique also! Fine-tuning the language model first is even better, as shown in chapter 10 from fastbook . Here&#39;s a quick example of training a model with this dataset using fastai! . First we&#39;ll need to create our vocab as we did before! . fastai_vocab = make_vocab(token_counter) . To continue using the same subset dataframes, we&#39;ll combine both the train_df and valid_df into combined_df, then let fastai split at index 40k by using the splitter argument in the DataBlocks API! . combined_df = pd.concat([train_df, valid_df]) combined_df.head() . label title text . 0 2 | Stuning even for the non-gamer | This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^ | . 1 2 | The best soundtrack ever to anything. | I&#39;m reading a lot of reviews saying that this is the best &#39;game soundtrack&#39; and I figured that I&#39;d write a review to disagree a bit. This in my opinino is Yasunori Mitsuda&#39;s ultimate masterpiece. The music is timeless and I&#39;m been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny. | . 2 2 | Amazing! | This soundtrack is my favorite music of all time, hands down. The intense sadness of &quot;Prisoners of Fate&quot; (which means all the more if you&#39;ve played the game) and the hope in &quot;A Distant Promise&quot; and &quot;Girl who Stole the Star&quot; have been an important inspiration to me personally throughout my teen years. The higher energy tracks like &quot;Chrono Cross ~ Time&#39;s Scar~&quot;, &quot;Time of the Dreamwatch&quot;, and &quot;Chronomantique&quot; (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer&#39;s work (I haven&#39;t heard the Xenogears s... | . 3 2 | Excellent Soundtrack | I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it&#39;s truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon&#39;s Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Uns... | . 4 2 | Remember, Pull Your Jaw Off The Floor After Hearing it | If you&#39;ve played the game, you know how divine the music is! Every single song tells a story of the game, it&#39;s that good! The greatest songs are without a doubt, Chrono Cross: Time&#39;s Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper. | . len(combined_df) . 42000 . amazon_polarity = DataBlock(blocks=(TextBlock.from_df(&#39;text&#39;, seq_len=128, vocab=fastai_vocab), CategoryBlock), get_x=ColReader(&#39;text&#39;), get_y=ColReader(&#39;label&#39;), splitter=IndexSplitter(range(40000, 42000))) # Passing a custom DataFrame in and splitting by the index! dls = amazon_polarity.dataloaders(combined_df, bs=32) . len(dls.train_ds), len(dls.valid_ds) . (40000, 2000) . . Note: Looks good! We still have our 40k training samples and 2k validation samples :) . len(dls.train), len(dls.valid) . (1250, 63) . dls.train.show_batch() . text category . 0 xxbos xxup buyer xxup beware ! ! ! yes xxup the xxup diapers xxup are xxup good xxup but i xxup received xxup the xxup diapers xxup with xxup stains xxup they xxup look xxup like xxup dna xxup stains . xxup so i xxup call xxup the xxup company xxup and xxup they xxup instruct xxup me xxup to xxup xxunk xxup them xxup away xxup all 200 xxup diapers xxup wow . xxup so i xxup wait xxup in xxup the xxup mail xxup for xxup some xxup coupons xxup they xxup are xxup supposed xxup to xxup send xxup me xxup and xxup turns xxup out i xxup have xxup to xxup spend xxup more xxup money xxup in xxup order xxup to xxup get xxup the xxup diapers i xxup had xxup already xxup paid xxup for . xxup so i xxup call xxup company xxup | 1 | . 1 xxbos xxup this xxup is xxup one xxup of xxup the xxup stupidest , xxup silly , xxup totally xxup xxunk xxup movies i xxup have xxup ever xxup seen . i xxup have xxup always xxup loved xxunk disasters xxup in xxup nature xxup movies xxunk . i xxup was xxup not xxup disapointed . xxup this xxup was xxup truly a xxup horrific xxup catastrophe . xxup the xxup best xxup part xxup of xxup the xxup movie xxup was xxunk xxunk was xxunk xxunk xxup was xxunk xxunk i xxup am xxup getting xxup there xxunk xxunk xxunk xxunk xxunk xxup the xxup story xxup had xxup no xxup meaning . xxup xxunk xxup xxunk xxup should xxup sell xxup xxunk xxup tickets xxup the xxup next xxup time xxup she xxup needs xxup money . xxup her xxup talents xxup were xxup totally xxup wasted . | 1 | . 2 xxbos xxup if xxup you xxup take a xxup look xxup at xxup the 1 xxup star xxup review , i xxup am xxup the xxup one xxup that xxup left a xxup comment xxup in xxup favor , xxup but xxup wondering xxup why xxup they xxup left xxup only xxup one xxup star . xxup it xxup had xxup to xxup be a xxup mistake xxup because xxup she xxup gave xxup it a xxunk haunting xxunk xxup review , xxup if xxup not xxup too xxup short . xxup the xxup tune xxunk &#39; estate &#39; xxup is xxup with xxup out a xxup doubt a xxup five xxup star xxup tune . xxup in xxup fact , xxup one xxup of xxup the xxup most xxup beautiful xxup jazz xxup vocal xxup numbers i xxup can xxup remember xxup hearing . xxup however , xxup the | 2 | . 3 xxbos xxmaj some may ask why , but i say bring on a collection like this . xxmaj why not ? xxmaj this is xxunk music . xxmaj is it techno or electronica , who cares , just dance and enjoy . xxmaj my biggest complaints are , why six tracks from xxunk homework xxunk , and only three from xxunk discovery xxunk ? xxmaj also , some of these songs like xxunk rollin &#39; &amp; xxmaj xxunk &#39; xxunk and xxunk robot xxmaj rock xxunk are xxup so repetitive ( i know that xxunk the point ) , they get old by the three minute mark . xxmaj yet , other tracks like xxunk around xxmaj the xxmaj world xxunk and xxunk one xxmaj more xxmaj time xxunk are perfection , hence the classics they are . xxmaj one other disappointment xxunk xxunk the hell is xxunk digital xxmaj | 2 | . 4 xxbos i absolutely loved this xxup cd and i think everyone in america should own a copy ( or maybe xxunk and my friends have been behind clay since the beginning ( well almost the beginning ) and were so excited when he got back on in the wild card show . i was so excited when this xxup cd came out . i love every song on this xxup cd and listen to it 24 / 7 ( more like 25 / xxunk ! xxmaj this is by far the best xxup cd i xxunk bought in years . xxmaj it was worth every penny . i ca nt wait for clays next xxup cd ! xxmaj people of all ages can enjoy clay xxunk music . xxmaj some of my favorites are xxmaj invisible , i will carry you , the way , when you say you love | 2 | . 5 xxbos xxmaj this is one of the worst books in the world xxrep 3 - in my humble opinion ! xxup ok , xxmaj i xxunk in the minority . xxmaj most of the reviewers xxup loved the book . i think this book is a waste of beautiful words ! xxmaj yes , xxmaj conrad had a beautiful xxmaj english vocabulary . xxmaj but , the xxunk story xxunk is xxunk xxunk xxunk with so many words you xxunk tire of it after awhile . xxup ok , try the book , but now , at least , you xxunk xxunk feel as i did , xxunk what xxunk wrong with xxup me that i do xxunk like this book . xxunk xxmaj you xxunk xxunk feel xxunk alone xxunk in your reaction . i think xxunk heart of xxmaj darkness xxunk is a much better book . xxmaj | 1 | . 6 xxbos xxmaj first , xxmaj i xxunk read all the books in the series and enjoyed them . xxmaj okay xxunk plains of xxmaj passage xxunk was a boring read but not too bad . xxmaj in xxunk the xxmaj shelters of xxmaj stone xxunk xxmaj jean xxmaj auel makes xxmaj ayla out to be some kind of super - woman , who invented open heart surgery to figuring out xxunk where babies come from . xxunk xxunk ? xxmaj it xxunk gone too far . i think the long wait between books as made me lose interest . xxmaj either that or xxmaj ms . xxmaj auel ca xxunk write xxmaj ayla anymore . xxmaj she lost her momentum . xxmaj at the beginning it was fun to see xxmaj ayla invent all this stuff , but the fun is gone . i suppose this can happen with a | 1 | . 7 xxbos i am like you at this very moment , you do xxunk want to believe the reviews you are reading . . xxunk they are xxup xxunk read all the reviews here before i purchased xxunk anne of xxmaj green gables - the xxmaj continuing xxmaj story xxunk . i did xxunk want to believe what i read , because this is xxmaj anne of xxmaj green xxmaj gables , the last two videos were xxup great , why would xxunk this one be too ? xxup it xxup is xxup not xxup worth xxup it ! , please trust me on xxunk beginning part of the movie is really nice to watch . . xxunk the war begins , xxmaj gilbert leaves and xxmaj anne leaves to go find xxmaj gilbert . xxmaj and i am waiting and waiting for xxmaj anne to come back to xxmaj avonlea | 1 | . 8 xxbos xxmaj chris xxunk . xxmaj chris was my next door neighbor on 13th floor of xxmaj xxunk hall at xxmaj xxunk xxmaj state xxmaj university in 1977 . xxmaj chris xxunk favorite album was xxunk smoke xxmaj from a xxmaj distant xxmaj fire xxunk from the xxmaj sandford and xxmaj townsend xxmaj band . i listened to it so much when i was in college in xxmaj chris xxunk room that i learned each one of the ten songs by heart . xxmaj try listening to the ending fugue of xxunk squire xxmaj james xxunk at 3 xxunk sometime when you xxunk been up all night partying - it was enough to make you rethink your xxunk youth . xxmaj the lyrics of this album are sometimes repetitive and simplistic . xxmaj for example , count the number of times the themes of smoke , fire , burning , | 2 | . . Tip: Take a close look at the samples above! We can see that we&#8217;re not using a simple tokenizer that just separates on spaces anymore! This tokenizer has lots of additional special tokens and special rules to help with better feature/token engeineering! (e.g. xxbos -&gt; beginning of a text, and xxmaj -&gt; next word capitalized) More can be found here in chapter 10 from fastbook . learn = text_classifier_learner(dls, AWD_LSTM, metrics=[accuracy]) . . Note: We&#8217;ll be using the AWD-LSTM language model as our pretrained model to finetune! . learn.lr_find() . SuggestedLRs(valley=tensor(0.0036)) . learn.fine_tune(5, base_lr=3e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.510850 | 0.458526 | 0.777000 | 00:45 | . epoch train_loss valid_loss accuracy time . 0 | 0.391805 | 0.325548 | 0.858000 | 01:53 | . 1 | 0.323382 | 0.286508 | 0.883500 | 01:53 | . 2 | 0.265536 | 0.264187 | 0.892000 | 01:54 | . 3 | 0.230185 | 0.256429 | 0.898500 | 01:54 | . 4 | 0.202922 | 0.268608 | 0.897500 | 01:54 | . . Important: Compare the results of this model with the previous one we built with pure PyTorch. We can see regularization come into effect! . 7. Using a Language Model via DistilBERT [HuggingFace &amp; PyTorch &amp; fastai] . We can load up a tokenizer and transformer from HuggingFace&#39;s Transformers API and train them using fastai! We&#39;ll be using DistilBERT as it&#39;s smaller and faster than the original BERT. . &quot;DistilBERT is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than bert-base-uncased, runs 60% faster while preserving over 95% of BERT’s performances as measured on the GLUE language understanding benchmark.&quot; . from transformers import AutoTokenizer, AutoModelForSequenceClassification hf_tokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;) hf_model = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;) . sample_text = train_df[&#39;text&#39;][0] sample_text . &#39;This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^&#39; . tokenizer_outputs = hf_tokenizer(sample_text, return_tensors=&quot;pt&quot;) tokenizer_outputs . {&#39;input_ids&#39;: tensor([[ 101, 2023, 2614, 2650, 2001, 3376, 999, 2009, 23262, 1996, 12411, 7301, 1999, 2115, 2568, 2061, 2092, 1045, 2052, 28667, 8462, 4859, 2009, 2130, 2000, 2111, 2040, 5223, 6819, 2094, 1012, 2208, 2189, 999, 1045, 2031, 2209, 1996, 2208, 10381, 4948, 2080, 2892, 2021, 2041, 1997, 2035, 1997, 1996, 2399, 1045, 2031, 2412, 2209, 2009, 2038, 1996, 2190, 2189, 999, 2009, 10457, 2185, 2013, 13587, 9019, 2075, 1998, 3138, 1037, 4840, 2121, 3357, 2007, 24665, 3686, 7334, 1998, 3969, 3993, 19505, 1012, 2009, 2052, 17894, 3087, 2040, 14977, 2000, 4952, 999, 1034, 1035, 1034, 102]]), &#39;attention_mask&#39;: tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])} . tokenizer_outputs[&#39;input_ids&#39;].shape, tokenizer_outputs[&#39;attention_mask&#39;].shape . (torch.Size([1, 95]), torch.Size([1, 95])) . . Note: We can see that bert&#8217;s tokenizer returns 2 items, input_ids (numericalization of tokens) and attention_mask (manually lets you control attention on specific tokens). DistilBERT will take both of these as input! . Let&#39;s go ahead and put this into another dataset class: . class HF_Dataset(torch.utils.data.Dataset): def __init__(self, df, hf_tokenizer): self.df = df self.hf_tokenizer = hf_tokenizer # label 1 is negative sentiment and label 2 is positive sentiment self.label_map = {1:0, 2:1} def __len__(self): return len(self.df) def decode(self, token_ids): return &#39; &#39;.join([hf_tokenizer.decode(x) for x in tokenizer_outputs[&#39;input_ids&#39;]]) def decode_to_original(self, token_ids): return self.hf_tokenizer.decode(token_ids.squeeze()) def __getitem__(self, index): label, title, text = self.df.iloc[index] label = self.label_map[label] label = torch.tensor(label) tokenizer_output = self.hf_tokenizer(text, return_tensors=&quot;pt&quot;, padding=&#39;max_length&#39;, truncation=True, max_length=512) tokenizer_output[&#39;input_ids&#39;].squeeze_() tokenizer_output[&#39;attention_mask&#39;].squeeze_() return tokenizer_output, label . . train_dataset = HF_Dataset(train_df, hf_tokenizer) valid_dataset = HF_Dataset(valid_df, hf_tokenizer) len(train_dataset), len(valid_dataset) . (40000, 2000) . tokenizer_outputs, label = train_dataset[0] tokenizer_outputs.keys(), label . (dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;]), tensor(1)) . train_dataset.decode(tokenizer_outputs[&#39;input_ids&#39;])[:500] . &#39;[CLS] this sound track was beautiful ! it paints the sen ##ery in your mind so well i would rec ##ome ##nd it even to people who hate vi ##d . game music ! i have played the game ch ##ron ##o cross but out of all of the games i have ever played it has the best music ! it backs away from crude keyboard ##ing and takes a fresh ##er step with gr ##ate guitars and soul ##ful orchestras . it would impress anyone who cares to listen ! ^ _ ^ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [&#39; . . Tip: Notice how DistilBERT also has a more efficient tokenizer compared to the standard tokenizer we used at the start. We can see it&#8217;s using subword tokenization (e.g. keyboarding -&gt; keyboard ##ing, as well as fresher -&gt; fresh ##er) . Here&#39;s the original input (tokens decoded, but without the subword tokenization showing): . train_dataset.decode_to_original(tokenizer_outputs[&#39;input_ids&#39;])[:500] . &#39;[CLS] this sound track was beautiful! it paints the senery in your mind so well i would recomend it even to people who hate vid. game music! i have played the game chrono cross but out of all of the games i have ever played it has the best music! it backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. it would impress anyone who cares to listen! ^ _ ^ [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [&#39; . train_dl = DataLoader(train_dataset, bs=16, shuffle=True) valid_dl = DataLoader(valid_dataset, bs=16) dls = DataLoaders(train_dl, valid_dl) . Let&#39;s make sure that items within the tokenizer_outputs dictionary can get batched together properly: . batched_data, batched_labels = train_dl.one_batch() batched_data.keys(), batched_data[&#39;input_ids&#39;].shape, batched_labels.shape . (dict_keys([&#39;input_ids&#39;, &#39;attention_mask&#39;]), torch.Size([16, 512]), torch.Size([16])) . To allow this model to be trained by fastai, we need to ensure that the model simply takes a single input, and returns the logits. We can create a small class to handle the intermediate stuff (like the decoupling of tokenizer_outputs via **tokenizer_outputs, and extracting the logits from the model output via .logits. Here&#39;s an example of a forward pass using HF&#39;s tokenizer and model: . hf_model(**batched_data) . SequenceClassifierOutput(loss=None, logits=tensor([[-0.0196, -0.0800], [-0.1307, -0.0872], [-0.0604, -0.0986], [-0.1189, -0.0459], [-0.0588, -0.0984], [-0.0342, -0.0900], [-0.1314, -0.1056], [ 0.0286, -0.0227], [-0.0680, -0.0357], [-0.0157, -0.0817], [-0.0831, -0.0794], [-0.0725, -0.0759], [-0.0438, -0.1197], [-0.0845, -0.1267], [-0.0698, -0.0478], [-0.1017, -0.0566]], grad_fn=&lt;AddmmBackward&gt;), hidden_states=None, attentions=None) . class HF_Model(nn.Module): def __init__(self, hf_model): super().__init__() self.hf_model = hf_model def forward(self, tokenizer_outputs): model_output = self.hf_model(**tokenizer_outputs) return model_output.logits . . model = HF_Model(hf_model) . With the same data, here&#39;s an example of a forward pass with our small wrapper over the hf_model . logits = model(batched_data) logits . tensor([[-0.0196, -0.0800], [-0.1307, -0.0872], [-0.0604, -0.0986], [-0.1189, -0.0459], [-0.0588, -0.0984], [-0.0342, -0.0900], [-0.1314, -0.1056], [ 0.0286, -0.0227], [-0.0680, -0.0357], [-0.0157, -0.0817], [-0.0831, -0.0794], [-0.0725, -0.0759], [-0.0438, -0.1197], [-0.0845, -0.1267], [-0.0698, -0.0478], [-0.1017, -0.0566]], grad_fn=&lt;AddmmBackward&gt;) . . Note: We no longer need double asteriks for decoupling the input data dictionary, and we don&#8217;t need to extract the logits from the output via .logits This allows for easy compatability with fastai&#8217;s Learner class . We have everything we need to finetune this model now! . # (doesn&#39;t automatically place model + data on gpu otherwise) learn = Learner(dls, model.cuda(), loss_func=nn.CrossEntropyLoss(), metrics=[accuracy]) . learn.lr_find() . SuggestedLRs(valley=tensor(3.6308e-05)) . learn.fit_one_cycle(3, 1e-4) . epoch train_loss valid_loss accuracy time . 0 | 0.240267 | 0.249942 | 0.897500 | 15:59 | . 1 | 0.142634 | 0.211986 | 0.921000 | 15:56 | . 2 | 0.068012 | 0.243243 | 0.926000 | 15:56 | . 8. Conclusion . We can take models written in pure PyTorch, or take existing models from elsewhere (e.g. HuggingFace), and train them with ease within fastai. | NLP has lots of variation in terms of tokenization methods. In my personal opinion*, libaries like fastai &amp; HuggingFace make the NLP data processing pipeline much easier/faster to get up and running! | Each method has it&#39;s pros and cons: DistilBERT may have performed better, but this model has a much much larger number of parameters and possibly a much larger vocabulary! | The runtime of DistilBERT was also much longer. ~16 minutes epochs (for a distilled transformer) vs ~2 min epochs (for a recurrent model) | In other tasks, it may be better to use a transformer, but for many common NLP tasks, sometimes a simpler/smaller model is good enough! | Even though transformers feed-forward it&#39;s input sequences in parallel (through positional encoding + self attention), they can be slower than simple recurrent networks due to their large model sizes. | . | . . Note: All the results in this blog post/notebook may differ between runs, and that a &quot;perfect&quot; hyperparameter search was not performed for any of these models. This tutorial was mainly done for showing examples of NLP pipelines using different libaries! . Thanks for reading!!! 🙂 .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/nlp/pytorch/fastai/huggingface/2021/06/27/NLP-from-Scratch-with-PyTorch-FastAI-and-HuggingFace.html",
            "relUrl": "/jupyter/nlp/pytorch/fastai/huggingface/2021/06/27/NLP-from-Scratch-with-PyTorch-FastAI-and-HuggingFace.html",
            "date": " • Jun 27, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Latent Space Exploration with StyleGAN2",
            "content": "Introduction &amp; Disclaimers . Welcome! . This notebook is an introduction to the concept of latent space, using a recent (and amazing) generative network: StyleGAN2 . Here are some great blog posts I found useful when learning about the latent space + StyleGAN2 . Latent Space Understanding: Ekin Tiu - Understanding Latent Space in Machine Learning | . A technical overview of the stylegan2 architecture: Connor Shorten - StyleGAN2 . | Overview of GANs + what&#39;s changed up to StyleGAN2: akira - From GAN basic to StyleGAN2 . | . . Tip: This stand-alone Jupyter notebook is all you need to get everything up and running! It&#8217;ll pull a (small) repo with everything that&#8217;s needed :D The Repo&#8217;s README.md contains original source links for the content! Link to Repo . . Tip: This notebook was successfully ran on Google Colab &amp; Gradient Paperspace&#8217;s TensorFlow 1.14 Container: every cell, from top-to-bottom! Feel free to run/experiment with this notebook yourselves! . . Warning: This tutorial is not too code-heavy in the deep learning/model aspects. Primarily because this tutorial uses the Official StyleGan2 Repo, which uses a depreciated version of Tensorflow (1.14). This blog post abstracts away from the depreciated TensorFlow code, and focuses more on the concepts of latent space and traversals :) . Experiment Layout . In this notebook, we will be experimenting with the following: . 0. Set-Up (Run only once!) . This section simply pulls the small repo containing necessary files needed to run things! | . 1. A Quick Review of GANs . A quick refresh of: z (latent vectors), Generators, and Discriminators. | GANs vs VAEs | . 2. Generate Images of People who don&#39;t Exist . Use the official StyleGAN2 repo to create Generator outputs. | View the latent codes of these generated outputs. | . 3. Interpolation of Latent Codes . Use the previous Generator outputs&#39; latent codes to morph images of people together. | . 4. Facial Image Alignment using Landmark Detection . Aligning (normalizing) our own input images for latent space projection. | . 5. Projecting our own Input Images into the Latent Space . Learning the latent codes of our new aligned input images. | Interpolation of projected latent codes. (Similar to Section 3, but with our images!) | . 6. Latent Directions/Controls to modify our projected images . Using pre-computed latent directions to alter facial features of our own images. | . 7. Bonus: Interactive Widget-App! . Play with latent controls yourself using this little jupyter app I built using ipywidgets. | . 0. Set-Up (Run only once!) . Clone Repo and extract contents . !git clone https://github.com/AmarSaini/Epoching_StyleGan2_Setup.git . import shutil from pathlib import Path repo_root = Path(&#39;Epoching_StyleGan2_Setup/&#39;) # Pull contents out of the repo, into our current directory. for content in repo_root.iterdir(): shutil.move(str(content), &#39;.&#39;) shutil.rmtree(repo_root) . Pip install needed packages . !pip install requests !pip install Pillow !pip install tqdm !pip install dlib . If you&#39;re running this on Google Colab, uncomment and run the following cell: . #!pip install tensorflow-gpu==1.14 . 1. A Quick Review of GANs . I&#39;m going to try to keep this section short, and just go over the needed information to understand the rest of this post: . GANs (Generative Adversarial Networks) consist of two models: . The Generator: A model that converts a latent code into some kind of output (an image of a person, in our case). | The Discriminator: A model that determines whether some input (an image of a person), is real or fake. Real: An image from the original dataset. | Fake: An image from the Generator. | . | . The input to a Generator is a latent code z, a vector of numbers if you will. (Such as: a vector of 512 numbers). . During training, the latent code is randomly sampled (i.e. a random vector of 512 numbers). | When this latent code is randomly sampled, we can call it a latent random variable, as shown in the figure below. | This magical latent code holds information that will allow the Generator to create a specific output. | If you can find a latent code for a particular input, you can represent it with smaller amounts of data! (Such as representing a picture of someone with only a latent vector of 512 numbers, as opposed to the original image size) | . . Important: Don&#8217;t confuse GANs with VAEs (Variational Auto-Encoders)! . . Note: GANs learn to generate outputs from random latent vectors that mimic the appearance of your input data, but not necessarily the exact samples of your input data. VAEs learn to encode your input samples into latent vectors, and then also learn to decode latent vectors back to it&#8217;s (mostly) original form. . . Tip: The main difference to takeaway from GANs vs VAEs, is that our Generator actually never sees the input images, hence we don&#8217;t have a way to automatically convert images into it&#8217;s corresponding latent code! Teaser: That&#8217;s what projection is for, Section 6 :) . . Source: https://www.slideshare.net/xavigiro/deep-learning-for-computer-vision-generative-models-and-adversarial-training-upc-2016 . 2. Generate Images of People who don&#39;t Exist . import sys sys.path.append(&#39;stylegan2/&#39;) from stylegan2 import pretrained_networks from stylegan2 import dnnlib from stylegan2.dnnlib import tflib from pathlib import Path from PIL import Image import pickle import numpy as np import ipywidgets as widgets from tqdm import tqdm model_path = &#39;gdrive:networks/stylegan2-ffhq-config-f.pkl&#39; fps = 20 results_size = 400 . . # Code to load the StyleGAN2 Model def load_model(): _G, _D, Gs = pretrained_networks.load_networks(model_path) noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith(&#39;noise&#39;)] Gs_kwargs = dnnlib.EasyDict() Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) Gs_kwargs.randomize_noise = False return Gs, noise_vars, Gs_kwargs # Generate images given a random seed (Integer) def generate_image_random(rand_seed): rnd = np.random.RandomState(rand_seed) z = rnd.randn(1, *Gs.input_shape[1:]) tflib.set_vars({var: rnd.randn(*var.shape.as_list()) for var in noise_vars}) images = Gs.run(z, None, **Gs_kwargs) return images, z # Generate images given a latent code ( vector of size [1, 512] ) def generate_image_from_z(z): images = Gs.run(z, None, **Gs_kwargs) return images . . Lets go ahead and start generating some outputs! . Gs, noise_vars, Gs_kwargs = load_model() . Downloading http://d36zk2xti64re0.cloudfront.net/stylegan2/networks/stylegan2-ffhq-config-f.pkl ... done Setting up TensorFlow plugin &#34;fused_bias_act.cu&#34;: Preprocessing... Compiling... Loading... Done. Setting up TensorFlow plugin &#34;upfirdn_2d.cu&#34;: Preprocessing... Compiling... Loading... Done. . images, latent_code1 = generate_image_random(42) image1 = Image.fromarray(images[0]).resize((results_size, results_size)) latent_code1.shape . (1, 512) . . Note: As shown in the previous cell&#8217;s output, we can see that the latent_code for this output is of size (1, 512). This means that the numbers inside latent_code1 can be used to create the image below! . image1 . Let&#39;s make another image! . images, latent_code2 = generate_image_random(1234) image2 = Image.fromarray(images[0]).resize((results_size, results_size)) latent_code2.shape . (1, 512) . latent_code1[0][:5], latent_code2[0][:5] . (array([ 0.49671415, -0.1382643 , 0.64768854, 1.52302986, -0.23415337]), array([ 0.47143516, -1.19097569, 1.43270697, -0.3126519 , -0.72058873])) . . Note: We can see the size of latent_code2 is also (1, 512). However, the two codes are not the same! This is seen in the first five values in the previous cell. Below is the corresponding image for generating output with latent_code2 . image2 . 3. Interpolation of Latent Codes . So what&#39;s the big deal? We have two codes to make two people that don&#39;t even exist right? Well, the cool thing about the latent space is that you can &quot;traverse&quot; through it! . Since the latent space is a compressed representation of some data, things that are similar in appearance should be &quot;close&quot; to each other in the latent space. . If the latent space is well developed, we can actually transition/interpolate between points in this space and create intermediate outputs! . In other words... we can morph two people together! See the gif below for a quick example! . . Source: https://hackernoon.com/latent-space-visualization-deep-learning-bits-2-bd09a46920df . Now let&#39;s do this on our examples we just generated! :D. . Let&#39;s interpolate halfway between latent_code1, latent_code2 . def linear_interpolate(code1, code2, alpha): return code1 * alpha + code2 * (1 - alpha) . interpolated_latent_code = linear_interpolate(latent_code1, latent_code2, 0.5) interpolated_latent_code.shape . (1, 512) . . Note: The latent_code size is still a vector of 512 numbers; We just took 50% of latent_code1 and 50% of latent_code2, (alpha=0.5), and summed them together! Below is the resulting image. . images = generate_image_from_z(interpolated_latent_code) Image.fromarray(images[0]).resize((results_size, results_size)) . Let&#39;s also make a cool interpolation animation; It&#39;ll help with visualizing the effect of interpolating from alpha=0 to alpha=1 . output_gifs_path = Path(&#39;output_gifs&#39;) # Make Output Gifs folder if it doesn&#39;t exist. if not output_gifs_path.exists(): output_gifs_path.mkdir() . . def get_concat_h(im1, im2): dst = Image.new(&#39;RGB&#39;, (im1.width + im2.width, im1.height)) dst.paste(im1, (0, 0)) dst.paste(im2, (im1.width, 0)) return dst def make_latent_interp_animation(code1, code2, img1, img2, num_interps): step_size = 1.0/num_interps all_imgs = [] amounts = np.arange(0, 1, step_size) for alpha in tqdm(amounts): interpolated_latent_code = linear_interpolate(code1, code2, alpha) images = generate_image_from_z(interpolated_latent_code) interp_latent_image = Image.fromarray(images[0]).resize((400, 400)) frame = get_concat_h(img1, interp_latent_image) frame = get_concat_h(frame, img2) all_imgs.append(frame) save_name = output_gifs_path/&#39;latent_space_traversal.gif&#39; all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0) . . make_latent_interp_animation(latent_code1, latent_code2, image1, image2, num_interps=200) . 100%|██████████| 200/200 [00:31&lt;00:00, 6.35it/s] . . Tip: If you&#8217;re running this notebook yourself, the interpolation gif will be saved in the following location: output_gifs/latent_space_traversal.gif :) . . . Note: This gif represents going from latent_code1 to latent_code2 by slowly changing alpha from 0 to 1. (increasing alpha by 1/200 per iteration, until it reaches 1.0) . 4. Facial Image Alignment using Landmark Detection . Ok so this is all fun and stuff right? How could we play around with our own images, instead of random people that don&#39;t exist? . Well, we first have to project our own images into this latent space. . . Important: The first step of projecting our own images is to make sure that they are representative of the training data. StyleGAN2 was trained on the FFHQ Dataset. More specifically, the images used during training were actually aligned first, before giving it to the discriminator in StyleGAN2. . To align (normalize) our images for StyleGAN2, we need to use a landmark detection model. This will automatically find the facial keypoints of interest, and crop/rotate accordingly. . Below is an example! . . Tip: At this point, if you want to run this with your own images, all you need to do is go to the imgs/ folder, and delete the example images, Jeremy_Howard.jpg and Obama.jpg. Then upload 2 of your own! . orig_img_path = Path(&#39;imgs&#39;) aligned_imgs_path = Path(&#39;aligned_imgs&#39;) # Make Aligned Images folder if it doesn&#39;t exist. if not aligned_imgs_path.exists(): aligned_imgs_path.mkdir() orig_img_path, aligned_imgs_path . . (PosixPath(&#39;imgs&#39;), PosixPath(&#39;aligned_imgs&#39;)) . if not Path(&#39;shape_predictor_68_face_landmarks.dat&#39;).exists(): !wget http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 !bzip2 -dv shape_predictor_68_face_landmarks.dat.bz2 . --2020-08-11 18:28:05-- http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 Resolving dlib.net (dlib.net)... 107.180.26.78 Connecting to dlib.net (dlib.net)|107.180.26.78|:80... connected. HTTP request sent, awaiting response... 200 OK Length: 64040097 (61M) Saving to: ‘shape_predictor_68_face_landmarks.dat.bz2’ shape_predictor_68_ 100%[===================&gt;] 61.07M 952KB/s in 3m 25s 2020-08-11 18:31:30 (306 KB/s) - ‘shape_predictor_68_face_landmarks.dat.bz2’ saved [64040097/64040097] shape_predictor_68_face_landmarks.dat.bz2: done . from align_face import align_face # Align all of our images using a landmark detection model! all_imgs = list(orig_img_path.iterdir()) for img in all_imgs: align_face(str(img)).save(aligned_imgs_path/(&#39;aligned_&#39;+img.name)) . Number of faces detected: 1 Detection 0: Left: 375 Top: 333 Right: 760 Bottom: 718 Part 0: (377, 507), Part 1: (391, 550) ... Number of faces detected: 1 Detection 0: Left: 1224 Top: 514 Right: 2022 Bottom: 1313 Part 0: (1333, 660), Part 1: (1323, 758) ... . Let&#39;s load the original + aligned images into Jupyter! . aligned_img_set = list(aligned_imgs_path.iterdir()) aligned_img_set.sort() aligned_img_set = [Image.open(x) for x in aligned_img_set] orig_img_set = list(orig_img_path.iterdir()) orig_img_set.sort() orig_img_set = [Image.open(x) for x in orig_img_set] . . . Note: Original image is on the left, Aligned image is on the right. The image size for the original images aren&#8217;t necessarily square. However, the output size of the aligned images is always 1024x1024, a square! StyleGAN2 uses square shapes :) . get_concat_h(orig_img_set[0], aligned_img_set[0]) . get_concat_h(orig_img_set[1], aligned_img_set[1]) . . Important: From these visualizations, we can see that this alignment process gives us a more focused view of the face, and orients the face upward/straight (removing any tilt of the person in the image) . 5. Projecting our own Input Images into the Latent Space . . Warning: At this point, I restarted my notebook kernel to clear my GPU card memory. The previous sections loaded models inside our notebook, so I ended up restarting the kernel to free space. This is recommended because the next section will be invoking official stylegan2 python scripts from inside notebook cells, and they load models interally (separately from this notebook). You&#8217;ll most likely run out of GPU memory if you don&#8217;t restart! Everything in this notebook will still run nicely from top-to-bottom, don&#8217;t worry! . You can either manually restart your kernel by going to Kernel -&gt; Restart, or run the below cell: . # Automatically restart the kernel by running this cell import IPython IPython.Application.instance().kernel.do_shutdown(True) . . {&#39;status&#39;: &#39;ok&#39;, &#39;restart&#39;: True} . import sys sys.path.append(&#39;stylegan2/&#39;) from stylegan2 import pretrained_networks from stylegan2 import dnnlib from stylegan2.dnnlib import tflib from pathlib import Path from PIL import Image import pickle import numpy as np import ipywidgets as widgets from tqdm import tqdm model_path = &#39;gdrive:networks/stylegan2-ffhq-config-f.pkl&#39; fps = 20 results_size = 400 . . . Note: First, we&#8217;ll make an official stylegan2 compatible dataset, to ignore the warnings from deprecated TensorFlow 1.14, we use the -W ignore option when executing python on the command line. . !python -W ignore stylegan2/dataset_tool.py create_from_images datasets_stylegan2/custom_imgs aligned_imgs/ . Loading images from &#34;aligned_imgs/&#34; Creating dataset &#34;datasets_stylegan2/custom_imgs&#34; Added 2 images. . . Note: Now let&#8217;s project our previously aligned images into the latent space, and get those latent codes that will hopefully well-represent the images. . Some things to explain here: . Projecting an image into the latent space basically means: Let&#39;s figure out what latent code (512 numbers) will cause the generator to make an output that looks like our image | The question is, how do we figure out the latent code? With VAEs (variational autoencoder), we just throw our image through the encoder and we get our latent code just like that! | With GANs, we don&#39;t necessarily have a direct way to extract latent codes from an input image, but we can optimize for it. | In a nut shell, here&#39;s how we can optimize for a latent code for given input images: | . For as many iterations as you&#39;d like, do: . Ask the generator to generate some output from a starting latent vector. | Take the generator&#39;s output image, and take your target image, put them both through a VGG16 model (image feature extractor). | Take the generator&#39;s output image features from the VGG16. | Take the target image features from the VGG16. | Compute the loss on the difference of features! | Backprop | . Important: Note that the loss between the generator&#8217;s output image, and the target image, is in the feature space of a VGG16! Why? If we performed pixel-wise loss on just the raw image pixels, it won&#8217;t account for any facial features, just color of pixels. Checking the difference of features allows us to optimize for a latent code that doesn&#8217;t just check for pixel values, but also the patterns in the pixels that correspond to high-level features, such as cheekbones, eyebrows, nose size, eye width, smile, etc. . Let&#39;s project our 2 custom aligned images of Jeremy and Obama! . tot_aligned_imgs = 2 . !python -W ignore stylegan2/epoching_custom_run_projector.py project-real-images --network=$model_path --dataset=custom_imgs --data-dir=datasets_stylegan2 --num-images=$tot_aligned_imgs --num-snapshots 500 . Local submit - run_dir: results/00001-project-real-images dnnlib: Running run_projector.project_real_images() on localhost... Loading networks from &#34;gdrive:networks/stylegan2-ffhq-config-f.pkl&#34;... Setting up TensorFlow plugin &#34;fused_bias_act.cu&#34;: Preprocessing... Loading... Done. Setting up TensorFlow plugin &#34;upfirdn_2d.cu&#34;: Preprocessing... Loading... Done. Downloading http://d36zk2xti64re0.cloudfront.net/stylegan1/networks/metrics/vgg16_zhang_perceptual.pkl ... done Loading images from &#34;custom_imgs&#34;... Projecting image 0/2 ... 2020-08-11 18:46:58.427331: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set. If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU. To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile. Projecting image 1/2 ... dnnlib: Finished run_projector.project_real_images() in 28m 47s. . . Note: This cell will take a while to run! 1000 iterations for optimizing latent codes (per image) are occuring here, and we&#8217;re saving progress-images every 2 steps for a cool gif later! . def get_concat_h(im1, im2): dst = Image.new(&#39;RGB&#39;, (im1.width + im2.width, im1.height)) dst.paste(im1, (0, 0)) dst.paste(im2, (im1.width, 0)) return dst def make_project_progress_gifs(): all_result_folders = list(Path(&#39;results/&#39;).iterdir()) all_result_folders.sort() last_result_folder = all_result_folders[-1] for img_num in range(tot_aligned_imgs): all_step_pngs = [x for x in last_result_folder.iterdir() if x.name.endswith(&#39;png&#39;) and &#39;image{0:04d}&#39;.format(img_num) in x.name] all_step_pngs.sort() target_image = Image.open(all_step_pngs[-1]).resize((results_size, results_size)) all_concat_imgs = [] for step_img_path in all_step_pngs[:-1]: step_img = Image.open(step_img_path).resize((results_size, results_size)) all_concat_imgs.append(get_concat_h(target_image, step_img)) all_concat_imgs[0].save(&#39;output_gifs/image{0:04d}_project_progress.gif&#39;.format(img_num), save_all=True, append_images=all_concat_imgs[1:], duration=1000/fps, loop=0) . . make_project_progress_gifs() . . Tip: If you&#8217;re running this notebook yourself, the project progress gifs will be saved in output_gifs/image####_project_progress.gif :) . . . Let&#39;s look at the optimized latent codes we have acquired through this projection process! . def get_final_latents(): all_results = list(Path(&#39;results/&#39;).iterdir()) all_results.sort() last_result = all_results[-1] latent_files = [x for x in last_result.iterdir() if &#39;final_latent_code&#39; in x.name] latent_files.sort() all_final_latents = [] for file in latent_files: with open(file, mode=&#39;rb&#39;) as latent_pickle: all_final_latents.append(pickle.load(latent_pickle)) return all_final_latents . . latent_codes = get_final_latents() len(latent_codes), latent_codes[0].shape, latent_codes[1].shape . (2, (1, 18, 512), (1, 18, 512)) . . Note: We now have two latent codes for our Jeremy Howard and Obama images! Notice how these codes are of the shape (1, 18, 512), instead of the (1, 512) shape we saw earlier on the generated (fake) examples. This is due to one of the architecture designs of StyleGAN2, it actually re-iterates the base latent vector at different levels in the generator to allow for small deviations in the latent code to support variance in style. During training, just one static latent vector of the shape (1, 512) is used. For a more detailed explanation, check out the recommended technical StyleGAN2 overview blog posts mentioned in the introduction! :) . Let&#39;s check out what our images look like from our latent codes! . def load_model(): _G, _D, Gs = pretrained_networks.load_networks(model_path) noise_vars = [var for name, var in Gs.components.synthesis.vars.items() if name.startswith(&#39;noise&#39;)] Gs_kwargs = dnnlib.EasyDict() Gs_kwargs.output_transform = dict(func=tflib.convert_images_to_uint8, nchw_to_nhwc=True) Gs_kwargs.randomize_noise = False return Gs, noise_vars, Gs_kwargs def generate_image_from_projected_latents(latent_vector): images = Gs.components.synthesis.run(latent_vector, **Gs_kwargs) return images . . Gs, noise_vars, Gs_kwargs = load_model() . Setting up TensorFlow plugin &#34;fused_bias_act.cu&#34;: Preprocessing... Loading... Done. Setting up TensorFlow plugin &#34;upfirdn_2d.cu&#34;: Preprocessing... Loading... Done. . # Must re-define these variables because we restarted our kernel! output_gifs_path = Path(&#39;output_gifs/&#39;) aligned_imgs_path = Path(&#39;aligned_imgs/&#39;) aligned_img_set = list(aligned_imgs_path.iterdir()) aligned_img_set.sort() aligned_img_set = [Image.open(x) for x in aligned_img_set] . . images = generate_image_from_projected_latents(latent_codes[0]) recreated_img1 = Image.fromarray(images[0]).resize((results_size, results_size)) orig_img1 = aligned_img_set[1].resize((results_size, results_size)) get_concat_h(orig_img1, recreated_img1) . images = generate_image_from_projected_latents(latent_codes[1]) recreated_img2 = Image.fromarray(images[0]).resize((results_size, results_size)) orig_img2 = aligned_img_set[0].resize((results_size, results_size)) get_concat_h(orig_img2, recreated_img2) . . Note: Not bad at all! There are better ways to get more accurate latent codes that have better &quot;visual fidelity&quot;, such as optimizing all (1, 18, 512) latent codes, instead of just the base latent code (1, 512), as mentioned in this blog post by 5agado . Now let&#39;s re-run the interpolation animation we did back in Section 4, but this time with our own projected latent codes! . def linear_interpolate(code1, code2, alpha): return code1 * alpha + code2 * (1 - alpha) def make_latent_interp_animation_real_faces(code1, code2, img1, img2, num_interps): step_size = 1.0/num_interps all_imgs = [] amounts = np.arange(0, 1, step_size) for alpha in tqdm(amounts): interpolated_latent_code = linear_interpolate(code1, code2, alpha) images = generate_image_from_projected_latents(interpolated_latent_code) interp_latent_image = Image.fromarray(images[0]).resize((400, 400)) frame = get_concat_h(img2, interp_latent_image) frame = get_concat_h(frame, img1) all_imgs.append(frame) save_name = output_gifs_path/&#39;projected_latent_space_traversal.gif&#39; all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0) . . make_latent_interp_animation_real_faces(latent_codes[0], latent_codes[1], recreated_img1, recreated_img2, num_interps=200) . 100%|██████████| 200/200 [00:31&lt;00:00, 6.35it/s] . . Tip: If you&#8217;re running this notebook yourself, the interpolation gif will be saved in output_gifs/projected_latent_space_traversal.gif :) . . . Note: This gif represents going from Jeremy&#8217;s latent code to Obama&#8217;s latent code by slowly changing alpha from 0 to 1. (increasing alpha by 1/200 per iteration, until it reaches 1.0) . 6. Latent Directions/Controls to modify our projected images . Time to be an astronaut and explore space! Well, the hidden (latent) kind of space. Alright... I admit, that joke was blatant. Sorry for the puns, I&#39;m just trying to relate things together. Ok ok ok, you need some space, got it. . There are ways to learn latent directions (both supervised, and unsupervised) in the latent space to control features. People have already open-sourced some directional latent vectors for StyleGAN2 that allow us to &quot;move&quot; in the latent space and control a particular feature. . Supervised Method of learning these latent directions:&quot;We first collect multiple samples (image + latent) from our model and manually classify the images for our target attribute (e.g. smiling VS not smiling), trying to guarantee proper class representation balance. We then train a model to classify or regress on our latents and manual labels. At this point we can use the learned functions of these support models as transition directions&quot; - 5agado&#39;s Blog . | Unsupervised Method: Unsupervised Discovery of Interpretable Directions in the GAN Latent Space | . To move in a latent direction we can do the following operation: latent_code = latent_code + latent_direction * magnitude . latent_code is our latent code, such as our recently optimized latent code! | latent_direction is a learnt directional vector that is of shape (18, 512). This vector tells you where to move in the latent space to control a feature, but not how much to move. | magnitude is the amount to move in the direction of latent_direction | . This means we can create more interpolations in the latent space! Yay, more animations :). Only this time, rather than interpolating between 2 points, we are slowing moving in a specific latent direction. . Instead of mixing two latent codes together, we slowly add more magnitude to our base latent code, and observe how it changes with respect to magnitude! | . . Note: We&#8217;ll be using the learnt latent directions shared by Robert Luxemburg . def get_control_latent_vectors(path): files = [x for x in Path(path).iterdir() if str(x).endswith(&#39;.npy&#39;)] latent_vectors = {f.name[:-4]:np.load(f) for f in files} return latent_vectors . . latent_controls = get_control_latent_vectors(&#39;stylegan2directions/&#39;) len(latent_controls), latent_controls.keys(), latent_controls[&#39;age&#39;].shape . (16, dict_keys([&#39;age&#39;, &#39;eye_distance&#39;, &#39;eye_eyebrow_distance&#39;, &#39;eye_ratio&#39;, &#39;eyes_open&#39;, &#39;gender&#39;, &#39;lip_ratio&#39;, &#39;mouth_open&#39;, &#39;mouth_ratio&#39;, &#39;nose_mouth_distance&#39;, &#39;nose_ratio&#39;, &#39;nose_tip&#39;, &#39;pitch&#39;, &#39;roll&#39;, &#39;smile&#39;, &#39;yaw&#39;]), (18, 512)) . . Note: We have 16 latent &quot;controls&quot; we can play around with! . def make_latent_control_animation(feature, start_amount, end_amount, step_size, person): all_imgs = [] amounts = np.linspace(start_amount, end_amount, abs(end_amount-start_amount)/step_size) for amount_to_move in tqdm(amounts): modified_latent_code = np.array(latent_code_to_use) modified_latent_code += latent_controls[feature]*amount_to_move images = generate_image_from_projected_latents(modified_latent_code) latent_img = Image.fromarray(images[0]).resize((results_size, results_size)) all_imgs.append(get_concat_h(image_to_use, latent_img)) save_name = output_gifs_path/&#39;{0}_{1}.gif&#39;.format(person, feature) all_imgs[0].save(save_name, save_all=True, append_images=all_imgs[1:], duration=1000/fps, loop=0) . . latent_code_to_use = latent_codes[1] image_to_use = recreated_img2 make_latent_control_animation(feature=&#39;age&#39;, start_amount=-10, end_amount=10, step_size=0.1, person=&#39;jeremy&#39;) . 100%|██████████| 200/200 [00:31&lt;00:00, 6.39it/s] . latent_code_to_use = latent_codes[0] image_to_use = recreated_img1 make_latent_control_animation(feature=&#39;age&#39;, start_amount=-5, end_amount=5, step_size=0.1, person=&#39;obama&#39;) . 100%|██████████| 100/100 [00:15&lt;00:00, 6.38it/s] . . Tip: If you&#8217;re running this notebook yourself, the latent control interpolation gifs will be saved in output_gifs/person_feature.gif :) . . Important: Here are 5 traversals in the latent space using latent directions: . . Note: Aging Jeremy: . . . Note: Female Jeremy: . . . Note: Closing Mouth Jeremy: . . . Note: Aging Obama: . . . Note: Smiling Obama: . . 7. Bonus: Interactive Widget-App! . def apply_latent_controls(self): image_outputs = controller.children[0] feature_sliders = controller.children[1] slider_hboxes = feature_sliders.children[:-2] latent_movements = [(x.children[1].value, x.children[0].value) for x in slider_hboxes] modified_latent_code = np.array(latent_code_to_use) for feature, amount_to_move in latent_movements: modified_latent_code += latent_controls[feature]*amount_to_move images = generate_image_from_projected_latents(modified_latent_code) latent_img = Image.fromarray(images[0]).resize((400, 400)) latent_img_output = image_outputs.children[1] with latent_img_output: latent_img_output.clear_output() display(latent_img) def reset_latent_controls(self): image_outputs = controller.children[0] feature_sliders = controller.children[1] slider_hboxes = feature_sliders.children[:-2] for x in slider_hboxes: x.children[0].value = 0 latent_img_output = image_outputs.children[1] with latent_img_output: latent_img_output.clear_output() display(image_to_use) def create_interactive_latent_controller(): orig_img_output = widgets.Output() with orig_img_output: orig_img_output.clear_output() display(image_to_use) latent_img_output = widgets.Output() with latent_img_output: latent_img_output.clear_output() display(image_to_use) image_outputs = widgets.VBox([orig_img_output, latent_img_output]) #collapse-hide generate_button = widgets.Button(description=&#39;Generate&#39;, layout=widgets.Layout(width=&#39;75%&#39;, height=&#39;10%&#39;)) generate_button.on_click(apply_latent_controls) reset_button = widgets.Button(description=&#39;Reset Latent Controls&#39;, layout=widgets.Layout(width=&#39;75%&#39;, height=&#39;10%&#39;)) reset_button.on_click(reset_latent_controls) feature_sliders = [] for feature in latent_controls: label = widgets.Label(feature) slider = widgets.FloatSlider(min=-50, max=50) feature_sliders.append(widgets.HBox([slider, label])) feature_sliders.append(generate_button) feature_sliders.append(reset_button) feature_sliders = widgets.VBox(feature_sliders) return widgets.HBox([image_outputs, feature_sliders]) . . . Tip: Try running this widget-app yourself to see how well-fit some of the latent directions/controls are, and how loose others are! . . Important: You can also try plugging-in your own latent directions. (Maybe find another set of latent directions someone open-sourced. Who knows, they might work better than this set of latent directions!) You can even try to learn your own latent directions! . latent_code_to_use = latent_codes[0] image_to_use = recreated_img1 controller = create_interactive_latent_controller() controller . . latent_code_to_use = latent_codes[1] image_to_use = recreated_img2 controller = create_interactive_latent_controller() controller . . . Important: Below are some examples of the interactive widget-app :D . . Tip: This is super useful for finding good starting/end points for the feature-changing gifs shown in the previous Section 6. . . Note: Old Jeremy: . . . Note: Young Jeremy: . . . Note: Female Jeremy: . . . Note: Close Mouth Jeremy: . . . Note: Old Obama: . . . Note: Young Obama: . . . Note: Close Mouth Obama: . . . Note: Smiling Obama: . . Conclusion . Thanks for reading, and I hope you had as much fun playing/exploring the latent space as I did! .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/2020/08/10/Latent-Space-Exploration-with-StyleGAN2.html",
            "relUrl": "/jupyter/2020/08/10/Latent-Space-Exploration-with-StyleGAN2.html",
            "date": " • Aug 10, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Self-Supervision with FastAI",
            "content": "Introduction . This notebook is an introduction to self-supervised learning. In short, self-supervised learning has 2 components: . Pretrain on a pretext task, where the labels can come from the data itself! | Transfer the features, and train on the actual classification labels!&quot;What if we can get labels for free for unlabelled data and train unsupervised dataset in a supervised manner? We can achieve this by framing a supervised learning task in a special form to predict only a subset of information using the rest. In this way, all the information needed, both inputs and labels, has been provided. This is known as self-supervised learning.&quot; - Lilian Weng . | Using FastAI2, we&#39;ll use rotation as a pretext task for learning representations/features of our data. . Here are some great overviews of self-supervised learning that I&#39;ve come across: . Lilian Weng - Self-Supervised Representation Learning . | Jeremy Howard - Self-supervised learning and computer vision . | . Experiment Layout . In this notebook, we will be using the MNIST dataset. . Also check out ImageWang from FastAI themselves! It&#39;s a dataset designed for self-supervision tasks! . Train a model on a rotation prediction task. . We will use all the training data for rotation prediction. | Input: A rotated image. | Target/Label: Classify the amount of degrees rotated. | Our model should learn useful features that can transfer well for a classification task. | (The model should learn what digits look like in order to be able to successfully predict the amount of rotation). | . | Transfer our rotation pretraining features to solve the classification task with much fewer labels, &lt; 1% of the original data. . Input: A normal image. | Target/Label: The images&#39; original categorical label. | Classification accuracy should be decent, even with only using &lt; 1% of the original data. | . | Train a classifier from scratch on the same amount of data used in experiment 2. . Input: A normal image. | Target/Label: The images&#39; original categorical label. | Classification accuracy should be low (lack of transfer learning &amp; too few labeled data!) | Model may overfit. | . | FastAI Vision Model Creation Methods . . Warning: This Jupyter notebook runs with fastai2! Make sure you have it installed, use the cell below to install it :) . !pip install fastai --upgrade # Uncomment and run the below line to get a fresh install of fastai, if needed # !pip install fastai --upgrade . . Important: Pay attention! It&#8217;s important. We will be using a small ConvNet to test our self-supervised learning method. The architecture is defined below in simple_arch. Note that simple_arch takes in one argument, pretrained. This is to allow FastAI to pass pretrained=True or pretrained=False when creating the model body! Below are some use cases of when we would want pretrained=True or pretrained=False. . pretrained=False = For training a new model on our rotation prediction task. | pretrained=True = For transferring the learnt features from our rotation task pretraining to solve a classification task. | pretrained=False = For training a new model from scratch on the main classification task (no transfer learning). | from fastai.vision.all import * . def simple_arch(pretrained=False): # Note that FastAI will automatically cut at pooling layer for the body! model = nn.Sequential( nn.Conv2d(1, 4, 3, 1), nn.BatchNorm2d(4), nn.ReLU(), nn.Conv2d(4, 16, 3, 1), nn.BatchNorm2d(16), nn.ReLU(), nn.Conv2d(16, 32, 3, 1), nn.BatchNorm2d(32), nn.AdaptiveAvgPool2d(1), ) if (pretrained): print(&quot;Loading pretrained model...&quot;) pretrained_weights = torch.load(save_path/&#39;rot_pretrained.pt&#39;) print(model.load_state_dict(pretrained_weights)) return model . . The follow below code snippets are examples of how FastAI creates CNNs. Every model will have a body and a head . body = create_body(arch=simple_arch, pretrained=False) body . . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . head = create_head(nf=32, n_out=8, lin_ftrs=[]) head . . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) . # Note that FastAI automatically determines nf for the head! model = create_cnn_model(arch=simple_arch, pretrained=False, n_out=8, lin_ftrs=[]) model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) ) . PyTorch Rotation/Classification Self-Supervised Dataset . import torchvision tensorToImage = torchvision.transforms.ToPILImage() imageToTensor = torchvision.transforms.ToTensor() . # Uncomment and run the below lines if torchvision has trouble downloading MNIST (in the next cell) # !wget -P data/MNIST/raw/ http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz # !wget -P data/MNIST/raw/ http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz # !wget -P data/MNIST/raw/ http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz # !wget -P data/MNIST/raw/ http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz . . torchvision.datasets.MNIST(&#39;data/&#39;, download=True) . Dataset MNIST Number of datapoints: 60000 Root location: data/ Split: Train . Below we define a dataset, here&#39;s the docstring: . A Dataset for Rotation-based Self-Supervision! Images are rotated clockwise. . file - MNIST processed .pt file. | pct - percent of data to use | classification - False=Use rotation labels. True=Use original classification labels. | . class Custom_Dataset_MNIST(): &#39;&#39;&#39; A Dataset for Rotation-based Self-Supervision! Images are rotated clockwise. - file - MNIST processed .pt file. - pct - percent of data to use - classification - False=Use rotation labels. True=Use original classification labels. &#39;&#39;&#39; def __init__(self, file, pct, classification): data = torch.load(file) self.imgs = data[0] self.labels = data[1] self.pct = pct self.classification = classification slice_idx = int(len(self.imgs)*self.pct) self.imgs = self.imgs[:slice_idx] def __len__(self): return len(self.imgs) def __getitem__(self, idx): img = self.imgs[idx].unsqueeze(0) img = tensorToImage(img) img = img.resize((32, 32), resample=1) img = imageToTensor(img) if (not self.classification): # 4 classes for rotation degrees = [0, 45, 90, 135, 180, 225, 270, 315] rand_choice = random.randint(0, len(degrees)-1) img = tensorToImage(img) img = img.rotate(degrees[rand_choice]) img = imageToTensor(img) return img, torch.tensor(rand_choice).long() return img, self.labels[idx] def show_batch(self, n=3): fig, axs = plt.subplots(n, n) fig.tight_layout() for i in range(n): for j in range(n): rand_idx = random.randint(0, len(self)-1) img, label = self.__getitem__(rand_idx) axs[i, j].imshow(tensorToImage(img), cmap=&#39;gray&#39;) if self.classification: axs[i, j].set_title(&#39;Label: {0} (Digit #{1})&#39;.format(label.item(), label.item())) else: axs[i, j].set_title(&#39;Label: {0} ({1} Degrees)&#39;.format(label.item(), label.item()*45)) axs[i, j].axis(&#39;off&#39;) . . Rotation Prediction Data . . Important: 60k training data and 10k validation data! . train_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/training.pt&#39;, pct=1.0, classification=False) valid_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/test.pt&#39;, pct=1.0, classification=False) print(&#39;{0} Training Samples | {1} Validation Samples&#39;.format(len(train_ds), len(valid_ds))) . 60000 Training Samples | 10000 Validation Samples . . Note: Notice that our labels don&#8217;t correspond to digits! They correspond to the amount of degrees rotated! Specifically from this predefined set: [0, 45, 90, 135, 180, 225, 270, 315] . from fastai.data.core import DataLoaders dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() # Override the show_batch function of dls to the one used in our dataset! dls.show_batch = train_ds.show_batch # We have 8 classes! [0, 1, 2, 3, 4, 5, 6, 7] that correspond to the [0, 45, 90, 135, 180, 225, 270, 315] degrees of rotation. dls.c = 8 dls.show_batch() . FastAI Vision Learner [Rotation] . rotation_head = create_head(nf=32, n_out=8, lin_ftrs=[]) rotation_head . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) . . Note: We want to measure top_2_accuracy along with regular (top_1) accuracy, because there are hard-cases where it&#8217;s understandable why our model got it wrong. For example: &#8217;0&#8217; rotated 90 or 270 degrees, or &#8217;1&#8217; rotated 0 or 180 degrees. (They can look the same!) . # - A zero rotated 90 or 270 degrees? # - A one rotated 0 or 180 degrees? # etc :P top_2_accuracy = lambda inp, targ: top_k_accuracy(inp, targ, k=2) top_2_accuracy . &lt;function __main__.&lt;lambda&gt;(inp, targ)&gt; . Here, we train a model on the rotation prediction task! . # Note to set a value for lin_ftrs, we use the defined config above. learner = cnn_learner(dls, simple_arch, pretrained=False, loss_func=CrossEntropyLossFlat(), custom_head=rotation_head, metrics=[accuracy, top_2_accuracy]) learner.model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=8, bias=False) ) ) . learner.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 30 x 30 Conv2d 40 True BatchNorm2d 8 True ReLU ____________________________________________________________________________ 64 x 16 x 28 x 28 Conv2d 592 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 64 x 32 x 26 x 26 Conv2d 4640 True BatchNorm2d 64 True ____________________________________________________________________________ [] AdaptiveAvgPool2d AdaptiveMaxPool2d Flatten BatchNorm1d 128 True Dropout ____________________________________________________________________________ 64 x 8 Linear 512 True ____________________________________________________________________________ Total params: 6,016 Total trainable params: 6,016 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fdc9da95840&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.013182567432522774) . learner.fit_one_cycle(5, lr_max=3e-2) . epoch train_loss valid_loss accuracy &lt;/th&gt; time &lt;/tr&gt; &lt;/thead&gt; 0 | 0.932141 | 1.084204 | 0.502600 | 0.875300 | 00:12 | . 1 | 0.861757 | 0.831546 | 0.631500 | 0.905800 | 00:12 | . 2 | 0.757740 | 0.841575 | 0.612800 | 0.932800 | 00:12 | . 3 | 0.678802 | 0.655642 | 0.706700 | 0.949000 | 00:11 | . 4 | 0.629287 | 0.550476 | 0.762000 | 0.968400 | 00:12 | . &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt; . Important: We were able to achieve 76.2% top-1 accuracy, and 96.8% top-2 accuracy after just 5 epochs! Now we want to grab our model from our Learner, and save the body of it! . . Note: Our model has two components, the body and the head. model is a list of size 2, where model[0] is the body, and model[1] is the head! . trained_body = learner.model[0] trained_body . Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) . . Tip: To save a model in PyTorch, save it&#8217;s state_dict function! You can use model.load_state_dict to re-load the weights. . save_path = Path(&#39;rotation_cps/&#39;) if not save_path.exists(): save_path.mkdir() # Save the rotation-pretraining weights of our model body torch.save(trained_body.state_dict(), save_path/&#39;rot_pretrained.pt&#39;) . Original Classification Data . Now that we have pretrained our model on the rotation prediction task, we want to switch over to the original labeled data for the classification task. . . Important: We&#8217;re only using 180 samples for training! . # Use 100% classification labeled data for validation! train_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/training.pt&#39;, pct=0.003, classification=True) valid_ds = Custom_Dataset_MNIST(&#39;data/MNIST/processed/test.pt&#39;, pct=1.0, classification=True) print(&#39;{0} Training Samples | {1} Validation Samples&#39;.format(len(train_ds), len(valid_ds))) . 180 Training Samples | 10000 Validation Samples . . Note: Notice the labels now correspond to the digit class! . from fastai.data.core import DataLoaders dls = DataLoaders.from_dsets(train_ds, valid_ds).cuda() dls.show_batch = train_ds.show_batch # We have 10 classes! One for each digit label! dls.c = 10 dls.show_batch() . FastAI Vision Learner [Transfer-Classification] . Here we will toggle pretrained=True to transfer our rotation prediction features, and train on the original 180 labeled data. . classification_head = create_head(nf=32, n_out=10, lin_ftrs=[]) classification_head . . Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) . . Note: We have n_out=10 because of the 10 different digit classes . # pretrained=True will load the saved rotation pretraining weights into our model&#39;s body! # See simple_arch() function definition for more details! learner = cnn_learner(dls, simple_arch, pretrained=True, loss_func=CrossEntropyLossFlat(), custom_head=classification_head, metrics=[accuracy]) learner.model . . Loading pretrained model... &lt;All keys matched successfully&gt; . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) ) . . Tip: Freezing a model&#8217;s body after transferring the weights over, allows the new head to get calibrated with the rest of the model! . learner.freeze() . . Note: Looking at the model summary, we can see that the model is frozen up to the new head! Good! . learner.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 30 x 30 Conv2d 40 False BatchNorm2d 8 True ReLU ____________________________________________________________________________ 64 x 16 x 28 x 28 Conv2d 592 False BatchNorm2d 32 True ReLU ____________________________________________________________________________ 64 x 32 x 26 x 26 Conv2d 4640 False BatchNorm2d 64 True ____________________________________________________________________________ [] AdaptiveAvgPool2d AdaptiveMaxPool2d Flatten BatchNorm1d 128 True Dropout ____________________________________________________________________________ 64 x 10 Linear 640 True ____________________________________________________________________________ Total params: 6,144 Total trainable params: 872 Total non-trainable params: 5,272 Optimizer used: &lt;function Adam at 0x7fdc9da95840&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model frozen up to parameter group #1 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . SuggestedLRs(lr_min=0.07585775852203369, lr_steep=0.14454397559165955) . learner.fit_one_cycle(10, lr_max=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.752726 | 4.032210 | 0.113500 | 00:01 | . 1 | 3.530574 | 3.048504 | 0.131500 | 00:01 | . 2 | 3.269396 | 2.611858 | 0.103700 | 00:01 | . 3 | 2.922121 | 2.407352 | 0.156700 | 00:01 | . 4 | 2.598596 | 2.234617 | 0.183000 | 00:01 | . 5 | 2.351149 | 2.088923 | 0.180100 | 00:01 | . 6 | 2.157926 | 1.959662 | 0.199200 | 00:01 | . 7 | 1.994265 | 1.835968 | 0.250600 | 00:01 | . 8 | 1.874377 | 1.722400 | 0.319700 | 00:01 | . 9 | 1.754826 | 1.629582 | 0.388200 | 00:01 | . . Tip: Unfreeze the model after calibrating the new head with the transferred body, and train a little more! . learner.unfreeze() . learner.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 30 x 30 Conv2d 40 True BatchNorm2d 8 True ReLU ____________________________________________________________________________ 64 x 16 x 28 x 28 Conv2d 592 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 64 x 32 x 26 x 26 Conv2d 4640 True BatchNorm2d 64 True ____________________________________________________________________________ [] AdaptiveAvgPool2d AdaptiveMaxPool2d Flatten BatchNorm1d 128 True Dropout ____________________________________________________________________________ 64 x 10 Linear 640 True ____________________________________________________________________________ Total params: 6,144 Total trainable params: 6,144 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fdc9da95840&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Model unfrozen Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . SuggestedLRs(lr_min=0.025118863582611083, lr_steep=6.309573450380412e-07) . learner.fine_tune(5, base_lr=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 0.926475 | 1.532705 | 0.473400 | 00:01 | . epoch train_loss valid_loss accuracy time . 0 | 1.057154 | 1.468854 | 0.514800 | 00:01 | . 1 | 1.079370 | 1.409364 | 0.542100 | 00:01 | . 2 | 1.009147 | 1.357950 | 0.562100 | 00:01 | . 3 | 0.976450 | 1.306930 | 0.586400 | 00:01 | . 4 | 0.935184 | 1.269005 | 0.606100 | 00:01 | . . Important: We were able to get 60.6% accuracy using transfer learning from our pretraining on the rotation prediction task! . FastAI Vision Learner [From Sratch-Classification] . Here we train a model from scratch on the original 180 labeled data. . # pretrained=False, Create the same model as before, but without using the rotation pretraining weights! learner = cnn_learner(dls, simple_arch, pretrained=False, loss_func=CrossEntropyLossFlat(), custom_head=classification_head, metrics=[accuracy]) learner.model . . Sequential( (0): Sequential( (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1)) (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU() (3): Conv2d(4, 16, kernel_size=(3, 3), stride=(1, 1)) (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (5): ReLU() (6): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)) (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.5, inplace=False) (4): Linear(in_features=64, out_features=10, bias=False) ) ) . learner.summary() . Sequential (Input shape: 64) ============================================================================ Layer (type) Output Shape Param # Trainable ============================================================================ 64 x 4 x 30 x 30 Conv2d 40 True BatchNorm2d 8 True ReLU ____________________________________________________________________________ 64 x 16 x 28 x 28 Conv2d 592 True BatchNorm2d 32 True ReLU ____________________________________________________________________________ 64 x 32 x 26 x 26 Conv2d 4640 True BatchNorm2d 64 True ____________________________________________________________________________ [] AdaptiveAvgPool2d AdaptiveMaxPool2d Flatten BatchNorm1d 128 True Dropout ____________________________________________________________________________ 64 x 10 Linear 640 True ____________________________________________________________________________ Total params: 6,144 Total trainable params: 6,144 Total non-trainable params: 0 Optimizer used: &lt;function Adam at 0x7fdc9da95840&gt; Loss function: FlattenedLoss of CrossEntropyLoss() Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . learner.lr_find() . SuggestedLRs(lr_min=0.04365158379077912, lr_steep=0.010964781977236271) . learner.fit_one_cycle(10, lr_max=3e-2) . epoch train_loss valid_loss accuracy time . 0 | 3.689067 | 6.550608 | 0.101000 | 00:01 | . 1 | 3.290434 | 5.411524 | 0.101100 | 00:01 | . 2 | 2.885511 | 4.980519 | 0.074900 | 00:01 | . 3 | 2.584637 | 9.233224 | 0.102800 | 00:01 | . 4 | 2.328669 | 12.973579 | 0.102800 | 00:01 | . 5 | 2.115707 | 14.907648 | 0.102800 | 00:01 | . 6 | 1.952985 | 15.155903 | 0.102800 | 00:01 | . 7 | 1.831354 | 14.910644 | 0.102800 | 00:01 | . 8 | 1.734552 | 14.626731 | 0.103000 | 00:01 | . 9 | 1.643174 | 14.119630 | 0.103400 | 00:01 | . . Important: We were able to only get 10.3% accuracy with training from scratch . Conclusion . . Important: Using self-supervision can help learn features that can transfer to a down-stream task, such as classification! In this example, we used rotation predication as our pretext task for feature representation learning. Pretraining our model on rotation prediction prior to training for classification allowed us to achieve 60.6% accuracy, on just 0.3% of the labeled data (180 samples)! Training from scratch with the same amount of data yields an accuracy of 10.3%. The motivation for using self-supervised learning is the ability to train models with decent accuracy without the need of much labeled data! . . Note: Be sure to try other self-supervised learning methods (or perhaps find your own!) and compete on the ImageWang Leadboard! How will model size, data difficultly, and dataset size (number of samples) affect self-supervised learning? . &lt;/div&gt; .",
            "url": "https://amarsaini.github.io/Epoching-Blog/jupyter/2020/03/23/Self-Supervision-with-FastAI.html",
            "relUrl": "/jupyter/2020/03/23/Self-Supervision-with-FastAI.html",
            "date": " • Mar 23, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://amarsaini.github.io/Epoching-Blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://amarsaini.github.io/Epoching-Blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}
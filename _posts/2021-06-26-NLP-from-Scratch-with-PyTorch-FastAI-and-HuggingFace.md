---
keywords: fastai
description: A technical NLP tutorial using a variety of libraries to show the different levels/layers of common NLP pipelines
title: NLP from Scratch with PyTorch, fastai, and HuggingFace
toc: true 
badges: true
comments: true
categories: [jupyter, nlp, pytorch, fastai, huggingface]
image: images/pfh.png
nb_path: _notebooks/06-27-2021-NLP-from-Scratch-with-PyTorch-FastAI-and-HuggingFace.ipynb
layout: notebook
---

<!--
#################################################
### THIS FILE WAS AUTOGENERATED! DO NOT EDIT! ###
#################################################
# file to edit: _notebooks/06-27-2021-NLP-from-Scratch-with-PyTorch-FastAI-and-HuggingFace.ipynb
-->

<div class="container" id="notebook-container">
        
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="0.-Introduction">0. Introduction<a class="anchor-link" href="#0.-Introduction"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Welcome! In this blog post/notebook, we'll be looking at NLP with 3 different methods:</p>
<ul>
<li>From Scratch/Ground-Up, with <a href="https://pytorch.org/">PyTorch</a></li>
<li>FastAI Language Model (<a href="https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM">AWD-LSTM</a>)</li>
<li>HuggingFace Transformers (<a href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a>)</li>
</ul>
<p>All 3 methods will utilize fastai to assist with keeping things organized and help with training the models, given the libary's ease of use through it's lovely <a href="https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/">Layered-API</a>!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="1.-Looking-at-the-Data-[Pandas]">1. Looking at the Data [Pandas]<a class="anchor-link" href="#1.-Looking-at-the-Data-[Pandas]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>For this notebook, we'll be looking at the Amazon Reviews Polarity dataset! The task is to predict whether a review is of positive or negative sentiment</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">fastai.text.all</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span> <span class="o">=</span> <span class="n">untar_data</span><span class="p">(</span><span class="n">URLs</span><span class="o">.</span><span class="n">AMAZON_REVIEWS_POLARITY</span><span class="p">)</span>
<span class="n">path</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv&#39;)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">path</span><span class="o">.</span><span class="n">ls</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#3) [Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv/train.csv&#39;),Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv/readme.txt&#39;),Path(&#39;/home/saini5/.fastai/data/amazon_review_polarity_csv/test.csv&#39;)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's go ahead and take a look at our two df's: <code>train_df</code> and <code>valid_df</code></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='We&#8217;re going to use 40k instead of 3.6m samples for training, and 2k instead of 400k samples for validation' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;train.csv&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">40000</span><span class="p">)</span>
<span class="n">valid_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">path</span><span class="o">/</span><span class="s1">&#39;test.csv&#39;</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="s1">&#39;title&#39;</span><span class="p">,</span> <span class="s1">&#39;text&#39;</span><span class="p">],</span> <span class="n">nrows</span><span class="o">=</span><span class="mi">2000</span><span class="p">)</span>
<span class="n">train_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea output_execute_result">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>label</th>
      <th>title</th>
      <th>text</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>2</td>
      <td>Stuning even for the non-gamer</td>
      <td>This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>The best soundtrack ever to anything.</td>
      <td>I'm reading a lot of reviews saying that this is the best 'game soundtrack' and I figured that I'd write a review to disagree a bit. This in my opinino is Yasunori Mitsuda's ultimate masterpiece. The music is timeless and I'm been listening to it for years now and its beauty simply refuses to fade.The price tag on this is pretty staggering I must say, but if you are going to buy any cd for this much money, this is the only one that I feel would be worth every penny.</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>Amazing!</td>
      <td>This soundtrack is my favorite music of all time, hands down. The intense sadness of "Prisoners of Fate" (which means all the more if you've played the game) and the hope in "A Distant Promise" and "Girl who Stole the Star" have been an important inspiration to me personally throughout my teen years. The higher energy tracks like "Chrono Cross ~ Time's Scar~", "Time of the Dreamwatch", and "Chronomantique" (indefinably remeniscent of Chrono Trigger) are all absolutely superb as well.This soundtrack is amazing music, probably the best of this composer's work (I haven't heard the Xenogears s...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>2</td>
      <td>Excellent Soundtrack</td>
      <td>I truly like this soundtrack and I enjoy video game music. I have played this game and most of the music on here I enjoy and it's truly relaxing and peaceful.On disk one. my favorites are Scars Of Time, Between Life and Death, Forest Of Illusion, Fortress of Ancient Dragons, Lost Fragment, and Drowned Valley.Disk Two: The Draggons, Galdorb - Home, Chronomantique, Prisoners of Fate, Gale, and my girlfriend likes ZelbessDisk Three: The best of the three. Garden Of God, Chronopolis, Fates, Jellyfish sea, Burning Orphange, Dragon's Prayer, Tower Of Stars, Dragon God, and Radical Dreamers - Uns...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>2</td>
      <td>Remember, Pull Your Jaw Off The Floor After Hearing it</td>
      <td>If you've played the game, you know how divine the music is! Every single song tells a story of the game, it's that good! The greatest songs are without a doubt, Chrono Cross: Time's Scar, Magical Dreamers: The Wind, The Stars, and the Sea and Radical Dreamers: Unstolen Jewel. (Translation varies) This music is perfect if you ask me, the best it can be. Yasunori Mitsuda just poured his heart on and wrote it down on paper.</td>
    </tr>
  </tbody>
</table>
</div>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='label 1 is negative sentiment and label 2 is positive sentiment' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">train_df</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(40000, 2000)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="2.-Tokenization-and-Numericalization-[PyTorch]">2. Tokenization and Numericalization [PyTorch]<a class="anchor-link" href="#2.-Tokenization-and-Numericalization-[PyTorch]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We now want to first tokenize our inputs, then numericalize them using a vocab. Quick recap of these terms:</p>
<ul>
<li><em>Tokenization</em> = The process of converting an input string into "pieces"<ul>
<li>These pieces can be <strong>whole words</strong>, <strong>sub words</strong>, or even <strong>characters</strong></li>
</ul>
</li>
<li><em>Numericalization</em> = The process of converting a token into a numeric representation<ul>
<li>(e.g. token -&gt; number)</li>
<li>This is done through the use (and creation of) a <strong>vocab</strong></li>
</ul>
</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>There are many fancy tokenizers out there, but since we're first doing things from scratch we'll go ahead and use a simple <code>basic_english</code> tokenizer from <code>torchtext</code> and split on spaces</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include tip.html content='More popular tokenizer pipelines can be found in libraries such as <a href="https://spacy.io/api/tokenizer">SpaCy</a>, <a href="https://huggingface.co/transformers/main_classes/tokenizer.html">HuggingFace</a>, and <a href="https://docs.fast.ai/text.core.html#Tokenizing">fastai</a>!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_text</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sample_text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torchtext</span>
<span class="kn">from</span> <span class="nn">torchtext.data</span> <span class="kn">import</span> <span class="n">get_tokenizer</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span><span class="s2">&quot;basic_english&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include tip.html content='Reminder that fastai&#8217;s <code>L</code> is basically <code>list</code> from Python, but has some convienent properties such as displaying the number of elements, and  additionally doesn&#8217;t spam your screen with output if the list is too long!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sample_text</span><span class="p">))</span>
<span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#81) [&#39;this&#39;,&#39;sound&#39;,&#39;track&#39;,&#39;was&#39;,&#39;beautiful&#39;,&#39;!&#39;,&#39;it&#39;,&#39;paints&#39;,&#39;the&#39;,&#39;senery&#39;...]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='Since we&#8217;re using a full word tokenizer, many of these words will be quite infrequent, such as typos or words with repetiting characters like <code>Dudeeeee</code>. Other tokenizers can use rules to better handle splitting of big words through subword tokenization, and can also handle numbers like prices as well. This would help with optimizing the <strong>vocab&#8217;s embedding table</strong> as well as reducing the number of <code>&lt;unk&gt;</code> tokens.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Next we'll need to check how many tokens there are in our dataset, and keep the frequent ones as part of our vocab.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">token_counter</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">()</span>

<span class="k">for</span> <span class="n">sample_text</span> <span class="ow">in</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">]:</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sample_text</span><span class="p">)</span>
    <span class="n">token_counter</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>

<span class="n">token_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;.&#39;, 213962),
 (&#39;the&#39;, 158787),
 (&#39;,&#39;, 116525),
 (&#39;i&#39;, 91270),
 (&#39;and&#39;, 86059),
 (&#39;a&#39;, 77977),
 (&#39;to&#39;, 74984),
 (&#39;it&#39;, 69999),
 (&#39;of&#39;, 65144),
 (&#34;&#39;&#34;, 60523),
 (&#39;this&#39;, 59382),
 (&#39;is&#39;, 56445),
 (&#39;in&#39;, 37890),
 (&#39;that&#39;, 33891),
 (&#39;for&#39;, 30532),
 (&#39;was&#39;, 29163),
 (&#39;you&#39;, 26740),
 (&#39;!&#39;, 25238),
 (&#39;book&#39;, 24698),
 (&#39;s&#39;, 23897),
 (&#39;but&#39;, 22602),
 (&#39;with&#39;, 21998),
 (&#39;not&#39;, 21988),
 (&#39;on&#39;, 20759),
 (&#39;t&#39;, 20097)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='The top 25 most common tokens are listed above! Let&#8217;s see what the least frequent tokens look like:' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">token_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">()[</span><span class="o">-</span><span class="mi">25</span><span class="p">:]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>[(&#39;knorflefob&#39;, 1),
 (&#39;&lt;&lt;dan&#39;, 1),
 (&#39;bg&#39;, 1),
 (&#39;apquire$&#39;, 1),
 (&#39;gtube&#39;, 1),
 (&#39;chafe&#39;, 1),
 (&#39;lubricates&#39;, 1),
 (&#39;lubricate&#39;, 1),
 (&#39;flights/vacation&#39;, 1),
 (&#39;lambdin&#39;, 1),
 (&#39;trafalgar&#39;, 1),
 (&#39;bloodedly&#39;, 1),
 (&#39;undifferentiated&#39;, 1),
 (&#39;code--no&#39;, 1),
 (&#39;exciting--because&#39;, 1),
 (&#39;nicaea&#39;, 1),
 (&#39;full-grown&#39;, 1),
 (&#39;yon&#39;, 1),
 (&#39;medium-to-large&#39;, 1),
 (&#39;ms-supplied&#39;, 1),
 (&#39;well@@&#39;, 1),
 (&#39;in-touch&#39;, 1),
 (&#39;*ms&#39;, 1),
 (&#39;specialist*&#39;, 1),
 (&#39;schapiro&#39;, 1)]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">token_counter</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>75889</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">token_counter</span><span class="p">[</span><span class="s1">&#39;well@@&#39;</span><span class="p">],</span> <span class="n">token_counter</span><span class="p">[</span><span class="s1">&#39;well&#39;</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(1, 5418)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='There were <strong>78,157</strong> unique tokens! It&#8217;s interesting seeing that some of them are typos, or just have additional characters attached to the word, such as <code>well@@</code> instead of <code>well</code>' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have our token frequency counter, we can go ahead and make our vocab!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Sort our counter so our vocab will have the most common tokens listed first</span>
<span class="n">sorted_counter</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">token_counter</span><span class="o">.</span><span class="n">most_common</span><span class="p">())</span>

<span class="c1"># Create vocab containing tokens with a minimum frequency of 20</span>
<span class="n">my_vocab</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">vocab</span><span class="p">(</span><span class="n">sorted_counter</span><span class="p">,</span> <span class="n">min_freq</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>

<span class="c1"># Add the unknown token, and use this by default for unknown words</span>
<span class="n">unk_token</span> <span class="o">=</span> <span class="s1">&#39;&lt;unk&gt;&#39;</span>
<span class="n">my_vocab</span><span class="o">.</span><span class="n">insert_token</span><span class="p">(</span><span class="n">unk_token</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">my_vocab</span><span class="o">.</span><span class="n">set_default_index</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Add the pad token</span>
<span class="n">pad_token</span> <span class="o">=</span> <span class="s1">&#39;&lt;pad&gt;&#39;</span>
<span class="n">my_vocab</span><span class="o">.</span><span class="n">insert_token</span><span class="p">(</span><span class="n">pad_token</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Show vocab size, and examples of tokens</span>
<span class="nb">len</span><span class="p">(</span><span class="n">my_vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()),</span> <span class="n">my_vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[:</span><span class="mi">25</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(7591,
 [&#39;&lt;unk&gt;&#39;,
  &#39;&lt;pad&gt;&#39;,
  &#39;.&#39;,
  &#39;the&#39;,
  &#39;,&#39;,
  &#39;i&#39;,
  &#39;and&#39;,
  &#39;a&#39;,
  &#39;to&#39;,
  &#39;it&#39;,
  &#39;of&#39;,
  &#34;&#39;&#34;,
  &#39;this&#39;,
  &#39;is&#39;,
  &#39;in&#39;,
  &#39;that&#39;,
  &#39;for&#39;,
  &#39;was&#39;,
  &#39;you&#39;,
  &#39;!&#39;,
  &#39;book&#39;,
  &#39;s&#39;,
  &#39;but&#39;,
  &#39;with&#39;,
  &#39;not&#39;])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='We&#8217;ll be using <code>&lt;unk&gt;</code> as our default token for tokens that are out of our vocab!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='Notice how we passed in a <code>min_freq</code> argument. This ensures that the vocab only includes high frequency tokens. We wouldn&#8217;t want to include tokens that only occur once/rarely. This brought our vocab count down from <strong>75,889</strong> to <strong>7,591</strong>! A <strong>~90% reduction</strong>!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Rather than starting from scratch, we can preload GloVe embeddings into our vocabulary!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Pretrained word vectors/embeddings</span>
<span class="n">glove</span> <span class="o">=</span> <span class="n">torchtext</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">GloVe</span><span class="p">(</span><span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;6B&#39;</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">glove</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([400000, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Since we're using GloVe vectors for <em>transfer learning</em> (by preloading our embedding), let's take a look at how many tokens can be successfully transferred from GloVe into our own vocab. Each token will have an embedding (vector) of size 100. This results in an embedding of size <strong>7591x100</strong></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Preloading our vocab with embeddings if our vocab shares similar tokens to GloVe</span>
<span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span> <span class="o">=</span> <span class="n">glove</span><span class="o">.</span><span class="n">get_vecs_by_tokens</span><span class="p">(</span><span class="n">my_vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">())</span>
<span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([7591, 100])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>By default, tokens that aren't able to transfer from GloVe into our own dataset get initialized with a vector of 0's. We can use this to count how many tokens were successfully preloaded!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tot_transferred</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">:</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">v</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
        <span class="n">tot_transferred</span> <span class="o">+=</span> <span class="mi">1</span>
        
<span class="n">tot_transferred</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">my_vocab</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(7517, 7591)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='Not bad, 7517 out of our 7591 tokens were successfully transferred!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[</span><span class="mi">3</span><span class="p">],</span> <span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;the&#39;,
 tensor([-0.0382, -0.2449,  0.7281, -0.3996,  0.0832,  0.0440, -0.3914,  0.3344,
         -0.5755,  0.0875,  0.2879, -0.0673,  0.3091, -0.2638, -0.1323, -0.2076,
          0.3340, -0.3385, -0.3174, -0.4834,  0.1464, -0.3730,  0.3458,  0.0520,
          0.4495, -0.4697,  0.0263, -0.5415, -0.1552, -0.1411, -0.0397,  0.2828,
          0.1439,  0.2346, -0.3102,  0.0862,  0.2040,  0.5262,  0.1716, -0.0824,
         -0.7179, -0.4153,  0.2033, -0.1276,  0.4137,  0.5519,  0.5791, -0.3348,
         -0.3656, -0.5486, -0.0629,  0.2658,  0.3020,  0.9977, -0.8048, -3.0243,
          0.0125, -0.3694,  2.2167,  0.7220, -0.2498,  0.9214,  0.0345,  0.4674,
          1.1079, -0.1936, -0.0746,  0.2335, -0.0521, -0.2204,  0.0572, -0.1581,
         -0.3080, -0.4162,  0.3797,  0.1501, -0.5321, -0.2055, -1.2526,  0.0716,
          0.7056,  0.4974, -0.4206,  0.2615, -1.5380, -0.3022, -0.0734, -0.2831,
          0.3710, -0.2522,  0.0162, -0.0171, -0.3898,  0.8742, -0.7257, -0.5106,
         -0.5203, -0.1459,  0.8278,  0.2706]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[</span><span class="mi">6555</span><span class="p">],</span> <span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="mi">6555</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(&#39;eargels&#39;,
 tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,
         0., 0., 0., 0.]))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='Note that the vector for index 3 corresponds to the word/token <code>the</code> and it has non-zero values in the vector since we were able to preload it from GloVe! Index 6559 corresponds to the word <code>eargels</code> and it&#8217;s all zeros, which means that it wasn&#8217;t part of Glove&#8217;s vocab and therefore couldn&#8217;t transfer it&#8217;s embedding to our vocab!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='There were 400k tokens/words in GloVe, and <code>stardust</code> wasn&#8217;t one of them :O' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include warning.html content='Initializing the embeddings to 0 isn&#8217;t the best idea. If we use these embeddings for transfer learning to our embedding layer, then <strong>all</strong> new tokens not found in GloVe will have an indentical vector representation of all 0s. I found the model to be harder to train because of this, and I concluded that it&#8217;s due to these 0s being multiplied in our model&#8217;s forward pass which creates more 0s, and possibly cause some gradients to be 0, making it much more difficult to learn these new embeddings.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include tip.html content='Rather than initializing new token embeddings to 0&#8217;s, let&#8217;s use <code>torch.randn</code> to create some diversity between the different token embeddings that weren&#8217;t preloaded with GloVe!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
    <span class="k">if</span> <span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="mi">100</span><span class="p">)):</span>
        <span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's use our vocab to <em>numericalize</em> our tokens!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_text</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sample_text</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;This sound track was beautiful! It paints the senery in your mind so well I would recomend it even to people who hate vid. game music! I have played the game Chrono Cross but out of all of the games I have ever played it has the best music! It backs away from crude keyboarding and takes a fresher step with grate guitars and soulful orchestras. It would impress anyone who cares to listen! ^_^&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span> <span class="o">=</span> <span class="n">L</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sample_text</span><span class="p">))</span>
<span class="n">tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(#81) [&#39;this&#39;,&#39;sound&#39;,&#39;track&#39;,&#39;was&#39;,&#39;beautiful&#39;,&#39;!&#39;,&#39;it&#39;,&#39;paints&#39;,&#39;the&#39;,&#39;senery&#39;...]</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can use our vocab to convert each token to it's numeric representation one-by-one using a list comprehension!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">my_vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>
<span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">)</span>
<span class="n">numericalized_tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([  12,  222,  505,   17,  405,   19,    9, 5931,    3,    0,   14,   74,
         424,   34,   85,    5,   49, 1490,    9,   78,    8,  120,   70,  704,
           0,    2,  255,  133,   19,    5,   27,  545,    3,  255,    0, 2380,
          22,   55,   10,   33,   10,    3,  801,    5,   27,  136,  545,    9,
          53,    3,  107,  133,   19,    9,    0,  274,   45, 3843,    0,    6,
         433,    7,    0, 1368,   23,    0, 3306,    6, 3844,    0,    2,    9,
          49, 4234,  183,   70, 2260,    8,  344,   19,    0])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">my_vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[</span><span class="n">num</span><span class="p">]</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">numericalized_tokens</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;this sound track was beautiful ! it paints the &lt;unk&gt; in your mind so well i would recomend it even to people who hate &lt;unk&gt; . game music ! i have played the game &lt;unk&gt; cross but out of all of the games i have ever played it has the best music ! it &lt;unk&gt; away from crude &lt;unk&gt; and takes a &lt;unk&gt; step with &lt;unk&gt; guitars and soulful &lt;unk&gt; . it would impress anyone who cares to listen ! &lt;unk&gt;&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">numericalized_tokens</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([81])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='Notice how uncommon words have value <code>0</code> in their numericalized form! This corresponds to the <code>&lt;unk&gt;</code> token.' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>This example has 81 tokens, but other examples may have more or less. It'll be a good idea to cap the number of tokens + pad the amount of tokens to a desired number of tokens. This will be needed in order to batch our samples together, as they can't vary in size!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">128</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">my_vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

<span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_tokens</span><span class="p">:</span>
    <span class="n">numericalized_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_tokens</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">))</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="n">numericalized_tokens</span><span class="p">[:</span><span class="n">max_tokens</span><span class="p">]</span>

<span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">)</span>
<span class="n">numericalized_tokens</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor([  12,  222,  505,   17,  405,   19,    9, 5931,    3,    0,   14,   74,
         424,   34,   85,    5,   49, 1490,    9,   78,    8,  120,   70,  704,
           0,    2,  255,  133,   19,    5,   27,  545,    3,  255,    0, 2380,
          22,   55,   10,   33,   10,    3,  801,    5,   27,  136,  545,    9,
          53,    3,  107,  133,   19,    9,    0,  274,   45, 3843,    0,    6,
         433,    7,    0, 1368,   23,    0, 3306,    6, 3844,    0,    2,    9,
          49, 4234,  183,   70, 2260,    8,  344,   19,    0,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
           1,    1,    1,    1,    1,    1,    1,    1])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='Remember that the pad token is represented by a 1 in it&#8217;s numericalized form!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="3.-Dataset-&amp;-DataLoaders-[PyTorch-&amp;-fastai]">3. Dataset &amp; DataLoaders [PyTorch &amp; fastai]<a class="anchor-link" href="#3.-Dataset-&amp;-DataLoaders-[PyTorch-&amp;-fastai]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now that we have everything we need to tokenize and numericalize our input, let's go ahead and make a simple Dataset class</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>

<span class="k">class</span> <span class="nc">Simple_Dataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">max_tokens</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_tokens</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">get_tokenizer</span><span class="p">(</span><span class="s2">&quot;basic_english&quot;</span><span class="p">)</span>
        
        <span class="c1"># label 1 is negative sentiment and label 2 is positive sentiment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">numericalized_tokens</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab</span><span class="o">.</span><span class="n">get_itos</span><span class="p">()[</span><span class="n">num</span><span class="p">]</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">numericalized_tokens</span><span class="p">])</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">label</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

        <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
        <span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="p">[</span><span class="n">my_vocab</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">tokens</span><span class="p">]</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">max_tokens</span><span class="p">:</span>
            <span class="n">numericalized_tokens</span> <span class="o">+=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_tokens</span><span class="o">-</span><span class="nb">len</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="n">numericalized_tokens</span><span class="p">[:</span><span class="n">max_tokens</span><span class="p">]</span>

        <span class="n">numericalized_tokens</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">numericalized_tokens</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">numericalized_tokens</span><span class="p">,</span> <span class="n">label</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">Simple_Dataset</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">my_vocab</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">Simple_Dataset</span><span class="p">(</span><span class="n">valid_df</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">my_vocab</span><span class="p">,</span> <span class="n">max_tokens</span><span class="o">=</span><span class="mi">128</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(40000, 2000)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='40k training samples and 2k validation samples. Feel free to try this on a larger dataset!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokens</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">tokens</span><span class="p">,</span> <span class="n">label</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>(tensor([  12,  222,  505,   17,  405,   19,    9, 5931,    3,    0,   14,   74,
          424,   34,   85,    5,   49, 1490,    9,   78,    8,  120,   70,  704,
            0,    2,  255,  133,   19,    5,   27,  545,    3,  255,    0, 2380,
           22,   55,   10,   33,   10,    3,  801,    5,   27,  136,  545,    9,
           53,    3,  107,  133,   19,    9,    0,  274,   45, 3843,    0,    6,
          433,    7,    0, 1368,   23,    0, 3306,    6, 3844,    0,    2,    9,
           49, 4234,  183,   70, 2260,    8,  344,   19,    0,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,
            1,    1,    1,    1,    1,    1,    1,    1]),
 tensor(1))</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokens</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&#39;this sound track was beautiful ! it paints the &lt;unk&gt; in your mind so well i would recomend it even to people who hate &lt;unk&gt; . game music ! i have played the game &lt;unk&gt; cross but out of all of the games i have ever played it has the best music ! it &lt;unk&gt; away from crude &lt;unk&gt; and takes a &lt;unk&gt; step with &lt;unk&gt; guitars and soulful &lt;unk&gt; . it would impress anyone who cares to listen ! &lt;unk&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt; &lt;pad&gt;&#39;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can now create our fastai DataLoaders <code>dls</code> to use for later!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>

<span class="n">dls</span> <span class="o">=</span> <span class="n">DataLoaders</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
<span class="n">dls</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;fastai.data.core.DataLoaders at 0x7f5c440896d0&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="4.-Model-[PyTorch]">4. Model [PyTorch]<a class="anchor-link" href="#4.-Model-[PyTorch]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now to the model creation section! Our PyTorch model will contain the following layers/components:</p>
<ul>
<li><strong>Embedding Layer</strong>: converts numericalized tokens into their embedding representation</li>
<li><strong>LSTM</strong>: processes the sequence of embeddings</li>
<li><strong>Head</strong>: Takes final feature vector of LSTM for classification prediction</li>
</ul>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse</span>
<span class="k">class</span> <span class="nc">Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">vocab</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span> <span class="o">=</span> <span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span><span class="o">.</span><span class="n">shape</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">emb</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">_weight</span><span class="o">=</span><span class="n">vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">input_size</span> <span class="o">=</span> <span class="n">emb_size</span><span class="p">,</span> <span class="n">hidden_size</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span> <span class="n">batch_first</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">head</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">))</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_data</span><span class="p">):</span>

        <span class="n">token_embs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">emb</span><span class="p">(</span><span class="n">batch_data</span><span class="p">)</span>
        
        <span class="n">outputs</span><span class="p">,</span> <span class="p">(</span><span class="n">h_n</span><span class="p">,</span> <span class="n">c_n</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lstm</span><span class="p">(</span><span class="n">token_embs</span><span class="p">)</span>
        
        <span class="c1"># Assuming a batch size of 32, h_n will have a shape of:</span>
        
        <span class="c1"># shape = 2, 32, 64</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">h_n</span>
        <span class="c1"># shape = 32, 2, 64</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
        <span class="c1"># shape = 32, 128</span>
        <span class="n">last_hidden_state</span> <span class="o">=</span> <span class="n">last_hidden_state</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">start_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head</span><span class="p">(</span><span class="n">last_hidden_state</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">logits</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include warning.html content='If you forget the <code>_weight=vocab.vectors</code> in the PyTorch <code>nn.Embedding()</code> layer creation function, you&#8217;ll simply initialize your embedding with all random numbers &amp; nothing will transfer from GloVe! Feel free to comment out that bit and see how the performance drops due to the lack of transfer learning!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">my_vocab</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">model</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Model(
  (emb): Embedding(7591, 100)
  (lstm): LSTM(100, 64, num_layers=2, batch_first=True)
  (head): Sequential(
    (0): Linear(in_features=128, out_features=64, bias=True)
    (1): ReLU()
    (2): Linear(in_features=64, out_features=2, bias=True)
  )
)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's double check that some of our embeddings were successfully loaded from the domain-overlapping tokens from GloVe. Below is our preloaded embedding matrix!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">emb</span><span class="o">.</span><span class="n">parameters</span><span class="p">())[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">embedding_matrix</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>Parameter containing:
tensor([[-0.4916,  1.7088,  0.7117,  ...,  0.1416,  0.9630,  0.4806],
        [ 0.4609, -0.7693,  0.0846,  ..., -0.5245,  0.7817, -0.2093],
        [-0.3398,  0.2094,  0.4635,  ..., -0.2339,  0.4730, -0.0288],
        ...,
        [ 0.2659,  0.1006, -0.1915,  ...,  0.4461,  0.2491,  0.0310],
        [ 0.2600,  0.0077,  0.6122,  ...,  0.1818, -0.2181,  0.0772],
        [-0.2763, -0.2575, -0.1396,  ...,  0.1692, -0.1993,  0.4247]],
       requires_grad=True)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Index <code>3</code> corresponds to <code>'the'</code>:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">my_vocab</span><span class="o">.</span><span class="n">vectors</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span><span class="o">.</span><span class="n">equal</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">[</span><span class="mi">3</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>True</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='Our pre-loaded embedding parameters for index 3 matches up with our vocab vectors, nice!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">total_params</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="n">total_params</span> <span class="o">+=</span> <span class="n">p</span><span class="o">.</span><span class="n">numel</span><span class="p">()</span>
<span class="n">total_params</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>843262</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='~865k parameters!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Now let's go ahead and make sure we can do a forward pass through our model, our loss function will be <code>CrossEntropyLoss</code> as it's a classification task.</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">batched_data</span><span class="p">,</span> <span class="n">batched_labels</span> <span class="o">=</span> <span class="n">train_dl</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batched_data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">batched_labels</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">

<div class="output_subarea output_stream output_stdout output_text">
<pre>torch.Size([32, 128]) torch.Size([32])
</pre>
</div>
</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batched_data</span><span class="p">)</span>
<span class="n">logits</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>torch.Size([32, 2])</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">loss_func</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>

<span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">batched_labels</span><span class="p">)</span>
<span class="n">loss</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>tensor(0.7026)</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Sweet!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="5.-Training/Fitting-[fastai]">5. Training/Fitting [fastai]<a class="anchor-link" href="#5.-Training/Fitting-[fastai]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Time to use fastai to contain our <code>dls</code>, <code>model</code>, and <code>metrics</code> &amp; assist with training using certain best practices!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">])</span>
<span class="n">learn</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>&lt;fastai.learner.Learner at 0x7f5c4113b3d0&gt;</pre>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

</div>

</div>

<div class="output_area">



<div class="output_text output_subarea output_execute_result">
<pre>SuggestedLRs(valley=tensor(0.0052))</pre>
</div>

</div>

<div class="output_area">



<div class="output_png output_subarea ">
<img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAY4AAAEKCAYAAAAFJbKyAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAz3ElEQVR4nO3deXxU1fn48c+THRIIgQQMa1hlJ2AAxb24oFURK4pSRQtVf63ab79+rXax2n5b61fr0iqKqChaK1o3aIsrVVFBJcgW9iUBAiFkgez7PL8/5oJDyDKTZDJJ5nm/Xnkl99x77n3uMMwz59x7zxFVxRhjjPFWSKADMMYY075Y4jDGGOMTSxzGGGN8YonDGGOMTyxxGGOM8YklDmOMMT4JC3QArSE+Pl6TkpICHYYxxrQra9euzVXVhNrlQZE4kpKSSE1NDXQYxhjTrojI3rrKravKGGOMTyxxGGOM8YklDmOMMT4JimscdamqqiIzM5Py8vJAhxIwUVFR9O3bl/Dw8ECHYoxpR4I2cWRmZtKlSxeSkpIQkUCH0+pUlby8PDIzMxk4cGCgwzHGtCNB21VVXl5Ojx49gjJpAIgIPXr0COoWlzGmaYI2cQBBmzSOCfbzN6Yjc7mUt9ZmUuNq+akzgjpxtCcxMTEAZGRkMHr06ABHY4xpy1wu5b6ladz1jw0s35TV4vu3xOGtjW/A46PhgW7u3xvfCHRExhhzEpdL+fW7abz69T5uO3cwl41NbPFjWOLwxsY34J93QsF+QN2//3lns5LHPffcw9NPP318+YEHHuB3v/sdU6dOZcKECYwZM4alS5c2uI+amhruvvtuJk6cyNixY3n22WcBuOGGG06oO3v2bJYtW9bkWI0x7YPLpfzqnU289s0+fnLeYO6ZdqpfuqQtcXhjxe+hquzEsqoyd3kTzZo1i9dff/348htvvMHNN9/MO++8w7fffssnn3zCXXfdRUNT+77wwgvExsayZs0a1qxZw3PPPUd6ejrz5s3jxRdfBKCgoIBVq1Zx6aWXNjlWY0z78PAH21myZj+3nz+Euy/2T9IAPycOEZkmIttFZJeI3FvH+rtFZL3zkyYiNSLS3Vm3SEQOi0harTrdReQjEdnp/I7z5zkAUJDpW7kXxo8fz+HDhzl48CAbNmwgLi6OxMREfvWrXzF27FguuOACDhw4QHZ2dr37+PDDD3n55ZdJTk5m8uTJ5OXlsXPnTs4991x27drF4cOHee211/jBD35AWFjQ3nltTND454aDTB3ek7suGubXm1/89mkiIqHAfOBCIBNYIyLLVHXLsW1U9RHgEWf7y4Gfq2q+s/ol4Cng5Vq7vhdYoaoPOcnoXuAef50HALF9nW6qOsqb4eqrr+bNN9/k0KFDzJo1i1dffZWcnBzWrl1LeHg4SUlJDd4uq6o8+eSTXHzxxSetu+GGG3j11VdZsmQJixYtalacxpi2r6CsigNHy5h9en+/3zHpzxbHJGCXqu5R1UpgCTC9ge2vA147tqCqK4H8OrabDix2/l4MXNki0TZk6m8hvNOJZeGd3OXNMGvWLJYsWcKbb77J1VdfTUFBAT179iQ8PJxPPvmEvXvrHJjyuIsvvphnnnmGqqoqAHbs2EFJSQkAN910E0888QQAo0aNalacxpi2b2tWIQAjE7v6/Vj+7L/oA3h+Tc8EJte1oYh0BqYBt3ux316qmgWgqlki0rO5gTZq7DXu3yt+7+6eiu3rThrHypto1KhRFBUV0adPHxITE5k9ezaXX345KSkpJCcnM3z48Abrz5s3j4yMDCZMmICqkpCQwLvvvgtAr169GDFiBFdeeWWzYjTGtA8dJXHU1Vaq70rv5cCXHt1UzT+4yC3ALQD9+/dv/g7HXtPsRFGXTZs2Hf87Pj6e1atX17ldcXEx4J5bJC3NfdknJCSEBx98kAcffPCk7UtLS9m5cyfXXXddi8dsjGl7thwsJD4mgoQukX4/lj+7qjKBfh7LfYGD9Ww7C49uqkZki0gigPP7cF0bqepCVU1R1ZSEhJMmsOrQPv74Y4YPH84dd9xBbGxsoMMxxrSCrYcKGZHYtVVGhPBni2MNMFREBgIHcCeH62tvJCKxwLnAD73c7zJgDvCQ87vhhx2C0AUXXMC+ffsCHYYxppVU1bjYcaiYm89MapXj+a3FoarVuK9ZfABsBd5Q1c0icpuI3Oax6QzgQ1Ut8awvIq8Bq4FTRSRTROY6qx4CLhSRnbjv2HrIX+dgjDHtwZ6cEiprXIxohesb4Odh1VV1ObC8VtmCWssv4b71tnbdOjvnVTUPmNpC8QX1QH8NPVxojGk/tmQVAHSMxNGWRUVFkZeXF7RDqx+bjyMqKirQoRgTdMoqa7jqmVVUVtcwIrErIxK7Mr5fN04f1IOQEN8/j7ZmFRERFsKghGg/RHuyoE0cffv2JTMzk5ycnECHEjDHZgA0xrSuV7/ey9asQs4ZlsCGzKP8a6N7BNtB8dHcdGYSP5jQl+hI7z+et2YVMqxXDOGhrTOKVNAmjvDwcJv5zhjT6korq1nw2W7OGhLPyz+aBEBheRWfbDvMoi8z+O3SzTzywXYuHNGLlKTuTEyKY0jPmHp7RlSVLQcLmTrC/4+0HRO0icMYYwLhldV7yS2u5OcXDj1e1jUqnOnJfZie3Idv9x3h5VUZrNyZw9vrDgDQq2skj85M5qyh8SftL6eogrySyla7vgGWOIwxptWUVFTz7Mo9nDMsgdMGdK9zmwn945jQPw5VJSOvlDUZ+Tz/+R5uXPQ1v5g2nFvPGXRC62NzKz4xfowlDmOMaSWLV2eQX1LJzy8Y2ui2IsLA+GgGxkfz/TGJ/OKtjTz03jY2Zh7l4avHEeNcAzk21MjwVkwcNh+HMca0gqLyKhau3MP5pyYwvr9vs0FER4bx1HXj+dWlw3k/7RDXPrua/JJKwD3USN+4TsR2CvdH2HWyxGGMMa3gjdRMjpZW8fMLhzWpvohwyzmDeWHORHYdLua6hV+RU1TB1qzCVr2+AZY4jDGmVaTnFhPXOZyxfbs1az/nD+/JizdNZF9+KdcuXE16bkmrXt8ASxzGGNMqCsqqW6w7acqQeF6eO4nDhRW4tPWeGD/GEocxxrSCgrKqFr0OMTGpO3+bN5kZ4/swZUiPFtuvN+yuKmOMaQWFZVV0beEL2Mn9upF8bXKL7tMb1uIwxphW4I/EESiWOIwxphW0dFdVIFniMMYYP1NVSxzGGGO8V1pZQ7VLLXEYY4zxTkFZFYAlDmOMMd4pLHcnjq5RljiMMcZ4oaDUWhzGGGN8YF1VxhhjfGKJwwciMk1EtovILhG5t471d4vIeucnTURqRKR7Q3VF5AEROeBR71J/noMxxjSXJQ4viUgoMB+4BBgJXCciIz23UdVHVDVZVZOBXwKfqWq+F3UfP1ZPVZf76xyMMaYlFJZVIQJdojrGKE/+bHFMAnap6h5VrQSWANMb2P464LUm1jXGmDaroKyKmMgwQkKk8Y3bAX8mjj7Afo/lTKfsJCLSGZgGvOVl3dtFZKOILBKROqfSEpFbRCRVRFJzcnKaeg7GGNNsheUtN6R6W+DPxFFXatV6tr0c+FJV872o+wwwGEgGsoBH69qhqi5U1RRVTUlISPA6aGOMaWkdabgR8G/iyAT6eSz3BQ7Ws+0svuumarCuqmarao2quoDncHdrGWNMm2WJw3trgKEiMlBEInAnh2W1NxKRWOBcYKk3dUUk0WO7GUCan+I3xpgW0dESh98u8atqtYjcDnwAhAKLVHWziNzmrF/gbDoD+FBVSxqr66x+WESScXddZQC3+uscjDGmJVji8IFzq+zyWmULai2/BLzkTV2n/IYWDdIYY/ysoyUOe3LcGGP8qLyqhspqV4eZ/Q8scRhjjF8VOk+NW+IwxhjjlY423AhY4jDGGL+yxGGMMcYnljiMMcb4xBKHMcYYnxy/ON5BRsYFSxzGGONXBWXVgN1VZYwxxksFZVVER4QSHtpxPm47zpkYY0wb1NGeGgdLHMYY41cFZVUdqpsKLHEYY4xfFVqLwxhjjC+sxWGMMcYnheXW4jDGGOMDuzhujDHGa1U1LkorayxxGGOM8U5HHG4ELHEYY4zfWOIwxhjjE0scxhhjfPLd7H8dZ4BD8HPiEJFpIrJdRHaJyL11rL9bRNY7P2kiUiMi3RuqKyLdReQjEdnp/I7z5zkYY0xTWYvDRyISCswHLgFGAteJyEjPbVT1EVVNVtVk4JfAZ6qa30jde4EVqjoUWOEsG2NMm9MR5xsH/7Y4JgG7VHWPqlYCS4DpDWx/HfCaF3WnA4udvxcDV7Z04MYY0xKsxeG7PsB+j+VMp+wkItIZmAa85UXdXqqaBeD87tmCMRtjTJMVlFWxP7/0hOWo8BAiw0IDGFXL82fikDrKtJ5tLwe+VNX8JtSt++Ait4hIqoik5uTk+FLVGGN8tjunmEv/8jmX/vVzCkrdLY2O+NQ4+DdxZAL9PJb7Agfr2XYW33VTNVY3W0QSAZzfh+vaoaouVNUUVU1JSEhoQvjGGOOdDfuPMnPBakorqykqr2bRl+mAM8BhlCUOX6wBhorIQBGJwJ0cltXeSERigXOBpV7WXQbMcf6eU6ueMca0qs935nDdc18RHRnKOz85k4tH9WLRl+kUlFVRWFbdIVscfru5WFWrReR24AMgFFikqptF5DZn/QJn0xnAh6pa0lhdZ/VDwBsiMhfYB8z01zkYY0xtLpeSdrCAT7bl8J/th9mYeZThp3Rl8c0T6dk1ijunDuWDzdks+sKdPBJjowIdcosTVZ8uHbRLKSkpmpqaGugwjDHt1I7sIr7clctXe/L4Jj2fI6VViMC4vt343vCe3HRm0gldUre+ksqq3XlEhIZw7rAEHrs2OXDBN4OIrFXVlNrlHetxRmOMaUHlVTX8/l9b+PvX+wDo170TU0f0YsrgHpw7LIEeMZF11jvW6oCO9wwHWOIwxpg67c4p5qevfsu2Q0Xccs4g5kxJok+3Tl7VHdU7lotG9uLDLdl2jcMYY4LBiq3Z3PnaOiLCQnjxpomcP9z3x8XunDqUj7Zm06trx7vGYYnDGGNqefTDHZwSG8Xf5k0mMda7VkZto/vE8tHPz6Ff984tHF3g2ei4xhjjQVXJyCvhnGEJTU4axwzp2aXDPTUOljiMMeYEucWVlFbWMKADthRaiiUOY4zxsC/f/UjZgB7RAY6k7bLEYYwxHjJy3YMUDuhhLY76WOIwxhgPe/NLCRHoG2eJoz6WOIwxxsO+vBISYzsREWYfj/WxV8YYYzxk5JWSFG+tjYZY4jDGGA/78kvp390ujDfEEocxxjgKy6vIL6m0C+ONsMRhjDGOfXnuO6qSLHE0yBKHMcY49jqJw7qqGmaJwxhjHHudh//6W4ujQZY4jDHGsTe3lPiYSGIibfzXhljiMMYYx978Ersw7gVLHKbDcLmUVbtzue/dNF5enYHL1fGnRTYta29eqQ1u6AWv2mMiEg2UqapLRIYBw4H3VLXKr9GZVlVZ7SKroKzdDe5WUFbFwpW7eXfdQQ4cLSMiNITKGhf/2pjFozPHdcj5EEzLK6+q4VBhebt7/weCty2OlUCUiPQBVgA3Ay/5KyjTumpcyltrM/neo59y7iOf8tR/dqLaPr6tV1TXMG/xGp75dDdDesbwl1nJbLj/Ih6+eixbDhYy7YmV/P3rfdQ00vpQVYorqhvdrrkOFZSz7VAhe3KKOXC0jMJy++7VVmQeKUXVBjf0hrdXgERVS0VkLvCkqj4sIusarSQyDfgLEAo8r6oP1bHNecATQDiQq6rnOuU/A34MCPCcqj7hlD/glOc4u/iVqi738jyMQ1XJPFLG2r1HePrTXezILmZ0n66M6t2VP3+4gz25JfzpqjFtehIaVeXetzaxJuMIf71uPFeM63183TUp/ZgyuAd3/2Mjv3pnE89/sYefnjeEK5J7Ex4aws7sIpZtOMgn2w+TU1TBkZIqKmtcDIyP5vk5KQxOiDnhWO4P+xLOHhpPlyjf5pBWVVbvyWPRFxms2JZN7Zw8KD6aCQPiSBkQx+g+sQxOiKFTRNNe96oaF++uO8CFI3vRrXNEk/YRrI6Nimt3VDXO68QhImcAs4G53tQVkVBgPnAhkAmsEZFlqrrFY5tuwNPANFXdJyI9nfLRuJPDJKASeF9E/q2qO52qj6vqn72M3XhYtSuXpz/dTdrBAo6Wur/tDoqPZv71E7hk9CmIwF9X7OLxj3eQmV/GTWcmkXmklH35pdS44BcXn0pcdNv4QHrqP7t4Z90B/vvCYSckjWP6xnXm1XmTeS/tEE99sou7/rGBxz/eQUxkGNsOFREiMDGpOyOHdaV7dCRdosJY9EU6M+Z/yYIbTmPK4Hgqqmv464qdLPhsDzUuJSIshPNPTeDSMYn0jetEZFgoUeGhiEBJRTXFFdUUlVeTX1JJTlEFOUUVrMnIZ9uhIrpHR3D7+UMYkdiVymoXldUucksq+HbvUVZszebNtZkAiEC/uM4M69WF0wbEMWlgHGP6dCMiLISjpZXsyy+loKyKMwb1ICz0u04Dl0v5xZsbeWfdAYb2jOGVuZM5JbbjzXftL3vzjz38Z11VjfE2cfwX8EvgHVXdLCKDgE8aqTMJ2KWqewBEZAkwHdjisc31wNuqug9AVQ875SOAr1S11Kn7GTADeNjLeE0d3l13gP/5xwYSu0VxyehERvfpyujesYzq3fWED6CfXTCUpPjO3P3mRn7y6rcAxHYKp7Symt05xbwyd1LAWyLLNhzk0Y92MGN8H+743pB6twsJEb4/NpFLx5zCf7YdZuHKPVS7lPsvH8n3xybSs8uJH6xXjOvNj15aw40vfMOdU4fyr40H2ZFdzMzT+jJjfB8+3JLN8k1ZfLA526s4YzuFkxQfzcM/GMsVyb2JCq/7dVNV0nNL2HaoiJ3Zxew4XMTWg4V8vNV9nMiwECLDQigsrz5eZ/LA7jx5/Xh6dolCVfnj8q28s+4AM0/ry3tph/jBM6t4dd5kkuLtg9Ab+/JK6BIZRlxn31qUwUh87csWkRAgRlULG9nuatwtiXnO8g3AZFW93WObJ3B3UY0CugB/UdWXRWQEsBQ4AyjDfV0lVVXvcLqqbgIKgVTgLlU9UsfxbwFuAejfv/9pe/fu9ek827OPt2Szance5w9POP6t9LmVe/jj8q2cMagHz954Gl296G7JPFLK0dIq+sV1JrZzOEvXH+BnS9Zz1fg+PHrNOESkFc7mRFkFZTzy/nbeXneAiUlx/G3e5BZPYoXlVfz01W/5fGcup3SN4k9XjeH84T2Pr3e5lLSDBRwpraK8qobyqhpUISYyjJioMGIiw+geHUGPmIhmx5ZbXEFqRj5rMo5QWe1iQI/O9O/emZziCv73X1voGhXO/NkTSM04wv+9v42bpiRx/+UjSTtQyJwXvyFEhD9cOZqK6hoOFZRTWF7FrIn97YaBOsxZ9A15JRX8646zAx1KmyEia1U15aRybxKHiPwduA2oAdYCscBjqvpIA3VmAhfXShyTVPUOj22eAlKAqUAnYDXwfVXd4VxP+SlQjLuVUqaqPxeRXkAuoMD/Aomq+qOG4k9JSdHU1NRGz7M17DpczEPvbePRmeOIbeCbjaryftohOkWEct6pPevdrnadZ1fu4aH3tiECqtA9OoKRiV35Ylcu3x+byGPXjGvWh9lfV+zksY928N8XDuPOqUObvB9flVfV8Mynu3l25W5cLvjRWQO5/XtD/PagVnWNi4+2ZDNlSDyxndrmN9Bthwq57ZW17D9SRo1LuWJcb564NpmQEHdC33W4mBte+JqsgvIT6g1KiOadn5zZZs8rUM7/86eMTOzK/NkTAh1Km1Ff4vD2f91IVS0UkdnAcuAe3Amk3sSB+7pGP4/lvsDBOrbJVdUSoEREVgLjgB2q+gLwghP8g862qOrxPgIReQ74l5fn0Ca8u+4AH2/N5rU1+7jt3MF1blNcUc1v3tnEu+sP0iUqjNW/nNroB2R1jYv7lm7mtW/2cfm43vzhytGs3p3H8k1ZrNqdy9yzBvLrS0cc/1Bpqju+N4SM3BIe+2gHPbtEMmtS/2btz1v3L93M66n7+f7YRO6dNtzv35jDQkO4ZEyiX4/RXMNP6cqyO87it++mUe1S/jxz3An/vkN6xrD8zrPZfLCQXl0jOSU2iq1ZRcx+/itu//u3vHjTxBO6KIPNrsPFREeGkhjbieoaF5lHSpk2+pRAh9UueJs4wkUkHLgSeEpVq0SksabKGmCoiAwEDgCzcF/T8LQUeEpEwoAIYDLwOICI9FTVwyLSH7gKd7cVIpKoqllO/RlAmpfn0Cas3pMHwOJVGcw9ayDhtf7jbsw8yh2vrWN/fimzJvZjyZr9LPlmH/POHlTvPqtrXMx7OZVPt+fwk/MG8z8XnUpIiDBt9Ckt/h9BRPjTD8aQXVTOvW9v4otdufzv9NFeXTB3uZQPNh+iuKKaGeP7eP2htX7/UV5P3c+8swbym8tGNvcUOpSuUeE8MWt8vevjoiM4a2j88eVJA7vzxyvH8Iu3NvKHf2/lgStGtUaYbUZxRTXL1h/k9TX72JBZgAicMzSBqSN6UlWjNiqul7xNHM8CGcAGYKWIDMB9jaFeqlotIrcDH+C+HXeRc2H9Nmf9AlXdKiLvAxsBF+5bdo8lgrdEpAdQBfzU4zrGwyKSjLurKgO41ctzCLiSimo27D/KyMSubMkq5IPNh7hs7Hd3A321J48bXviahJhIXr/1DCYmdWdPbgkvfpnBnClJJyWZ7+rl8+n2HH596Qh+fE79CaalRIaFsvjmSTzz6W7+smInX6fnc99lIymrrGbt3iOs3XuEiLBQLh+XyPTkPvSOjeKzHTk8/P52tmS53zaLvszgwRmjGd8/rsFjuVzK/UvTSOgSyc8uaL2usY7smon92JFdxPNfpDOkZww/PH1Aqx27stpV55Ssu3OKWfjZHk49pQuXj+tNQpfIFjleWWUN6/YdIXWv+2dNej5lVTWc2qsL9102koLSSt5IzeSzHe67+21UXO/4fHH8eEWRMFWtbnzLwGsr1zg+25HDnEXf8NLNE7l/2WZ6REfw9k/OBNxJ5eInVhIWIrz70zOP34P/8ZZs5r2cyl9mJTM9uU+d+/31O5t4Z90Bvr3vwnrv2vGXtAMF3PXGBrZnFwEQ1zmcCf3jOFpWxdq97lzfv3tn9uWX0jeuE3ddNIxO4aE8sGwL2UXlXD+pPxeM7EVCTCQ9u0TSIyaSUI/uljfW7OcXb23ksWvGcdWEvq16bh1ZjUuZt3gNn+3I4enZE5g22v/dct+k5zP7+a+4cGQv7rroVAYnxKCqvPr1Pv7w7y24XFBZ4yI0RDh7aDzTk3szdUSv4zdyqCpfp+cz/5NdZBWUs/hHk+jTrVO9x9ufX8rMBas5VFiOCAzr2YVJA7tz1YQ+JPfrdvzmjuoaFyt35rB+fwF3fG9IvV/QglFzL47HAvcD5zhFnwG/V9WCFo3ST9pK4njovW288MUeNtx/Ea+v2c/v/rmFd34yhfH94/jNu5t49et9vOG0NI5xuZQLHv+MzhGh/PP2s066k6nGpUx+cAWTB3Vn/vWBuahXUV3Dl7tyGdAjmkHx0cdj3J9fytL1B1i1O49po09h1sT+x79tFldU8+iH21m8KgPPh7V7REdwyzmDuOGMAVTVKN/786ckxUfz5m1nBOQuro6spKKaH77wNWkHCnh+zkTOHZbgt2OpKlcvWM3unGKqql2UV7uYeVpfcosr+HjrYc4eGs+fZ46joKyKd9cdYOn674aPOXtoPGcOiee9tCzWZBwhPiaSiqoa4rtE8vqtp590SzVAfkklVz+zirySSh65eiyTB/WwmwGaoLmJ4y3c1xIWO0U3AONU9aoWjdJP2krimD7/S8JDhDf/3xSKK6o548EVnD+8J9dO7Mfs579m7lkDua+OPvzXvtnHL9/exN9/PJkpg+NPWPdNej7XPLuaJ68bz+V1PATX1h0uLGf/kTLnYblyPtp6mJU7cugRHcGQnjF8k5HPP28/i9F9YgMdaodUUFrFrOe+Ij23mFfmTj7hS0tL+s+2bH70Uip/nDGaaaNOYf4nu/nbV3tB4J5pw7l5StIJF/ZdLmXd/qO8tymL99IOceBoGb1jo7j13MFcO7Efmw8W8MPnv2FAj84sueX0E56SL62s5vrnvmZrViGvzptMip/OKRg0N3GsV9XkxsraqraQOIrKq0j+/Uf85LzB3HXRqQD877+2sHhVBgldIukUHsq/7zy7zqEmyqtqOOv//sOYPrG8ePOkE9b97p+befXrfXx734UdZg6BtXuP8MTHO/h8Zy6zJ/fnjzPGBDqkDi23uIJrnl1NTmEFC29M4YzBPRrcfsP+o3yy/TC3njPYq6FRXC7lsie/oLiimhV3nXu8KyiroIzqGm30DrljD0f2jet8wvWRL3flcvNLaxhxShd+c9lIOoWHEhUewp+Wb+OT7Yd55oencfEou0uqOZp7O26ZiJylql84OzsT94N5xktrMvKpcSlnDPruP+VNU5J48ct0sgvL+cdtU+r9TxgVHsqNZyTx2Ec72HywgFG93d++VZUP0g5xztCEDpM0AE4bEMcrcyezJ6fYHlRrBfExkfxt7mSuf+4rrnvuK2ZP7s+9lww/aUyuY92LL63KQBVSM47w/JyU+q+rbXwDVvweKchkoasHWSl3Ex56/vHVibH1X5/wJCIMqjV2GMCZQ+J5+voJ3Pa3tcxcsPqEdX+4crQlDT/y9tPmNuBl51oHwBFgjn9C6phW784jIjSECQO+u4uoX/fO3HXRqXSNCuO0AQ3fXXTD6QNYvCqDu/+xkXd/eiYRYSFsyCzgYEH58RZMR1PXh4Xxj97dOrH8Z2fz2Ic7WPRlOiu2HubnFw4ltlM4VTVKQVkVT3+yi6zCcm44fQBDe3Xht0vT+PHLqTx3ozt55BRV8MTHO/hgcza/7LORGQceJqS6DAH6huTSZ9MDkNQdxl7TYnFfMLIXK+46l715pZRX1VBWVUPPLlGNtppM83iVOFR1AzBORLo6y4Ui8l+4b6M1Xli9J4/x/bud9O3sp+fXP86Sp7joCP501RhueWUtT3y8g19MG857aVmEhQgXjOjlj5BNkOkcEcZvLhvJZeN6c+9bG7nnrU0nrB/WK4Y3r59y/EtOVFgIv3hrI7e8spZJSXE88+luKqpdnDG4B6dnzCdETuyUkKoyWPH7Fk0cAAN6RNscGq3Mp/6NWuNT/Tfu4dBNIwpKq9h8sJCfNXOIjotGncK1Kf1Y8Nluzh/ek/fTDnHG4B4NDl1ijK+S+3Xjn3ecxY7sIkJECAsRwkJD6BfX6YSHNmem9EMV7nl7Iyt35HDxqF7cM204gxJi0Afy6t55QWYrnYXxp+Z0jNu9kV76Oj0PVU64vtFU910+klV7crn1lbXkl1Ry6zl1D1tiTHOEh4Ycv5bWkGsm9iOxWxSdwkNPuHtJYvtCwf6TK8TaszgdQXOedGkfU8S1Aav35BEZFkJy/27N3ldMZBiPX5PM0dJKQgQuGmXdVCawzh6acPItr1N/C+G1Ln6Hd3KXm3avscmYiqg7QQju0WyNF1bvziMlKa7Fhv9OSerOry4dQVZBOfExLTM0gzEt6th1jBW/d3dPxfZ1J40Wvr5hAqPBxKGqXVorkI5qZ3YR2w4V1flgX3M0NOihMW3C2GssUXRQNiiLn7297gChIVLn1KbGGNMeWeLwI5dLeXfdAc4ZGt9io30aY0ygWeLwo6/25JFVUG6juhpjOhRLHH701rcH6BIZxoUj7c4nY0zHYYnDT0orq3k/LYtLxyS2+hwZxhjjT5Y4/OTDzdmUVNZw1YS6J18yxpj2yhKHn7z1bSZ9unXy2/wGxhgTKJY4/CC7sJwvd+Vy1YQ+J0xOY4wxHYEljhZWVlnDr9/ZhEthxnjrpjLGdDwdZ/afNuBoaSVzF6fy7b4j/H76KJtPwhjTIfm1xSEi00Rku4jsEpF769nmPBFZLyKbReQzj/KfiUiaU/5fHuXdReQjEdnp/G54BqRWcuBoGVcvWM2mzALmXz+BG89ICnRIxhjjF35LHCISCswHLgFGAteJyMha23QDngauUNVRwEynfDTwY2ASMA64TESOTWZxL7BCVYcCK5zlgKqucXHdwq/ILizn5bmTuHRMYqBDMsYYv/Fni2MSsEtV96hqJbAEmF5rm+uBt1V1H4CqHnbKRwBfqWqpqlYDnwEznHXTgcXO34uBK/13Ct75JiOfffml/HHGGE5vgTk3jDGmLfNn4ugDeM7kkumUeRoGxInIpyKyVkRudMrTgHNEpIeIdAYuBfo563qpahaA87tnXQcXkVtEJFVEUnNyclrolOr2ftohosJDmDq8zlCMMaZD8efF8bruQ609t0cYcBowFff8HqtF5CtV3Soi/wd8BBQDG4BqXw6uqguBhQApKSl+m3TK5VLeTzvEucMSiI60ew2MMR2fP1scmXzXSgDoCxysY5v3VbVEVXOBlbivaaCqL6jqBFU9B8gHdjp1skUkEcD5fZgAWrf/CIeLKrhktF3XMMYEB38mjjXAUBEZKCIRwCxgWa1tlgJni0iY0yU1GdgKICI9nd/9gauA15w6y4A5zt9znH0EzHubDhEeKnxvhHVTGWOCg9/6VlS1WkRuBz4AQoFFqrpZRG5z1i9wuqTeBzYCLuB5VU1zdvGWiPQAqoCfquoRp/wh4A0RmQvsw7kTKxBUlffSDnH20AS6RoUHKgxjjGlVfu2UV9XlwPJaZQtqLT8CPFJH3bPr2Wce7msiAbfpQAEHjpbxswuGNr6xMcZ0EDbkSDO8l3aI0BDhwhE234YxJnhY4mgiVffdVGcM6kFcdESgwzHGmFZjiaOJtmcXkZ5bwiVjTgl0KMYY06oscTTRf7a57wK+aKQlDmNMcLHE0UQHjpTRPTqChC6RgQ7FGGNalSWOJsotriA+xq5tGGOCjyWOJsopqrDWhjEmKFniaKLc4koSYixxGGOCjyWOJsopqiDeEocxJghZ4miCkopqyqpqrKvKGBOULHE0QU5RBYC1OIwxQckSRxPkFLsTh7U4jDHByBJHE+Rai8MYE8QscTSBtTiMMcHMEkcT5BZVECLQ3QY3NMYEIUscTZBTXEH36EhCQ+qaVt0YYzo2SxxNkFNUacONGGOCliWOJsgptuFGjDHByxJHE+TaOFXGmCBmicNHqupucdituMaYIOXXxCEi00Rku4jsEpF769nmPBFZLyKbReQzj/KfO2VpIvKaiEQ55Q+IyAGnznoRudSf51BbYXk1ldUua3EYY4KW3xKHiIQC84FLgJHAdSIystY23YCngStUdRQw0ynvA9wJpKjqaCAUmOVR9XFVTXZ+lvvrHOqSW2wP/xljgps/WxyTgF2qukdVK4ElwPRa21wPvK2q+wBU9bDHujCgk4iEAZ2Bg36M1WvHxqmyFocxJlj5M3H0AfZ7LGc6ZZ6GAXEi8qmIrBWRGwFU9QDwZ2AfkAUUqOqHHvVuF5GNIrJIROL8dwonsxaHMSbY+TNx1PV0nNZaDgNOA74PXAzcJyLDnGQwHRgI9AaiReSHTp1ngMFAMu6k8midBxe5RURSRSQ1JyenuedynLU4jDHBzp+JIxPo57Hcl5O7mzKB91W1RFVzgZXAOOACIF1Vc1S1CngbmAKgqtmqWqOqLuA53F1iJ1HVhaqaoqopCQkJLXZSOUUVhIYI3TqFt9g+jTGmPfFn4lgDDBWRgSISgfvi9rJa2ywFzhaRMBHpDEwGtuLuojpdRDqLiABTnXJEJNGj/gwgzY/ncJLc4griYyIIseFGjDFBKsxfO1bVahG5HfgA911Ri1R1s4jc5qxfoKpbReR9YCPgAp5X1TQAEXkT+BaoBtYBC51dPywiybi7vTKAW/11DnWxKWONMcHOb4kDwLlVdnmtsgW1lh8BHqmj7v3A/XWU39DCYfokt7jSrm8YY4KaPTnuo5wie2rcGBPcLHH4wOVS8koqiLcWhzEmiFni8EFBWRVVNWotDmNMULPE4YNjU8Zai8MYE8wscfgg99jDf9biMMYEMUscPjjW4kjoYrP/GWOClyUOHxwfbiQmKsCRGGNM4Fji8EFOcQURoSF07eTXx1+MMaZNs8Thg9yiSuJjInCPgmKMMcHJEocPcortGQ5jjLHE4QN7atwYYyxx+CS3uMLGqTLGBD1LHF6qcSl5xTYyrjHGWOLwUnZhOS6FU2LtVlxjTHCzxOGl9NwSAAbFRwc4EmOMCSxLHF46ljgGJljiMMYEN0scXkrPLSEqPIReXayryhgT3CxxeCk9t4SkHtE217gxJuhZ4vBSem4Jg6ybyhhjLHF4o6rGxf78UgbahXFjjLHE4Y3MI2VUu5SkHpY4jDHGr4lDRKaJyHYR2SUi99azzXkisl5ENovIZx7lP3fK0kTkNRGJcsq7i8hHIrLT+R3nz3MASM8tBrCuKmOMwY+JQ0RCgfnAJcBI4DoRGVlrm27A08AVqjoKmOmU9wHuBFJUdTQQCsxyqt0LrFDVocAKZ9mv0nNLARgYH+PvQxljTJvnzxbHJGCXqu5R1UpgCTC91jbXA2+r6j4AVT3ssS4M6CQiYUBn4KBTPh1Y7Py9GLjSP+F/Jz23mK5RYcR1Dvf3oYwxps3zZ+LoA+z3WM50yjwNA+JE5FMRWSsiNwKo6gHgz8A+IAsoUNUPnTq9VDXL2S4L6FnXwUXkFhFJFZHUnJycZp1Iem4JAxNibB4OY4zBv4mjrk9ZrbUcBpwGfB+4GLhPRIY51y2mAwOB3kC0iPzQl4Or6kJVTVHVlISEBN+j95CeU2JDjRhjjMOfc6BmAv08lvvyXXeT5za5qloClIjISmCcsy5dVXMARORtYArwNyBbRBJVNUtEEoHD+FF5VQ0HC8rtVlxjjHH4s8WxBhgqIgNFJAL3xe1ltbZZCpwtImEi0hmYDGzF3UV1uoh0Fnf/0FSnHGcfc5y/5zj78JuMPPcYVUmWOIwxBvBji0NVq0XkduAD3HdFLVLVzSJym7N+gapuFZH3gY2AC3heVdMARORN4FugGlgHLHR2/RDwhojMxZ1gZvrrHMDdTQU2Kq4xxhzjz64qVHU5sLxW2YJay48Aj9RR937g/jrK83C3QFpFurU4jDHmBPbkeCPSc0ro2SWSmEi/5lhjjGk3LHE0Ij23xFobxhjjwRJHI9Jz7VZcY4zxZImjAQVlVeSVVNqtuMYY48ESRwMycu3CuDHG1GaJowHH5hm3ripjjPmOJY4GpOeWIAL9e3QOdCjGGNNmWOJogKoyuncskWGhgQ7FGGPaDFGtPe5gx5OSkqKpqamBDsMYY9oVEVmrqim1y63FYYwxxieWOIwxxvjEEocxxhifWOIwxhjjE0scxhhjfGKJwxhjjE8scRhjjPGJJQ5jjDE+CYoHAEUkB9gLxAIFTnFjfx/7HQ/k+nhIz/35sr52eUPLteNsTrxNjbk58XqWtdZr3FhZe3tPtLd4G4vTn+9hf8TrWdZR38MDVDXhpFJVDZofYKG3f3v8Tm3OcXxZX7u8oeU64mxyvE2NuTnxBuI1bqysvb0n2lu8XsTpt/ewP+INxGscyPew50+wdVX904e/Pcuacxxf1tcub2i5dpzNideb+nWtb0683hzT13gaW99YWXt7T7S3eGsvt+Z72B/xNnbMxrS39/BxQdFV1Rwikqp1jNXSVrW3eKH9xWzx+ld7ixfaX8zNjTfYWhxNsTDQAfiovcUL7S9mi9e/2lu80P5ibla81uIwxhjjE2txGGOM8YklDmOMMT6xxGGMMcYnljiaQUTOFpEFIvK8iKwKdDyNEZEQEfmjiDwpInMCHU9jROQ8EfnceY3PC3Q83hKRaBFZKyKXBTqWxojICOf1fVNE/l+g42mMiFwpIs+JyFIRuSjQ8TRGRAaJyAsi8magY6mP835d7Lyus72pE7SJQ0QWichhEUmrVT5NRLaLyC4Rubehfajq56p6G/AvYHFbjxeYDvQBqoBMf8XqxNUS8SpQDETh53id2FoiZoB7gDf8E+UJcbXEe3ir8x6+BvDr7aQtFO+7qvpj4CbgWj+G21Lx7lHVuf6Msy4+xn4V8Kbzul7h1QGa8/Rge/4BzgEmAGkeZaHAbmAQEAFsAEYCY3AnB8+fnh713gC6tvV4gXuBW526b7aDeEOcer2AV9vDewK4AJiF+4PtsrYer1PnCmAVcH17iNep9ygwoR3F69f/b82M/ZdAsrPN373ZfxhBSlVXikhSreJJwC5V3QMgIkuA6ar6J6DObgcR6Q8UqGphW49XRDKBSmexxo/httjr6zgCRPolUA8t9BqfD0Tj/g9ZJiLLVdXVVuN19rMMWCYi/wb+7o9YWypeERHgIeA9Vf3WX7G2VLyB4kvsuFvzfYH1eNkLFbSJox59gP0ey5nA5EbqzAVe9FtEDfM13reBJ0XkbGClPwOrh0/xishVwMVAN+Apv0ZWP59iVtVfA4jITUCuv5JGA3x9jc/D3VURCSz3Z2D18PU9fAfuVl2siAxR1QX+DK4Ovr6+PYA/AuNF5JdOggmU+mL/K/CUiHwfL4ckscRxIqmjrMEnJFX1fj/F4g2f4lXVUtyJLlB8jfdt3MkukHx+TwCo6kstH4pXfH2NPwU+9VcwXvA13r/i/qALFF/jzQNu8184PqkzdlUtAW72ZUdBe3G8HplAP4/lvsDBAMXiDYvX/9pbzBavf7W3eD21WOyWOE60BhgqIgNFJAL3Rc5lAY6pIRav/7W3mC1e/2pv8Xpqudhb80p/W/oBXgOy+O7W1LlO+aXADtx3H/w60HFavBazxWvxtrXYbZBDY4wxPrGuKmOMMT6xxGGMMcYnljiMMcb4xBKHMcYYn1jiMMYY4xNLHMYYY3xiicMELREpbuXjtcicLeKep6RARNaJyDYR+bMXda4UkZEtcXxjLHEY00JEpMGx31R1Sgse7nNVHQ+MBy4TkTMb2f5K3CP2GtNsNsihMR5EZDAwH0gASoEfq+o2Ebkc+A3ueQzygNmqmi0iDwC9gSQgV0R2AP1xz3nQH3hC3QPzISLFqhrjjEj7AJALjAbWAj9UVRWRS4HHnHXfAoNUtd7hulW1TETW4x75FBH5MXCLE+cu4AYgGfecG+eKyG+AHzjVTzrPpr5uJrhYi8OYEy0E7lDV04D/AZ52yr8ATne+5S8BfuFR5zTcczJc7ywPxz0c/CTgfhEJr+M444H/wt0KGAScKSJRwLPAJap6Fu4P9QaJSBwwlO+GyX9bVSeq6jhgK+6hJlbhHpPoblVNVtXdDZynMY2yFocxDhGJAaYA/3DPFwR8N4FUX+B1EUnE/W0+3aPqMlUt81j+t6pWABUichj3DIa1p779RlUzneOux91iKQb2qOqxfb+Gu/VQl7NFZCNwKvCQqh5yykeLyB9wz2ESA3zg43ka0yhLHMZ8JwQ4qqrJdax7EnhMVZd5dDUdU1Jr2wqPv2uo+/9ZXdvUNV9CfT5X1ctEZBjwhYi8o6rrgZeAK1V1gzOZ1Hl11G3oPI1plHVVGeNQ9/S/6SIyE9zTlIrIOGd1LHDA+XuOn0LYBgzymPLz2sYqqOoO4E/APU5RFyDL6R6b7bFpkbOusfM0plGWOEww6ywimR4//437w3auiGwANuOekxncLYx/iMjnuC9ctzinu+snwPsi8gWQDRR4UXUBcI6IDATuA74GPsKdiI5ZAtzt3MI7mPrP05hG2bDqxrQhIhKjqsXivvgwH9ipqo8HOi5jPFmLw5i25cfOxfLNuLvHng1sOMaczFocxhhjfGItDmOMMT6xxGGMMcYnljiMMcb4xBKHMcYYn1jiMMYY4xNLHMYYY3zy/wE4I1+YG/wLzQAAAABJRU5ErkJggg==
"
>
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">lr_max</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

<div class="output_wrapper">
<div class="output">

<div class="output_area">


<div class="output_html rendered_html output_subarea ">

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='0' class='' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>
      0.00% [0/5 00:00<00:00]
    </div>
    
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: left;">
      <th>epoch</th>
      <th>train_loss</th>
      <th>valid_loss</th>
      <th>accuracy</th>
      <th>time</th>
    </tr>
  </thead>
  <tbody>
  </tbody>
</table><p>

    <div>
        <style>
            /* Turns off some styling */
            progress {
                /* gets rid of default border in Firefox and Opera. */
                border: none;
                /* Needs to be in here for Safari polyfill so background images work as expected. */
                background-size: auto;
            }
            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {
                background: #F44336;
            }
        </style>
      <progress value='850' class='' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>
      68.00% [850/1250 01:15<00:35 0.6711]
    </div>
    
</div>

</div>

</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Nice!</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="6.-Using-a-Language-Model-via-AWD-LSTM-[fastai]">6. Using a Language Model via AWD-LSTM [fastai]<a class="anchor-link" href="#6.-Using-a-Language-Model-via-AWD-LSTM-[fastai]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Using a pretrained language model for downstream tasks is a popular and efficient technique also! Fine-tuning the language model first is even better, as shown in <a href="https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb">chapter 10 from fastbook</a></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's a quick example of training a model with this dataset using fastai!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>First we'll need to create our vocab as we did before!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">fastai_vocab</span> <span class="o">=</span> <span class="n">make_vocab</span><span class="p">(</span><span class="n">token_counter</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To continue using the same subset dataframes, we'll combine both the <code>train_df</code> and <code>valid_df</code> into <code>combined_df</code>, then let fastai split at index 40k by using the <code>splitter</code> argument in the <a href="https://docs.fast.ai/tutorial.datablock.html">DataBlocks API</a>!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Creating a single DataFrame and to split within the DataBlocks API!</span>
<span class="n">combined_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">train_df</span><span class="p">,</span> <span class="n">valid_df</span><span class="p">])</span>
<span class="n">combined_df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="nb">len</span><span class="p">(</span><span class="n">combined_df</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">amazon_polarity</span> <span class="o">=</span> <span class="n">DataBlock</span><span class="p">(</span><span class="n">blocks</span><span class="o">=</span><span class="p">(</span><span class="n">TextBlock</span><span class="o">.</span><span class="n">from_df</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="n">seq_len</span><span class="o">=</span><span class="mi">128</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="n">fastai_vocab</span><span class="p">),</span>
                                    <span class="n">CategoryBlock</span><span class="p">),</span>
                            <span class="n">get_x</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;text&#39;</span><span class="p">),</span>
                            <span class="n">get_y</span><span class="o">=</span><span class="n">ColReader</span><span class="p">(</span><span class="s1">&#39;label&#39;</span><span class="p">),</span>
                            <span class="n">splitter</span><span class="o">=</span><span class="n">IndexSplitter</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">40000</span><span class="p">,</span> <span class="mi">42000</span><span class="p">)))</span>

<span class="c1"># Passing a custom DataFrame in and splitting by the index!</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">amazon_polarity</span><span class="o">.</span><span class="n">dataloaders</span><span class="p">(</span><span class="n">combined_df</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">32</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Checking for the correct amount of samples</span>
<span class="nb">len</span><span class="p">(</span><span class="n">dls</span><span class="o">.</span><span class="n">train_ds</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dls</span><span class="o">.</span><span class="n">valid_ds</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='Looks good! We still have our 40k training samples and 2k validation samples :)' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Number of batches</span>
<span class="nb">len</span><span class="p">(</span><span class="n">dls</span><span class="o">.</span><span class="n">train</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">dls</span><span class="o">.</span><span class="n">valid</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">dls</span><span class="o">.</span><span class="n">train</span><span class="o">.</span><span class="n">show_batch</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include tip.html content='Take a close look at the samples above! We can see that we&#8217;re not using a simple tokenizer that just separates on spaces anymore! This tokenizer has lots of additional special tokens and special rules to help with better feature/token engeineering! (e.g. <code>xxbos</code> -&gt; beginning of a text, and <code>xxmaj</code> -&gt; next word capitalized) More can be found here in <a href="https://github.com/fastai/fastbook/blob/master/10_nlp.ipynb">chapter 10 from fastbook</a>' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span> <span class="o">=</span> <span class="n">text_classifier_learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">AWD_LSTM</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='We&#8217;ll be using the <a href="https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM">AWD-LSTM</a> language model as our pretrained model to finetune!' %}</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fine_tune</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">base_lr</span><span class="o">=</span><span class="mf">3e-3</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include important.html content='Compare the results of this model with the previous one we built with pure PyTorch. We can see regularization come into effect!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="7.-Using-a-Language-Model-via-DistilBERT-[HuggingFace-&amp;-PyTorch-&amp;-fastai]">7. Using a Language Model via DistilBERT [HuggingFace &amp; PyTorch &amp; fastai]<a class="anchor-link" href="#7.-Using-a-Language-Model-via-DistilBERT-[HuggingFace-&amp;-PyTorch-&amp;-fastai]"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We can load up a tokenizer and transformer from <a href="https://huggingface.co/transformers/">HuggingFace's Transformers API</a> and train them using fastai! We'll be using <a href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a> as it's smaller and faster than the original <a href="https://huggingface.co/transformers/model_doc/bert.html">BERT</a>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p>"<a href="https://huggingface.co/transformers/model_doc/distilbert.html">DistilBERT</a> is a small, fast, cheap and light Transformer model trained by distilling BERT base. It has 40% less parameters than <code>bert-base-uncased</code>, runs 60% faster while preserving over 95% of BERTs performances as measured on the GLUE language understanding benchmark."</p>
</blockquote>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModelForSequenceClassification</span>

<span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
<span class="n">hf_model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;distilbert-base-uncased&quot;</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">sample_text</span> <span class="o">=</span> <span class="n">train_df</span><span class="p">[</span><span class="s1">&#39;text&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="n">sample_text</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_outputs</span> <span class="o">=</span> <span class="n">hf_tokenizer</span><span class="p">(</span><span class="n">sample_text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
<span class="n">tokenizer_outputs</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_outputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">tokenizer_outputs</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='We can see that bert&#8217;s tokenizer returns 2 items, <code>input_ids</code> (numericalization of tokens) and <code>attention_mask</code> (manually lets you control attention on specific tokens). DistilBERT will take both of these as input!' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's go ahead and put this into another dataset class:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse</span>
<span class="k">class</span> <span class="nc">HF_Dataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">df</span> <span class="o">=</span> <span class="n">df</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span> <span class="o">=</span> <span class="n">hf_tokenizer</span>
        
        <span class="c1"># label 1 is negative sentiment and label 2 is positive sentiment</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="mi">1</span><span class="p">:</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="mi">1</span><span class="p">}</span>
        
    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">decode</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="s1">&#39; &#39;</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">tokenizer_outputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]])</span>
    
    <span class="k">def</span> <span class="nf">decode_to_original</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">token_ids</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">token_ids</span><span class="o">.</span><span class="n">squeeze</span><span class="p">())</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">index</span><span class="p">):</span>
        <span class="n">label</span><span class="p">,</span> <span class="n">title</span><span class="p">,</span> <span class="n">text</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">df</span><span class="o">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">index</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_map</span><span class="p">[</span><span class="n">label</span><span class="p">]</span>
        <span class="n">label</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

        <span class="n">tokenizer_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">512</span><span class="p">)</span>
        
        <span class="n">tokenizer_output</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">()</span>
        <span class="n">tokenizer_output</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze_</span><span class="p">()</span>
        
        <span class="k">return</span> <span class="n">tokenizer_output</span><span class="p">,</span> <span class="n">label</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">HF_Dataset</span><span class="p">(</span><span class="n">train_df</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
<span class="n">valid_dataset</span> <span class="o">=</span> <span class="n">HF_Dataset</span><span class="p">(</span><span class="n">valid_df</span><span class="p">,</span> <span class="n">hf_tokenizer</span><span class="p">)</span>
<span class="nb">len</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">tokenizer_outputs</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="n">train_dataset</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">tokenizer_outputs</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">label</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Only printing first 500 characters due to the excess [PAD] tokens!</span>
<span class="n">train_dataset</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="n">tokenizer_outputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])[:</span><span class="mi">500</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include tip.html content='Notice how DistilBERT also has a more efficient tokenizer compared to the standard tokenizer we used at the start. We can see it&#8217;s using subword tokenization (e.g. <code>keyboarding</code> -&gt; <code>keyboard</code> <code>##ing</code>, as well as <code>fresher</code> -&gt; <code>fresh</code> <code>##er</code>)' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Here's the original input (tokens decoded, but without the subword tokenization showing):</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Only printing first 500 characters due to the excess [PAD] tokens!</span>
<span class="n">train_dataset</span><span class="o">.</span><span class="n">decode_to_original</span><span class="p">(</span><span class="n">tokenizer_outputs</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])[:</span><span class="mi">500</span><span class="p">]</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">valid_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">valid_dataset</span><span class="p">,</span> <span class="n">bs</span><span class="o">=</span><span class="mi">8</span><span class="p">)</span>
<span class="n">dls</span> <span class="o">=</span> <span class="n">DataLoaders</span><span class="p">(</span><span class="n">train_dl</span><span class="p">,</span> <span class="n">valid_dl</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>Let's make sure that items within the <code>tokenizer_outputs</code> dictionary can get batched together properly:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Making sure things are getting batched correctly!</span>
<span class="n">batched_data</span><span class="p">,</span> <span class="n">batched_labels</span> <span class="o">=</span> <span class="n">train_dl</span><span class="o">.</span><span class="n">one_batch</span><span class="p">()</span>
<span class="n">batched_data</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">batched_data</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">batched_labels</span><span class="o">.</span><span class="n">shape</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>To allow this model to be trained by fastai, we need to ensure that the model simply takes a single input, and returns the logits. We can create a small class to handle the intermediate stuff (like the decoupling of tokenizer_outputs via <code>**tokenizer_outputs</code>, and extracting the logits from the model output via <code>.logits</code>. Here's an example of a forward pass using HF's tokenizer and model:</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">hf_model</span><span class="p">(</span><span class="o">**</span><span class="n">batched_data</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<details class="description">
      <summary class="btn btn-sm" data-open="Hide Code" data-close="Show Code"></summary>
        <p><div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1">#collapse</span>
<span class="k">class</span> <span class="nc">HF_Model</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
  
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hf_model</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span> <span class="o">=</span> <span class="n">hf_model</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tokenizer_outputs</span><span class="p">):</span>
        
        <span class="n">model_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">hf_model</span><span class="p">(</span><span class="o">**</span><span class="n">tokenizer_outputs</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">model_output</span><span class="o">.</span><span class="n">logits</span>
</pre></div>

    </div>
</div>
</div>
</p>
    </details>
</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">HF_Model</span><span class="p">(</span><span class="n">hf_model</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>With the same data, here's an example of a forward pass with our small wrapper over the <code>hf_model</code></p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">logits</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">batched_data</span><span class="p">)</span>
<span class="n">logits</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>{% include note.html content='We no longer need double asteriks for decoupling the input data dictionary, and we don&#8217;t need to extract the logits from the output via <code>.logits</code> This allows for easy compatability with <code>fastai</code>&#8217;s <code>Learner</code> class' %}</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p>We have everything we need to finetune this model now!</p>

</div>
</div>
</div>
    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="c1"># Manually popping the model onto the gpu since the data is in a dictionary format</span>
<span class="c1"># (doesn&#39;t automatically place model + data on gpu otherwise)</span>
<span class="n">learn</span> <span class="o">=</span> <span class="n">Learner</span><span class="p">(</span><span class="n">dls</span><span class="p">,</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">(),</span> <span class="n">loss_func</span><span class="o">=</span><span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">(),</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="n">accuracy</span><span class="p">])</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">lr_find</span><span class="p">()</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

    {% raw %}
    
<div class="cell border-box-sizing code_cell rendered">
<div class="input">

<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython3"><pre><span></span><span class="n">learn</span><span class="o">.</span><span class="n">fit_one_cycle</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mf">3e-4</span><span class="p">)</span>
</pre></div>

    </div>
</div>
</div>

</div>
    {% endraw %}

<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="8.-Conclusion">8. Conclusion<a class="anchor-link" href="#8.-Conclusion"> </a></h1>
</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<ul>
<li>Each method has it's pros and cons, but in this case the runtime of DistilBERT was not worth it's performance.<ul>
<li>In other tasks, it may be better to use a transformer, but for many common NLP tasks, sometimes a simple model is enough!</li>
</ul>
</li>
<li>We can take models written in pure PyTorch, or use existing models from elsewhere (e.g. HuggingFace), and train them with ease within fastai.</li>
<li>NLP has lots of variation in terms of tokenization methods. Libaries like fastai &amp; HuggingFace make the NLP data processing pipeline much easier/quicker to get up and running!</li>
</ul>

</div>
</div>
</div>
</div>
 

